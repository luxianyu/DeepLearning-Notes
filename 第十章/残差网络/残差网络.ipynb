{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 残差网络（Residual Networks）\n",
    "\n",
    "在本节中，你将学习如何构建非常深的卷积神经网络，使用残差网络（ResNets）。理论上，非常深的网络可以表示非常复杂的函数；但在实践中，它们很难训练。由 [He 等人](https://arxiv.org/pdf/1512.03385.pdf) 提出的残差网络，使你可以训练比以往更深的网络。\n",
    "\n",
    "**在本节中，你将：**\n",
    "- 实现 ResNets 的基本构建模块。\n",
    "- 将这些模块组合起来，实现并训练一个用于图像分类的先进神经网络。\n",
    "\n",
    "在开始之前，先运行下面的代码以加载所需的包。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 导入库与环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 导入必要的库\n",
    "# ==============================\n",
    "\n",
    "# ------------------------------\n",
    "# 数值计算\n",
    "# ------------------------------\n",
    "import numpy as np                   # NumPy，用于高效的数组和矩阵运算\n",
    "\n",
    "# ------------------------------\n",
    "# PyTorch 核心模块\n",
    "# ------------------------------\n",
    "import torch                         # PyTorch 主模块\n",
    "import torch.nn as nn                # 神经网络模块（定义模型用）\n",
    "import torch.nn.functional as F      # 常用函数模块（激活函数、卷积操作等）\n",
    "import torch.optim as optim          # 优化器模块（如 SGD, Adam）\n",
    "\n",
    "# ------------------------------\n",
    "# 数据处理与模型相关\n",
    "# ------------------------------\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset  \n",
    "# Dataset: 自定义数据集类的基类\n",
    "# DataLoader: 用于批量加载数据，支持 shuffle、batch 等\n",
    "# TensorDataset: 可以直接把 Tensor 包装成 Dataset\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "# datasets: PyTorch 提供的常用数据集（MNIST、CIFAR 等）\n",
    "# transforms: 图像预处理、数据增强\n",
    "# models: torchvision 提供的预训练模型（ResNet, VGG 等）\n",
    "\n",
    "# ------------------------------\n",
    "# 图像处理与可视化\n",
    "# ------------------------------\n",
    "from PIL import Image               # PIL 图像处理库\n",
    "import matplotlib.pyplot as plt     # 绘图库，用于显示图像、绘制曲线\n",
    "\n",
    "# ------------------------------\n",
    "# 系统与文件操作\n",
    "# ------------------------------\n",
    "import os                           # 操作系统接口（文件路径、目录创建等）\n",
    "\n",
    "# ------------------------------\n",
    "# HDF5 数据读取\n",
    "# ------------------------------\n",
    "import h5py                         # h5py：用于读取和写入 HDF5 文件，常用于存储大规模数据集\n",
    "\n",
    "# ------------------------------\n",
    "# 设置随机数种子\n",
    "# ------------------------------\n",
    "torch.manual_seed(42)               # PyTorch 随机数种子，保证可复现\n",
    "np.random.seed(42)                  # NumPy 随机数种子，保证可复现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 数据加载相关\n",
    "\n",
    "让我们加载 SIGNS 数据集。\n",
    "\n",
    "<img src=\"images/signs_data_kiank.png\" style=\"width:450px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 6** </u><font color='purple'>  : **SIGNS 数据集** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 数据加载函数\n",
    "# ==============================\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    功能：\n",
    "        从 HDF5 文件中加载训练集和测试集的数据和标签\n",
    "        \n",
    "    返回：\n",
    "        train_set_x_orig -- 训练集特征，shape = (m_train, 64, 64, 3)，每张图片为 64x64 RGB 图像\n",
    "        train_set_y_orig -- 训练集标签，shape = (m_train,)，整数表示类别\n",
    "        test_set_x_orig  -- 测试集特征，shape = (m_test, 64, 64, 3)\n",
    "        test_set_y_orig  -- 测试集标签，shape = (m_test,)\n",
    "        classes          -- 标签类别数组，shape = (n_classes,)\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # 加载训练集 HDF5 文件\n",
    "    # ------------------------------\n",
    "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
    "    # train_dataset: HDF5 文件对象\n",
    "    # 参数 \"datasets/train_signs.h5\" 表示文件路径\n",
    "    # \"r\" 表示只读模式，不能修改文件内容\n",
    "\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    # train_dataset[\"train_set_x\"]: HDF5 文件中的训练特征数据集\n",
    "    # [:] 表示读取整个数据集\n",
    "    # np.array(...) 将 HDF5 数据转为 numpy 数组，方便后续处理\n",
    "    # 结果 shape = (m_train, 64, 64, 3)\n",
    "\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "    # train_dataset[\"train_set_y\"]: HDF5 文件中的训练标签数据集\n",
    "    # np.array(...) 转为 numpy 数组\n",
    "    # 结果 shape = (m_train,)，每个元素为整数类别\n",
    "\n",
    "    # ------------------------------\n",
    "    # 加载测试集 HDF5 文件\n",
    "    # ------------------------------\n",
    "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
    "    # test_dataset: HDF5 文件对象，测试集数据\n",
    "    # \"r\" 表示只读模式\n",
    "\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    # 测试集特征数据，shape = (m_test, 64, 64, 3)\n",
    "\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
    "    # 测试集标签数据，shape = (m_test,)\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "    # list_classes: 类别列表，如 [0,1,2,...,5]\n",
    "    # np.array(...) 转为 numpy 数组，方便索引和分类\n",
    "\n",
    "    # ------------------------------\n",
    "    # 返回加载的数据\n",
    "    # ------------------------------\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用之前定义的 load_dataset() 函数，加载训练集和测试集\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "# X_train_orig: 训练集特征，shape = (m_train, 64, 64, 3)\n",
    "# Y_train_orig: 训练集标签，shape = (m_train,)\n",
    "# X_test_orig: 测试集特征，shape = (m_test, 64, 64, 3)\n",
    "# Y_test_orig: 测试集标签，shape = (m_test,)\n",
    "# classes: 标签类别数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 自定义 Dataset类\n",
    "# ==============================\n",
    "class SignsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "        将 NumPy 数据集封装为 PyTorch Dataset，用于 DataLoader 迭代\n",
    "        支持图像归一化、通道转换以及可选 transform\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_np, Y_np, transform=None):\n",
    "        \"\"\"\n",
    "        初始化 Dataset\n",
    "\n",
    "        参数：\n",
    "        X_np     -- NumPy 图像数组，shape = (m,64,64,3)，dtype uint8 或 float32\n",
    "        Y_np     -- NumPy 标签数组，shape = (m,) 或 (m,1)\n",
    "        transform-- 可选图像变换（如 torchvision.transforms）\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------------------------\n",
    "        # 特征数据转换为 float32 类型\n",
    "        # ------------------------------\n",
    "        self.X = X_np.astype(np.float32)\n",
    "        # astype(np.float32)：确保数据类型为 float32，以便后续 PyTorch 计算\n",
    "\n",
    "        # 如果最大值 > 2.0，说明是原始 0-255 图像，需要归一化到 [0,1]\n",
    "        if self.X.max() > 2.0:\n",
    "            self.X = self.X / 255.0\n",
    "\n",
    "        # ------------------------------\n",
    "        # PyTorch 要求图像通道在前（C,H,W）\n",
    "        # ------------------------------\n",
    "        self.X = np.transpose(self.X, (0,3,1,2))\n",
    "        # np.transpose：调整数组维度顺序\n",
    "        # (0,3,1,2) 说明：\n",
    "        # 0: 样本数 m 不变\n",
    "        # 3: 通道数 C 从最后一维移到第二维\n",
    "        # 1: 高度 H\n",
    "        # 2: 宽度 W\n",
    "        # 最终 shape = (m,3,64,64)\n",
    "\n",
    "        # ------------------------------\n",
    "        # 标签处理\n",
    "        # ------------------------------\n",
    "        self.y = np.array(Y_np).reshape(-1).astype(np.int64)\n",
    "        # reshape(-1)：确保标签为一维向量\n",
    "        # int64：PyTorch 分类任务要求标签为 long 类型\n",
    "\n",
    "        # ------------------------------\n",
    "        # 图像变换（可选）\n",
    "        # ------------------------------\n",
    "        self.transform = transform\n",
    "        # 如果传入 transform（如随机裁剪、旋转等），后续 __getitem__ 会应用\n",
    "\n",
    "    # ------------------------------\n",
    "    # 返回样本总数\n",
    "    # ------------------------------\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "        # Dataset 长度就是样本数 m\n",
    "\n",
    "    # ------------------------------\n",
    "    # 返回单个样本和标签\n",
    "    # ------------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        功能：\n",
    "            根据索引 idx 返回单个样本图像和标签\n",
    "\n",
    "        参数：\n",
    "        idx -- 样本索引，整数\n",
    "\n",
    "        返回：\n",
    "        img   -- torch.Tensor 图像，shape = (3,64,64)，dtype float32\n",
    "        label -- torch.Tensor 标签，dtype long\n",
    "        \"\"\"\n",
    "\n",
    "        # 取第 idx 张图像\n",
    "        img = self.X[idx]       \n",
    "\n",
    "        # 取对应标签\n",
    "        label = self.y[idx]     \n",
    "\n",
    "        # 如果定义了 transform，应用图像变换\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # 转换为 PyTorch Tensor\n",
    "        return torch.from_numpy(img), torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "        # ------------------------------\n",
    "        # torch.from_numpy(img)\n",
    "        # 功能：将 NumPy 数组 img 转换为 PyTorch Tensor\n",
    "        # 输入 img：\n",
    "        #   - 形状为 (C, H, W)，通道数在前\n",
    "        #   - 数据类型通常为 float32\n",
    "        # 输出：\n",
    "        #   - PyTorch Tensor，保持原数组数据\n",
    "        #   - 与原 NumPy 数组共享内存，修改 Tensor 会影响原数组\n",
    "        # 用途：\n",
    "        #   - PyTorch 模型训练需要 Tensor 输入，而不是 NumPy 数组\n",
    "        #   - 这里 img 是单张图像，用于 Dataset 返回一个样本\n",
    "        \n",
    "        # torch.tensor(label, dtype=torch.long)\n",
    "        # 功能：将单个标签 label 转换为 PyTorch Tensor\n",
    "        # 参数：\n",
    "        #   - label：样本的类别标签（整数）\n",
    "        #   - dtype=torch.long：指定数据类型为 long (int64)，PyTorch 分类任务要求\n",
    "        # 输出：\n",
    "        #   - PyTorch Tensor，形状为标量 ( ) 或 (1,)\n",
    "        #   - 数据类型为 torch.int64\n",
    "        # 用途：\n",
    "        #   - 作为目标标签，输入到分类损失函数 nn.CrossEntropyLoss 时必须是 long 类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 创建 Dataset 和 DataLoader\n",
    "# ==============================\n",
    "\n",
    "# 定义类别数\n",
    "num_classes = 6\n",
    "# 原始脚本中 X_train/X_test 已除以 255，这里在 SignsDataset 内部已做归一化\n",
    "\n",
    "# 使用自定义 SignsDataset 封装训练集和测试集\n",
    "train_dataset = SignsDataset(X_train_orig, Y_train_orig)\n",
    "# train_dataset: 封装训练集的 Dataset 对象，可用于 DataLoader 迭代\n",
    "test_dataset  = SignsDataset(X_test_orig,  Y_test_orig)\n",
    "# test_dataset: 封装测试集的 Dataset 对象\n",
    "\n",
    "# 定义每个 batch 的大小\n",
    "batch_size = 64\n",
    "\n",
    "# ------------------------------\n",
    "# 设置随机种子保证 DataLoader shuffle 可复现\n",
    "# ------------------------------\n",
    "g = torch.Generator()   # 创建一个随机数生成器\n",
    "g.manual_seed(3)        # 设置固定种子 3，确保每次 shuffle 顺序相同\n",
    "\n",
    "# 创建训练集 DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,      # 数据集对象\n",
    "    batch_size=batch_size,  # 每个 batch 的样本数\n",
    "    shuffle=True,       # 打乱样本顺序\n",
    "    generator=g         # 使用固定种子生成器，保证可复现\n",
    ")\n",
    "\n",
    "# 创建测试集 DataLoader\n",
    "test_loader  = DataLoader(\n",
    "    test_dataset,       # 数据集对象\n",
    "    batch_size=batch_size,  # 每个 batch 的样本数\n",
    "    shuffle=False       # 测试集不打乱\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 非常深的神经网络问题\n",
    "近年来，神经网络越来越深，最先进的网络从最初的几层（例如 AlexNet）发展到上百层甚至更多。\n",
    "\n",
    "### 深度网络的优势\n",
    "非常深的网络主要优势在于：\n",
    "1. **表示能力强**：可以拟合非常复杂的函数。\n",
    "2. **特征学习层次丰富**：  \n",
    "   - 较浅的层学习低级特征，例如边缘（edges）。  \n",
    "   - 较深的层学习高级特征，例如对象的复杂结构或纹理。  \n",
    "\n",
    "### 深度网络的训练难点\n",
    "尽管深层网络能力强，但**更深的网络并不总是效果更好**。主要障碍是 **梯度消失（vanishing gradients）**：\n",
    "\n",
    "- 在深层网络中，梯度信号在向前传播和反向传播时可能迅速衰减为零。\n",
    "- 在梯度下降训练过程中，当你从最后一层反向传播到第一层时，每一步都会与权重矩阵相乘，因此梯度可能指数级衰减（vanishing）或在少数情况下指数级增长（exploding）。\n",
    "\n",
    "### 梯度消失的表现\n",
    "- 在训练过程中，你可能会观察到**前几层梯度的大小（或范数）迅速变为零**，导致梯度下降几乎无法更新这些层的权重。\n",
    "- 这会使得训练非常缓慢甚至陷入停滞。\n",
    "\n",
    "> 小结：非常深的神经网络可以学习复杂的特征，但梯度消失问题会阻碍前几层权重的有效更新，需要特殊的网络结构或训练技巧来解决。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vanishing_grad_kiank.png\" style=\"width:450px;height:220px;\">\r\n",
    "<caption><center> \r\n",
    "<u> <font color='purple'> **图 1** </u><font color='purple'> ：**梯度消失示意图** <br> \r\n",
    "随着网络训练，前几层的学习速度会非常迅速地下降\r\n",
    "</center></caption>\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 解决梯度消失问题：残差网络（Residual Network, ResNet）\r\n",
    "\r\n",
    "- 如图 1 所示，普通深度神经网络在训练时，前几层的梯度会迅速减小，导致学习变慢。\r\n",
    "- 为了解决这个问题，我们可以使用 **残差网络（ResNet）**：\r\n",
    "  1. ResNet 通过 **残差模块（residual block）** 引入跳跃连接（skip connections）。\r\n",
    "  2. 这些跳跃连接允许梯度直接沿着网络向前或向后传播，**缓解梯度消失**。\r\n",
    "  3. 这样，即使网络非常深，也能保持前几层的有效学习。\r\n",
    "  \r\n",
    "> 小结：接下来，我们将通过构建 ResNet，学习如何训练非常深的卷积神经网络，同时避免梯度消失的问题。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 构建残差网络（Residual Network, ResNet）\n",
    "\n",
    "在 ResNet 中，**“捷径（shortcut）”或“跳跃连接（skip connection）”** 允许梯度直接反向传播到前面的层，从而缓解梯度消失问题：  \n",
    "\n",
    "<img src=\"images/skip_connection_kiank.png\" style=\"width:650px;height:200px;\">\n",
    "<caption><center> \n",
    "<u> <font color='purple'> **图 2** </u><font color='purple'> ：一个 ResNet 模块示意图，展示了 **跳跃连接** <br> \n",
    "</center></caption>\n",
    "\n",
    "---\n",
    "\n",
    "### 图示解读\n",
    "- 左侧图：表示网络的“主路径”（main path）。\n",
    "- 右侧图：在主路径上添加了一个跳跃连接（shortcut）。\n",
    "- 通过将这样的 ResNet 模块堆叠起来，就可以形成一个**非常深的网络**。\n",
    "\n",
    "---\n",
    "\n",
    "### ResNet 模块的优势\n",
    "1. **轻松学习恒等映射（identity function）**：\n",
    "   - 如果某个模块只需要输出等于输入，跳跃连接可以让网络直接实现恒等映射。\n",
    "   - 这意味着即使堆叠更多的模块，也不会损害训练集性能。\n",
    "2. **缓解梯度消失**：\n",
    "   - 跳跃连接允许梯度直\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - 恒等块（Identity Block）\n",
    "\n",
    "**恒等块**是 ResNet 中标准的模块，用于输入激活（$a^{[l]}$）和输出激活（$a^{[l+2]}$）**维度相同**的情况。  \n",
    "\n",
    "#### 模块示意图\n",
    "\n",
    "<img src=\"images/idblock2_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> \n",
    "<u> <font color='purple'> **图 3** </u><font color='purple'> ：恒等块示意图。跳跃连接（shortcut）“跨越”2层。 \n",
    "</center></caption>\n",
    "\n",
    "- 上方路径是 **shortcut path（捷径路径）**  \n",
    "- 下方路径是 **main path（主路径））**  \n",
    "- 图中明确标出了每一层的 **CONV2D + ReLU** 步骤  \n",
    "- 为了加快训练，还加入了 **BatchNorm（批量归一化）**  \n",
    "- 在 PyTorch中，BatchNorm 只需一行代码即可实现，非常简单  \n",
    "\n",
    "---\n",
    "\n",
    "#### 升级版恒等块\n",
    "\n",
    "在本练习中，你将实现一个**更强大的恒等块**，其跳跃连接跨越 **3 层隐藏层** 而非 2 层：\n",
    "\n",
    "<img src=\"images/idblock3_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> \n",
    "<u> <font color='purple'> **图 4** </u><font color='purple'> ：恒等块示意图。跳跃连接“跨越”3层。 \n",
    "</center></caption>\n",
    "\n",
    "---\n",
    "\n",
    "#### 主路径（Main Path）的每一步\n",
    "\n",
    "**第一部分**：\n",
    "- CONV2D：$F_1$ 个卷积核，大小 (1,1)，步幅 (1,1)，padding=\"valid\"，名称为 `conv_name_base + '2a'`  \n",
    "- BatchNorm：沿通道维归一化，名称为 `bn_name_base + '2a'`  \n",
    "- ReLU 激活函数：无名称、无超参数  \n",
    "\n",
    "**第二部分**：\n",
    "- CONV2D：$F_2$ 个卷积核，大小 (f,f)，步幅 (1,1)，padding=\"same\"，名称为 `conv_name_base + '2b'`  \n",
    "- BatchNorm：沿通道维归一化，名称为 `bn_name_base + '2b'`  \n",
    "- ReLU 激活函数：无名称、无超参数  \n",
    "\n",
    "**第三部分**：\n",
    "- CONV2D：$F_3$ 个卷积核，大小 (1,1)，步幅 (1,1)，padding=\"valid\"，名称为 `conv_name_base + '2c'`  \n",
    "- BatchNorm：沿通道维归一化，名称为 `bn_name_base + '2c'`  \n",
    "- **注意**：这一部分没有 ReLU 激活函数  \n",
    "\n",
    "**最终步骤**：\n",
    "- 将主路径输出与 **shortcut** 相加  \n",
    "- 再应用 ReLU 激活函数  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> 小结：首先实现主路径的第一步（1x1 卷积 + BatchNorm + ReLU），然后依次完成第二步和第三步，最后与 shortcut 相加并激活即可。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 恒等块 (Identity Block)\n",
    "# =====================================\n",
    "class IdentityBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet 恒等块：输入和输出通道数一致\n",
    "    特点：\n",
    "        - 输入 x 与输出 out 的维度相同\n",
    "        - 通过三层卷积 + 批量归一化 + ReLU 激活\n",
    "        - 最后通过残差连接（shortcut）实现加法\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, filters, f):\n",
    "        \"\"\"\n",
    "        初始化恒等块\n",
    "\n",
    "        参数：\n",
    "            in_channels -- 输入通道数\n",
    "            filters     -- list 类型 [F1, F2, F3]，三个卷积层的输出通道数\n",
    "            f           -- 第二层卷积核大小 (中间卷积)\n",
    "        \"\"\"\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        F1, F2, F3 = filters  # 解包三个卷积层的通道数\n",
    "\n",
    "        # ------------------------------\n",
    "        # 第一层卷积 1x1\n",
    "        # ------------------------------\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,  # 输入通道\n",
    "            out_channels=F1,          # 输出通道\n",
    "            kernel_size=1,            # 卷积核 1x1\n",
    "            stride=1,                 # 步长 1\n",
    "            padding=0,                # 不填充\n",
    "            bias=False                # 不使用偏置（因为后面有 BatchNorm）\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(F1) # 批量归一化层\n",
    "\n",
    "        # ------------------------------\n",
    "        # 第二层卷积 fxf\n",
    "        # ------------------------------\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=F1,           # 输入通道 = 第一层输出\n",
    "            out_channels=F2,          # 输出通道\n",
    "            kernel_size=f,            # 卷积核大小 fxf\n",
    "            stride=1,                 # 步长 1\n",
    "            padding=f//2,             # SAME 填充，保持宽高不变\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "\n",
    "        # ------------------------------\n",
    "        # 第三层卷积 1x1\n",
    "        # ------------------------------\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=F2, \n",
    "            out_channels=F3, \n",
    "            kernel_size=1, \n",
    "            stride=1, \n",
    "            padding=0, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "\n",
    "        # ReLU 激活函数\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数：\n",
    "            x -- 输入 Tensor，形状 (batch_size, in_channels, H, W)\n",
    "\n",
    "        返回：\n",
    "            输出 Tensor，形状与输入相同 (batch_size, F3, H, W)\n",
    "        \"\"\"\n",
    "        shortcut = x  # 保存残差（输入 x）\n",
    "\n",
    "        # 第一层卷积 -> BN -> ReLU\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # 第二层卷积 -> BN -> ReLU\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # 第三层卷积 -> BN\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        # 残差连接\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模式: train\n",
      "输入: mean = 0.0810, std = 0.9571\n",
      "输出: mean = 0.5569, std = 0.8006\n",
      "out = [2.8718672  0.10689706 0.         0.         0.8642092  0.        ]\n",
      "\n",
      "模式: eval\n",
      "输入: mean = 0.0810, std = 0.9571\n",
      "输出: mean = 0.4203, std = 0.5829\n",
      "out = [1.6499243 0.        0.        0.        0.8540906 0.       ]\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 测试恒等块 (Identity Block)\n",
    "# =============================\n",
    "\n",
    "# ------------------------------\n",
    "# 设置随机种子，保证实验可复现\n",
    "# ------------------------------\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# ------------------------------\n",
    "# 定义输入张量 X\n",
    "# ------------------------------\n",
    "# batch_size = 3, 通道数 = 6, 高 = 4, 宽 = 4\n",
    "# 使用 NumPy 生成随机数，然后转为 PyTorch Tensor\n",
    "X = torch.tensor(np.random.randn(3, 6, 4, 4), dtype=torch.float32)\n",
    "\n",
    "# ------------------------------\n",
    "# 定义恒等块\n",
    "# ------------------------------\n",
    "# 输入通道 = 6, 中间卷积核大小 f = 3, 三层卷积输出通道分别 = [2,4,6]\n",
    "identity_block = IdentityBlock(in_channels=6, f=3, filters=[2,4,6])\n",
    "\n",
    "# ------------------------------\n",
    "# 分别在训练模式和评估模式下测试\n",
    "# ------------------------------\n",
    "for mode in [\"train\", \"eval\"]:\n",
    "    if mode == \"train\":\n",
    "        identity_block.train()  # 设置为训练模式（BN 会使用 batch 统计）\n",
    "    else:\n",
    "        identity_block.eval()   # 设置为评估模式（BN 使用滑动平均值）\n",
    "\n",
    "    # 前向传播\n",
    "    out = identity_block(X)\n",
    "\n",
    "    # 打印输入输出统计信息\n",
    "    print(f\"\\n模式: {mode}\")\n",
    "    print(\"输入: mean = %.4f, std = %.4f\" % (X.mean().item(), X.std().item()))\n",
    "    print(\"输出: mean = %.4f, std = %.4f\" % (out.mean().item(), out.std().item()))\n",
    "    # 展示输出前 6 个元素（展平后）\n",
    "    print(\"out =\", out.flatten()[:6].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - 卷积块（Convolutional Block）\n",
    "\n",
    "在实现了 ResNet 的恒等块（Identity Block）之后，另一种常用模块是 **卷积块（Convolutional Block）**。  \n",
    "当输入和输出维度不一致时，就需要使用卷积块。它和恒等块的主要区别是：**shortcut（捷径路径）上也有一个 CONV2D 卷积层**。\n",
    "\n",
    "#### 模块示意图\n",
    "\n",
    "<img src=\"images/convblock_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> \n",
    "<u> <font color='purple'> **图 4** </u><font color='purple'> ：卷积块示意图 \n",
    "</center></caption>\n",
    "\n",
    "---\n",
    "\n",
    "### 卷积块的作用\n",
    "\n",
    "- shortcut 路径上的卷积层用于调整输入 $x$ 的维度，使主路径和捷径路径的输出维度一致，以便最终相加。\n",
    "- 举例：如果希望将激活的高和宽缩小 2 倍，可以使用 1x1 卷积，stride=2。\n",
    "- 这个卷积层不使用非线性激活函数，其主要作用是学习一个线性变换，使维度匹配，方便后续相加。\n",
    "\n",
    "---\n",
    "\n",
    "### 主路径（Main Path）每一步\n",
    "\n",
    "**第一部分**：\n",
    "- CONV2D：$F_1$ 个卷积核，大小 (1,1)，步幅 (s,s)，padding=\"valid\"`\n",
    "- BatchNorm：沿通道维归一化\n",
    "- ReLU 激活函数：无名称、无超参数  \n",
    "\n",
    "**第二部分**：\n",
    "- CONV2D：$F_2$ 个卷积核，大小 (f,f)，步幅 (1,1)，padding=\"same\"\n",
    "- BatchNorm：沿通道维归一化\n",
    "- ReLU 激活函数：无名称、无超参数  \n",
    "\n",
    "**第三部分**：\n",
    "- CONV2D：$F_3$ 个卷积核，大小 (1,1)，步幅 (1,1)，padding=\"valid\"\n",
    "- BatchNorm：沿通道维归一化  \n",
    "- **注意**：这一部分没有 ReLU 激活函数  \n",
    "\n",
    "---\n",
    "\n",
    "### 捷径路径（Shortcut Path）\n",
    "\n",
    "- CONV2D：$F_3$ 个卷积核，大小 (1,1)，步幅 (s,s)，padding=\"valid\"`\n",
    "- BatchNorm：沿通道维归一化，名称为   \n",
    "\n",
    "---\n",
    "\n",
    "### 最终步骤\n",
    "\n",
    "- 将主路径输出与捷径路径输出相加  \n",
    "- 再应用 ReLU 激活函数  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> 小结：卷积块和恒等块的差别主要在捷径路径上加了一个 1x1 卷积来调整维度，其余主路径逻辑与恒等块类似。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 卷积块 (Convolutional Block)\n",
    "# =====================================\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet 卷积块：\n",
    "        - 输入输出通道可能不一致\n",
    "        - 需要通过卷积调整 shortcut\n",
    "        - 主路径：Conv -> BN -> ReLU -> Conv -> BN -> ReLU -> Conv -> BN\n",
    "        - 捷径路径：Conv1x1 + BN\n",
    "        - 最后两者相加 -> ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, filters, f, s=2):\n",
    "        \"\"\"\n",
    "        初始化卷积块\n",
    "\n",
    "        参数：\n",
    "            in_channels -- 输入通道数\n",
    "            filters     -- list [F1, F2, F3]，主路径三层卷积输出通道数\n",
    "            f           -- 第二层卷积核大小\n",
    "            s           -- 主路径第一层卷积步幅，同时控制捷径卷积步幅\n",
    "        \"\"\"\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        F1, F2, F3 = filters  # 解包三个卷积层输出通道\n",
    "\n",
    "        # ------------------------------\n",
    "        # 主路径\n",
    "        # ------------------------------\n",
    "        # 第一层卷积 1x1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=F1, \n",
    "            kernel_size=1, \n",
    "            stride=s,    # 控制降采样\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "\n",
    "        # 第二层卷积 fxf，stride=1，padding SAME\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=F1, \n",
    "            out_channels=F2, \n",
    "            kernel_size=f, \n",
    "            stride=1, \n",
    "            padding=f//2, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "\n",
    "        # 第三层卷积 1x1\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=F2, \n",
    "            out_channels=F3, \n",
    "            kernel_size=1, \n",
    "            stride=1, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "\n",
    "        # ------------------------------\n",
    "        # 捷径分支 (shortcut)\n",
    "        # ------------------------------\n",
    "        # 卷积 1x1 用于匹配主路径输出通道和尺寸\n",
    "        self.shortcut_conv = nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=F3, \n",
    "            kernel_size=1, \n",
    "            stride=s,   # 与主路径第一层卷积步幅相同\n",
    "            bias=False\n",
    "        )\n",
    "        self.shortcut_bn = nn.BatchNorm2d(F3)\n",
    "\n",
    "        # ReLU 激活\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数：\n",
    "            x -- 输入 Tensor, shape = (batch_size, in_channels, H, W)\n",
    "\n",
    "        返回：\n",
    "            输出 Tensor, shape = (batch_size, F3, H_out, W_out)\n",
    "        \"\"\"\n",
    "        # ------------------------------\n",
    "        # 捷径路径\n",
    "        # ------------------------------\n",
    "        shortcut = self.shortcut_conv(x)\n",
    "        shortcut = self.shortcut_bn(shortcut)\n",
    "\n",
    "        # ------------------------------\n",
    "        # 主路径\n",
    "        # ------------------------------\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        # ------------------------------\n",
    "        # 尺寸对齐（可选）\n",
    "        # ------------------------------\n",
    "        # 当主路径和捷径尺寸不一致时，插值对齐\n",
    "        if out.shape != shortcut.shape:\n",
    "            shortcut = F.interpolate(shortcut, size=out.shape[2:], mode='nearest')\n",
    "\n",
    "        # ------------------------------\n",
    "        # 残差连接 + ReLU\n",
    "        # ------------------------------\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出尺寸: torch.Size([3, 6, 3, 3])\n",
      "out.flatten()[:6] = [0.         0.         0.20515898 0.         0.         0.19972707]\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 设置随机种子，保证实验可复现\n",
    "# =====================================\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# =====================================\n",
    "# 构造输入张量\n",
    "# =====================================\n",
    "# batch_size=3, 输入通道=6, 高=4, 宽=4\n",
    "X = torch.tensor(np.random.randn(3, 6, 4, 4), dtype=torch.float32)\n",
    "\n",
    "# =====================================\n",
    "# 定义卷积块\n",
    "# =====================================\n",
    "# 输入通道=6, 三层卷积通道=[2,4,6], 中间卷积核大小 f=2, 步幅 s=2\n",
    "conv_block = ConvolutionalBlock(in_channels=6, f=2, filters=[2,4,6], s=2)\n",
    "\n",
    "# 切换到评估模式（不计算梯度，不更新 BN 统计）\n",
    "conv_block.eval()\n",
    "\n",
    "# =====================================\n",
    "# 前向传播\n",
    "# =====================================\n",
    "out = conv_block(X)\n",
    "\n",
    "# =====================================\n",
    "# 打印输出\n",
    "# =====================================\n",
    "print(\"输出尺寸:\", out.shape)\n",
    "# 展平后的前6个元素\n",
    "print(\"out.flatten()[:6] =\", out.flatten()[:6].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - 构建你的第一个 50 层 ResNet 模型\n",
    "\n",
    "在前面我们已经实现了 **ResNet 的恒等块（Identity Block）** 和 **卷积块（Convolutional Block）**，现在可以构建一个非常深的网络了。  \n",
    "下图展示了 ResNet-50 的整体结构：\"ID BLOCK\" 表示 Identity Block，\"ID BLOCK x3\" 表示堆叠 3 个恒等块。\n",
    "\n",
    "<img src=\"images/resnet_kiank.png\" style=\"width:850px;height:150px;\">\n",
    "<caption><center> \n",
    "<u> <font color='purple'> **图 5** </u><font color='purple'> ： **ResNet-50 模型** </center></caption>\n",
    "\n",
    "---\n",
    "\n",
    "### ResNet-50 模型详细说明\n",
    "\n",
    "#### 输入层\n",
    "- Zero-padding：对输入图像进行填充，pad=(3,3)\n",
    "\n",
    "---\n",
    "\n",
    "#### Stage 1\n",
    "- Conv2D：\n",
    "  - 卷积核数量：64\n",
    "  - 卷积核大小：(7,7)\n",
    "  - 步幅：(2,2)\n",
    "- BatchNorm：对通道维归一化\n",
    "- MaxPooling：\n",
    "  - 窗口大小：(3,3)\n",
    "  - 步幅：(2,2)\n",
    "\n",
    "---\n",
    "\n",
    "#### Stage 2\n",
    "- **卷积块（Conv Block）**\n",
    "  - 卷积核组大小：[64,64,256]\n",
    "  - 卷积核大小 f=3\n",
    "  - 步幅 s=1\n",
    "  - 名称后缀：\"a\"\n",
    "- **恒等块（Identity Block）**\n",
    "  - 2 个恒等块\n",
    "  - 卷积核组大小：[64,64,256]\n",
    "  - 卷积核大小 f=3\n",
    "\n",
    "---\n",
    "\n",
    "#### Stage 3\n",
    "- 卷积块：\n",
    "  - 卷积核组大小：[128,128,512]\n",
    "  - f=3, s=2\n",
    "- 恒等块：\n",
    "  - 3 个恒等块\n",
    "  - 卷积核组大小：[128,128,512]\n",
    "  - f=3\n",
    "\n",
    "---\n",
    "\n",
    "#### Stage 4\n",
    "- 卷积块：\n",
    "  - 卷积核组大小：[256,256,1024]\n",
    "  - f=3, s=2\n",
    "- 恒等块：\n",
    "  - 5 个恒等块\n",
    "  - 卷积核组大小：[256,256,1024]\n",
    "  - f=3\n",
    "\n",
    "---\n",
    "\n",
    "#### Stage 5\n",
    "- 卷积块：\n",
    "  - 卷积核组大小：[512,512,2048]\n",
    "  - f=3, s=2\n",
    "- 恒等块：\n",
    "  - 2 个恒等块\n",
    "  - 卷积核组大小：[512,512,2048]\n",
    "  - f=3\n",
    "---\n",
    "\n",
    "#### 输出层\n",
    "- Average Pooling：\n",
    "  - 窗口大小：(2,2)\n",
    "- Flatten：无超参数，无名称\n",
    "- Fully Connected（Dense）层：\n",
    "  - 输出维度 = 类别数\n",
    "  - 激活函数：softmax\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> 小结：  \n",
    "> ResNet-50 由 5 个 stage 组成，每个 stage 由 1 个卷积块和若干恒等块组成。卷积块用于调整维度，恒等块用于学习更深的特征。最后通过平均池化和全连接层完成分类。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# ResNet50 主体定义\n",
    "# =====================================\n",
    "class ResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet50 网络结构（简化版，用于小型分类任务）\n",
    "    输入: (batch_size, 3, H, W)\n",
    "    输出: (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(ResNet50, self).__init__()\n",
    "\n",
    "        # ------------------------------\n",
    "        # Stage 1: 初始卷积 + BN + ReLU + MaxPool\n",
    "        # ------------------------------\n",
    "        self.pad = nn.ZeroPad2d(3)          # 四周填充 3 个像素 (left, right, top, bottom)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,                   # 输入通道数\n",
    "            out_channels=64,                 # 输出通道数\n",
    "            kernel_size=7,                   # 卷积核 7x7\n",
    "            stride=2,                        # 步幅 2\n",
    "            bias=False                        # 不使用偏置，BN 会有偏置\n",
    "        )\n",
    "        self.bn1   = nn.BatchNorm2d(64)     # BatchNorm 层，规范化 64 个通道\n",
    "        self.relu  = nn.ReLU(inplace=True)  # ReLU 激活\n",
    "        self.maxpool = nn.MaxPool2d(\n",
    "            kernel_size=3,                   # 池化窗口 3x3\n",
    "            stride=2,                        # 步幅 2\n",
    "            padding=1                        # SAME 填充\n",
    "        )\n",
    "\n",
    "        # ------------------------------\n",
    "        # Stage 2: 1 个卷积块 + 2 个恒等块\n",
    "        # ------------------------------\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ConvolutionalBlock(64,  [64, 64, 256], f=3, s=1),\n",
    "            IdentityBlock(256, [64, 64, 256], f=3),\n",
    "            IdentityBlock(256, [64, 64, 256], f=3)\n",
    "        )\n",
    "\n",
    "        # ------------------------------\n",
    "        # Stage 3: 1 个卷积块 + 3 个恒等块\n",
    "        # ------------------------------\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ConvolutionalBlock(256, [128, 128, 512], f=3, s=2),\n",
    "            IdentityBlock(512, [128, 128, 512], f=3),\n",
    "            IdentityBlock(512, [128, 128, 512], f=3),\n",
    "            IdentityBlock(512, [128, 128, 512], f=3)\n",
    "        )\n",
    "\n",
    "        # ------------------------------\n",
    "        # Stage 4: 1 个卷积块 + 5 个恒等块\n",
    "        # ------------------------------\n",
    "        self.stage4 = nn.Sequential(\n",
    "            ConvolutionalBlock(512, [256, 256, 1024], f=3, s=2),\n",
    "            IdentityBlock(1024, [256, 256, 1024], f=3),\n",
    "            IdentityBlock(1024, [256, 256, 1024], f=3),\n",
    "            IdentityBlock(1024, [256, 256, 1024], f=3),\n",
    "            IdentityBlock(1024, [256, 256, 1024], f=3),\n",
    "            IdentityBlock(1024, [256, 256, 1024], f=3)\n",
    "        )\n",
    "\n",
    "        # ------------------------------\n",
    "        # Stage 5: 1 个卷积块 + 2 个恒等块\n",
    "        # ------------------------------\n",
    "        self.stage5 = nn.Sequential(\n",
    "            ConvolutionalBlock(1024, [512, 512, 2048], f=3, s=2),\n",
    "            IdentityBlock(2048, [512, 512, 2048], f=3),\n",
    "            IdentityBlock(2048, [512, 512, 2048], f=3)\n",
    "        )\n",
    "\n",
    "        # ------------------------------\n",
    "        # 全局平均池化 + 全连接输出\n",
    "        # ------------------------------\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))  # 将每个通道池化成 1x1\n",
    "        self.fc = nn.Linear(2048, num_classes)      # 全连接层输出类别数\n",
    "\n",
    "    # ------------------------------\n",
    "    # 前向传播\n",
    "    # ------------------------------\n",
    "    def forward(self, x):\n",
    "        # Stage 1\n",
    "        x = self.pad(x)         # 填充\n",
    "        x = self.conv1(x)       # 卷积\n",
    "        x = self.bn1(x)         # BN\n",
    "        x = self.relu(x)        # ReLU\n",
    "        x = self.maxpool(x)     # 最大池化\n",
    "\n",
    "        # Stage 2~5\n",
    "        x = self.stage2(x)      # stage2 卷积块+恒等块\n",
    "        x = self.stage3(x)      # stage3\n",
    "        x = self.stage4(x)      # stage4\n",
    "        x = self.stage5(x)      # stage5\n",
    "\n",
    "        # 全局池化 + 展平 + 全连接\n",
    "        x = self.avgpool(x)            # 输出 (batch, 2048, 1, 1)\n",
    "        x = torch.flatten(x, 1)        # 展平成 (batch, 2048)\n",
    "        x = self.fc(x)                 # 全连接输出 (batch, num_classes)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的单元格，在 2 个 epoch 上训练你的模型，批量大小为 32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8993\n",
      "Epoch 2, Loss: 0.6568\n",
      "准确率 = 36.67%\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 训练和测试 ResNet50 示例\n",
    "# =====================================\n",
    "\n",
    "# ------------------------------\n",
    "# 设备选择\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 如果 GPU 可用，则使用 GPU，否则使用 CPU\n",
    "\n",
    "# ------------------------------\n",
    "# 模型实例化\n",
    "# ------------------------------\n",
    "model = ResNet50(num_classes=6).to(device)\n",
    "# 将模型移到设备（GPU/CPU）\n",
    "\n",
    "# ------------------------------\n",
    "# 损失函数和优化器\n",
    "# ------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 多分类交叉熵损失\n",
    "# 输入 logits (未经过 softmax) 形状 = (batch_size, num_classes)\n",
    "# 标签为类别索引 (long 类型)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Adam 优化器，自适应学习率\n",
    "# model.parameters() 获取所有可训练参数\n",
    "# lr=0.001 为初始学习率\n",
    "\n",
    "# ------------------------------\n",
    "# 简单训练循环（2 个 epoch 示例）\n",
    "# ------------------------------\n",
    "for epoch in range(2):\n",
    "    model.train()  # 设置模型为训练模式（启用 BN/Dropout）\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)  # 移动数据到设备\n",
    "\n",
    "        outputs = model(images)                # 前向传播，得到 logits\n",
    "        loss = criterion(outputs, labels)     # 计算损失\n",
    "\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward()        # 反向传播计算梯度\n",
    "        optimizer.step()       # 更新参数\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    # 打印当前 epoch 最后一个 batch 的损失\n",
    "\n",
    "# ------------------------------\n",
    "# 测试集评估\n",
    "# ------------------------------\n",
    "model.eval()  # 设置模型为评估模式（禁用 BN/Dropout）\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():  # 不计算梯度，节省内存\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)                     # 前向传播\n",
    "        _, preds = torch.max(outputs, 1)           # 取最大值索引作为预测类别\n",
    "        total += labels.size(0)                     # 累加样本总数\n",
    "        correct += (preds == labels).sum().item()  # 累加正确预测数\n",
    "\n",
    "# 打印准确率\n",
    "print(\"准确率 = {:.2f}%\".format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 查看模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50(\n",
      "  (pad): ZeroPad2d((3, 3, 3, 3))\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (stage2): Sequential(\n",
      "    (0): ConvolutionalBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (shortcut_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): IdentityBlock(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): IdentityBlock(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (stage3): Sequential(\n",
      "    (0): ConvolutionalBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut_conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (shortcut_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): IdentityBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): IdentityBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): IdentityBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (stage4): Sequential(\n",
      "    (0): ConvolutionalBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut_conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (shortcut_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): IdentityBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): IdentityBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): IdentityBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): IdentityBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): IdentityBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (stage5): Sequential(\n",
      "    (0): ConvolutionalBlock(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut_conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (shortcut_bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): IdentityBlock(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): IdentityBlock(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50()  \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \r\n",
    "**你应该记住的内容：**\r\n",
    "- 在实践中，非常深的“普通”网络效果不佳，因为梯度容易消失，训练困难。  \r\n",
    "- 跳跃连接（skip-connections）有助于解决梯度消失问题，同时也使 ResNet 块更容易学习恒等映射（identity function）。\r\n",
    "- ResNet 有两种主要类型的块：恒等块（identity block）和卷积块（convolutional block）。\r\n",
    "- 非常深的残差网络（Residual Networks）是通过堆叠这些块构建起来的。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献\r\n",
    "\r\n",
    "本笔记本介绍了 He 等人（2015）的 ResNet 算法。实现过程中也参考了 Francois Chollet 的 GitHub 仓库，并遵循了其结构：\r\n",
    "\r\n",
    "- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - [Deep Residual Learning for Image Recognition (2015)](https://arxiv.org/abs/1512.03385)\r\n",
    "- Francois Chollet 的 GitHub 仓库: https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\r\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OEpi5",
   "launcher_item_id": "jK9EQ"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
