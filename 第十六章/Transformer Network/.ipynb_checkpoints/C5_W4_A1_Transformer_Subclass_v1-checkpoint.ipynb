{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbzZLqIPv6b7",
    "outputId": "19f2fc2b-6f1d-4b43-fd50-4c513e3936fd"
   },
   "source": [
    "# Transformer ç½‘ç»œ\n",
    "\n",
    "åœ¨è¯¾ç¨‹çš„å‰é¢éƒ¨åˆ†ï¼Œä½ å·²ç»å®ç°äº†é¡ºåºç¥ç»ç½‘ç»œï¼ˆsequential neural networksï¼‰ï¼Œå¦‚ RNNã€GRU å’Œ LSTMã€‚  \n",
    "åœ¨æœ¬ç¬”è®°ä¸­ï¼Œä½ å°†æ¢ç´¢ **Transformer æ¶æ„**ï¼Œè¿™æ˜¯ä¸€ç§å¯ä»¥åˆ©ç”¨å¹¶è¡Œå¤„ç†å¹¶æ˜¾è‘—åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹çš„ç¥ç»ç½‘ç»œã€‚\n",
    "\n",
    "**å®Œæˆæœ¬ä½œä¸šåï¼Œä½ å°†èƒ½å¤Ÿï¼š**\n",
    "\n",
    "* åˆ›å»ºä½ç½®ç¼–ç ï¼ˆpositional encodingsï¼‰ä»¥æ•æ‰æ•°æ®ä¸­çš„åºåˆ—å…³ç³»\n",
    "* ä½¿ç”¨è¯å‘é‡è®¡ç®—ç¼©æ”¾ç‚¹ç§¯è‡ªæ³¨æ„åŠ›ï¼ˆscaled dot-product self-attentionï¼‰\n",
    "* å®ç°æ©ç å¤šå¤´æ³¨æ„åŠ›ï¼ˆmasked multi-head attentionï¼‰\n",
    "* æ„å»ºå¹¶è®­ç»ƒä¸€ä¸ª Transformer æ¨¡å‹\n",
    "\n",
    "æœ€åä¸€æ¬¡ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç›®å½•\n",
    "\n",
    "- [Packages](#0) è½¯ä»¶åŒ…\n",
    "- [1 - ä½ç½®ç¼–ç  (Positional Encoding)](#1)\n",
    "    - [1.1 - æ­£å¼¦å’Œä½™å¼¦è§’åº¦ (Sine and Cosine Angles)](#1-1)\n",
    "        - [ç»ƒä¹  1 - get_angles](#ex-1)\n",
    "    - [1.2 - æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç  (Sine and Cosine Positional Encodings)](#1-2)\n",
    "        - [ç»ƒä¹  2 - positional_encoding](#ex-2)\n",
    "- [2 - Masking æ©ç ](#2)\n",
    "    - [2.1 - å¡«å……æ©ç  (Padding Mask)](#2-1)\n",
    "    - [2.2 - å‰ç»æ©ç  (Look-ahead Mask)](#2-2)\n",
    "- [3 - è‡ªæ³¨æ„åŠ› (Self-Attention)](#3)\n",
    "    - [ç»ƒä¹  3 - scaled_dot_product_attention](#ex-3)\n",
    "- [4 - ç¼–ç å™¨ (Encoder)](#4)\n",
    "    - [4.1 ç¼–ç å™¨å±‚ (Encoder Layer)](#4-1)\n",
    "        - [ç»ƒä¹  4 - EncoderLayer](#ex-4)\n",
    "    - [4.2 å®Œæ•´ç¼–ç å™¨ (Full Encoder)](#4-2)\n",
    "        - [ç»ƒä¹  5 - Encoder](#ex-5)\n",
    "- [5 - è§£ç å™¨ (Decoder)](#5)\n",
    "    - [5.1 è§£ç å™¨å±‚ (Decoder Layer)](#5-1)\n",
    "        - [ç»ƒä¹  6 - DecoderLayer](#ex-6)\n",
    "    - [5.2 å®Œæ•´è§£ç å™¨ (Full Decoder)](#5-2)\n",
    "        - [ç»ƒä¹  7 - Decoder](#ex-7)\n",
    "- [6 - Transformer](#6)\n",
    "    - [ç»ƒä¹  8 - Transformer](#ex-8)\n",
    "- [7 - å‚è€ƒæ–‡çŒ® (References)](#7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## è½¯ä»¶åŒ… (Packages)\n",
    "\n",
    "è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒä»¥åŠ è½½æ‰€éœ€çš„è½¯ä»¶åŒ…ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_OpwqWL2QH5G"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# å¯¼å…¥ PyTorch åŠç›¸å…³åº“\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# å¯¼å…¥ PyTorch ä¸»åº“\n",
    "# PyTorch æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¼ é‡è®¡ç®—å’Œç¥ç»ç½‘ç»œæ„å»º\n",
    "import torch\n",
    "\n",
    "# å¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆnnï¼‰\n",
    "# å…¶ä¸­åŒ…å«äº†å„ç§å¸¸è§çš„å±‚ï¼ˆå¦‚å·ç§¯å±‚ã€çº¿æ€§å±‚ã€åµŒå…¥å±‚ç­‰ï¼‰\n",
    "import torch.nn as nn\n",
    "\n",
    "# å¯¼å…¥å‡½æ•°å¼æ¥å£æ¨¡å—ï¼ˆnn.functionalï¼‰\n",
    "# æä¾›æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ã€å·ç§¯è¿ç®—ç­‰åº•å±‚æ“ä½œçš„å‡½æ•°å¼è°ƒç”¨\n",
    "# ä¸ nn.Module ä¸åŒï¼ŒF ä¸­çš„å‡½æ•°ä¸ä¼šè‡ªåŠ¨ä¿å­˜å‚æ•°\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# å¯¼å…¥æ•°å­¦åº“ math\n",
    "# ç”¨äºæ‰§è¡ŒåŸºç¡€æ•°å­¦æ“ä½œï¼ˆå¦‚å¼€æ–¹ã€å¯¹æ•°ã€ä¸‰è§’å‡½æ•°ã€å¹‚è¿ç®—ç­‰ï¼‰\n",
    "import math\n",
    "\n",
    "# å¯¼å…¥ numpy æ•°å€¼è®¡ç®—åº“\n",
    "# numpy æ˜¯ç§‘å­¦è®¡ç®—çš„åŸºç¡€åº“ï¼Œç”¨äºæ“ä½œå¤šç»´æ•°ç»„ã€çŸ©é˜µåŠçº¿æ€§ä»£æ•°\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - ä½ç½®ç¼–ç  (Positional Encoding)\n",
    "\n",
    "åœ¨åºåˆ—åˆ°åºåˆ—ï¼ˆsequence-to-sequenceï¼‰ä»»åŠ¡ä¸­ï¼Œæ•°æ®çš„ç›¸å¯¹é¡ºåºå¯¹å…¶å«ä¹‰éå¸¸é‡è¦ã€‚  \n",
    "å½“ä½ è®­ç»ƒé¡ºåºç¥ç»ç½‘ç»œï¼ˆå¦‚ RNNï¼‰æ—¶ï¼Œè¾“å…¥æ•°æ®æŒ‰é¡ºåºä¼ å…¥ç½‘ç»œï¼Œæ¨¡å‹è‡ªåŠ¨è·å¾—æ•°æ®é¡ºåºçš„ä¿¡æ¯ã€‚  \n",
    "\n",
    "ç„¶è€Œï¼Œå½“ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼ˆmulti-head attentionï¼‰è®­ç»ƒ Transformer ç½‘ç»œæ—¶ï¼Œä½ ä¼šä¸€æ¬¡æ€§å°†æ‰€æœ‰æ•°æ®è¾“å…¥æ¨¡å‹ã€‚  \n",
    "è™½ç„¶è¿™æ ·å¤§å¤§å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼Œä½†æ¨¡å‹æ— æ³•ç›´æ¥è·å–æ•°æ®é¡ºåºä¿¡æ¯ã€‚  \n",
    "\n",
    "è¿™æ—¶ï¼Œ**ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰**å°±éå¸¸æœ‰ç”¨â€”â€”ä½ å¯ä»¥ä¸“é—¨å¯¹è¾“å…¥ä½ç½®è¿›è¡Œç¼–ç ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹æ­£å¼¦å’Œä½™å¼¦å…¬å¼å°†å…¶ä¼ å…¥ç½‘ç»œï¼š\n",
    "    \n",
    "$$\n",
    "PE_{(pos, 2i)}= \\sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{1}$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= \\cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{2}$$\n",
    "\n",
    "* $d$ ä¸ºè¯å‘é‡ï¼ˆword embeddingï¼‰å’Œä½ç½®ç¼–ç çš„ç»´åº¦\n",
    "* $pos$ ä¸ºå•è¯çš„ä½ç½®\n",
    "* $i$ è¡¨ç¤ºä½ç½®ç¼–ç çš„å„ä¸ªç»´åº¦\n",
    "\n",
    "ä¸ºäº†ç†è§£ä½ç½®ç¼–ç ï¼Œä½ å¯ä»¥å°†å…¶çœ‹ä½œåŒ…å«å•è¯ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„ç‰¹å¾ã€‚  \n",
    "æœ€ç»ˆï¼Œ**è¯å‘é‡ä¸ä½ç½®ç¼–ç çš„å’Œ**ä¼šè¢«è¾“å…¥æ¨¡å‹ã€‚  \n",
    "\n",
    "å¦‚æœä½ åªæ˜¯ç¡¬ç¼–ç ä½ç½®ï¼Œæ¯”å¦‚å°†ä¸€ä¸ªå…¨ 1 æˆ–æ•´æ•°çŸ©é˜µç›´æ¥åŠ åˆ°è¯å‘é‡ä¸Šï¼Œä¼šæ‰­æ›²è¯­ä¹‰ä¿¡æ¯ã€‚  \n",
    "è€Œæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å€¼åœ¨ [-1, 1] ä¹‹é—´ï¼ŒåŠ å…¥åˆ°è¯å‘é‡åï¼Œä¸ä¼šæ˜¾è‘—æ‰­æ›²è¯å‘é‡ï¼Œè€Œæ˜¯ä¸ºå…¶å¢åŠ äº†ä½ç½®ä¿¡æ¯ã€‚  \n",
    "ä½¿ç”¨è¿™ä¸¤ä¸ªå…¬å¼çš„ç»„åˆï¼Œå¯ä»¥å¸®åŠ© Transformer ç½‘ç»œå…³æ³¨è¾“å…¥æ•°æ®çš„ç›¸å¯¹ä½ç½®ã€‚  \n",
    "\n",
    "> è¿™åªæ˜¯å¯¹ä½ç½®ç¼–ç çš„ç®€è¦ä»‹ç»ï¼Œè‹¥æƒ³è¿›ä¸€æ­¥ç†è§£ï¼Œå¯ä»¥å‚è€ƒ *Positional Encoding Ungraded Lab*ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼š** åœ¨è¯¾ç¨‹è®²è§£ä¸­ Andrew ä½¿ç”¨çš„æ˜¯ç«–å‘å‘é‡ï¼Œä½†æœ¬ä½œä¸šä¸­æ‰€æœ‰å‘é‡å‡ä¸ºæ¨ªå‘ã€‚æ‰€æœ‰çŸ©é˜µä¹˜æ³•è¯·ç›¸åº”è°ƒæ•´ã€‚\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - æ­£å¼¦ä¸ä½™å¼¦è§’åº¦ (Sine and Cosine Angles)\n",
    "\n",
    "æ³¨æ„ï¼Œå°½ç®¡æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç å…¬å¼è¾“å…¥çš„å‚æ•°ä¸åŒï¼ˆ`2i` ä¸ `2i+1`ï¼Œå¶æ•°ä¸å¥‡æ•°ï¼‰ï¼Œä½†å…¬å¼çš„å†…éƒ¨é¡¹ç›¸åŒï¼š\n",
    "$$\\theta(pos, i, d) = \\frac{pos}{10000^{\\frac{2i}{d}}} \\tag{3}$$\n",
    "\n",
    "åœ¨è®¡ç®—åºåˆ—ä¸­æŸä¸ªå•è¯çš„ä½ç½®ç¼–ç æ—¶ï¼Œå¯ä»¥å‚è€ƒå†…éƒ¨é¡¹ï¼š  \n",
    "$PE_{(pos, 0)}= \\sin\\left(\\frac{pos}{{10000}^{0/d}}\\right)$ï¼Œå› ä¸º `2i = 0` å¾— `i = 0`  \n",
    "$PE_{(pos, 1)}= \\cos\\left(\\frac{pos}{{10000}^{0/d}}\\right)$ï¼Œå› ä¸º `2i + 1 = 1` å¾— `i = 0`  \n",
    "\n",
    "ä¸¤è€…çš„è§’åº¦ç›¸åŒï¼  \n",
    "$PE_{(pos, 2)}$ å’Œ $PE_{(pos, 3)}$ çš„è§’åº¦ä¹Ÿç›¸åŒï¼Œå› ä¸º `i = 1`ï¼Œæ‰€ä»¥å†…éƒ¨é¡¹ä¸º $\\frac{pos}{{10000}^{1/d}}$ã€‚  \n",
    "è¿™ç§å…³ç³»å¯¹æ‰€æœ‰æˆå¯¹çš„æ­£å¼¦å’Œä½™å¼¦æ›²çº¿éƒ½æˆç«‹ï¼š\n",
    "\n",
    "|      k         | <code>0</code> | <code>1</code> | <code>2</code> | <code>3</code> | <code>...</code> | <code>d-2</code> | <code>d-1</code> |\n",
    "|----------------|:---------------:|:---------------:|:---------------:|:---------------:|:----------------:|:----------------:|:----------------:|\n",
    "| encoding(0) =  | [$\\sin(\\theta(0,0,d))$ | $\\cos(\\theta(0,0,d))$ | $\\sin(\\theta(0,1,d))$ | $\\cos(\\theta(0,1,d))$ | ... | $\\sin(\\theta(0,d//2,d))$ | $\\cos(\\theta(0,d//2,d))$] |\n",
    "| encoding(1) =  | [$\\sin(\\theta(1,0,d))$ | $\\cos(\\theta(1,0,d))$ | $\\sin(\\theta(1,1,d))$ | $\\cos(\\theta(1,1,d))$ | ... | $\\sin(\\theta(1,d//2,d))$ | $\\cos(\\theta(1,d//2,d))$] |\n",
    "| ...            | ...             | ...             | ...             | ...             | ...              | ...              | ...              |\n",
    "| encoding(pos) =| [$\\sin(\\theta(pos,0,d))$ | $\\cos(\\theta(pos,0,d))$ | $\\sin(\\theta(pos,1,d))$ | $\\cos(\\theta(pos,1,d))$ | ... | $\\sin(\\theta(pos,d//2,d))$ | $\\cos(\\theta(pos,d//2,d))$] |\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### ç»ƒä¹  1 - get_angles\n",
    "\n",
    "å®ç° `get_angles()` å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç å¯èƒ½çš„è§’åº¦ã€‚\n",
    "\n",
    "**æç¤ºï¼š**\n",
    "\n",
    "- å¦‚æœ `k = [0, 1, 2, 3, 4, 5]`ï¼Œé‚£ä¹ˆ `i` åº”ä¸º `i = [0, 0, 1, 1, 2, 2]`\n",
    "- `i = k//2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bPzwMVfcQpT-"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION get_angles\n",
    "# ---------------------------------------------------------\n",
    "def get_angles(pos, k, d):\n",
    "    \"\"\"\n",
    "    åŠŸèƒ½ï¼š\n",
    "        æ ¹æ® Transformer çš„ä½ç½®ç¼–ç å…¬å¼ï¼Œè®¡ç®—æ¯ä¸ªä½ç½® (pos)\n",
    "        å’Œæ¯ä¸ªç»´åº¦ (k) å¯¹åº”çš„è§’åº¦å€¼çŸ©é˜µã€‚\n",
    "        \n",
    "    å‚æ•°è¯´æ˜ï¼š\n",
    "        pos -- ä½ç½®å‘é‡ï¼ˆåˆ—å‘é‡å½¢å¼ï¼‰ï¼Œå½¢çŠ¶ä¸º [[0], [1], [2], ..., [N-1]]\n",
    "                è¡¨ç¤ºå¥å­ä¸­æ¯ä¸ªå•è¯çš„ä½ç½®ç´¢å¼•ã€‚\n",
    "        k   -- ç»´åº¦ç´¢å¼•å‘é‡ï¼ˆè¡Œå‘é‡å½¢å¼ï¼‰ï¼Œå½¢çŠ¶ä¸º [[0, 1, 2, ..., d-1]]\n",
    "                è¡¨ç¤ºç¼–ç ç»´åº¦çš„åºå·ã€‚\n",
    "        d   -- æ•´æ•°ï¼Œç¼–ç ç»´åº¦ï¼ˆä¾‹å¦‚ d_model = 512 æˆ– 100ï¼‰\n",
    "        \n",
    "    è¿”å›å€¼ï¼š\n",
    "        angles -- ä¸€ä¸ªå½¢çŠ¶ä¸º (pos, d) çš„ numpy æ•°ç»„ï¼Œ\n",
    "                  æ¯ä¸ªå…ƒç´ è¡¨ç¤ºä½ç½® pos åœ¨ç¬¬ k ä¸ªç»´åº¦ä¸Šçš„è§’åº¦å€¼ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 1. è®¡ç®—ç»´åº¦ç´¢å¼• i\n",
    "    # ----------------------------\n",
    "    # åœ¨åŸå§‹ Transformer è®ºæ–‡ä¸­ï¼Œ\n",
    "    # å¶æ•°ç»´ (2i) ä½¿ç”¨ sin å‡½æ•°ï¼Œå¥‡æ•°ç»´ (2i+1) ä½¿ç”¨ cos å‡½æ•°ã€‚\n",
    "    # å› æ­¤è¿™é‡Œä½¿ç”¨ k//2ï¼ˆæ•´é™¤ 2ï¼‰æ¥å¾—åˆ°æ¯ä¸ªç»´åº¦æ‰€å±çš„å¶æ•°ç´¢å¼• iã€‚\n",
    "    i = k // 2\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. æŒ‰ç…§è®ºæ–‡å…¬å¼è®¡ç®—è§’åº¦çŸ©é˜µ\n",
    "    # ----------------------------\n",
    "    # å…¬å¼ï¼šangle(pos, 2i) = pos / (10000^(2i / d))\n",
    "    # è¿™é‡Œä½¿ç”¨ numpy çš„å¹‚è¿ç®—ï¼šnp.power(10000, 2 * i / d)\n",
    "    # ç„¶åæ¯ä¸ªä½ç½® pos é™¤ä»¥å¯¹åº”çš„å¹‚æ¬¡ç»“æœï¼Œå¾—åˆ° (pos Ã— d) çš„è§’åº¦çŸ©é˜µã€‚\n",
    "    #\n",
    "    # ä¾‹å¦‚ï¼š\n",
    "    # pos = [[0], [1], [2]]\n",
    "    # k = [[0, 1, 2, 3]]\n",
    "    # d = 4\n",
    "    # åˆ™è¿”å›çš„ angles æ˜¯ä¸€ä¸ª 3Ã—4 çŸ©é˜µï¼Œ\n",
    "    # è¡¨ç¤º 3 ä¸ªä½ç½®åœ¨ 4 ä¸ªç»´åº¦ä¸Šçš„è§’åº¦å€¼ã€‚\n",
    "    angles = pos / np.power(10000, 2 * i / d)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 3. è¿”å›è§’åº¦çŸ©é˜µ\n",
    "    # ----------------------------\n",
    "    # è¯¥çŸ©é˜µå°†åœ¨åç»­æ­¥éª¤ä¸­åˆ†åˆ«åº”ç”¨ sin() å’Œ cos() å‡½æ•°ï¼Œ\n",
    "    # ä»è€Œç”Ÿæˆå®Œæ•´çš„æ­£å¼¦-ä½™å¼¦ä½ç½®ç¼–ç ã€‚\n",
    "    return angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è§’åº¦çŸ©é˜µ angles çš„å½¢çŠ¶: (4, 8)\n",
      "[[0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]\n",
      " [1.e+00 1.e+00 1.e-01 1.e-01 1.e-02 1.e-02 1.e-03 1.e-03]\n",
      " [2.e+00 2.e+00 2.e-01 2.e-01 2.e-02 2.e-02 2.e-03 2.e-03]\n",
      " [3.e+00 3.e+00 3.e-01 3.e-01 3.e-02 3.e-02 3.e-03 3.e-03]]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ç¤ºä¾‹ï¼šè°ƒç”¨ get_angles å‡½æ•°ï¼Œç”Ÿæˆè§’åº¦çŸ©é˜µ\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. è®¾ç½®åºåˆ—é•¿åº¦ï¼ˆpositionï¼‰\n",
    "# ---------------------------------------------------------\n",
    "# è¡¨ç¤ºä¸€å…±æœ‰ 4 ä¸ªä½ç½®ï¼ˆå³å¥å­ä¸­æœ‰ 4 ä¸ª tokenï¼‰ã€‚\n",
    "# ä¾‹å¦‚å¥å­ [\"I\", \"like\", \"deep\", \"learning\"]ã€‚\n",
    "position = 4\n",
    "\n",
    "# 2. è®¾ç½®ç¼–ç ç»´åº¦ï¼ˆd_modelï¼‰\n",
    "# ---------------------------------------------------------\n",
    "# è¡¨ç¤ºæ¯ä¸ªå•è¯çš„ç¼–ç ç»´åº¦ä¸º 8ã€‚\n",
    "# è¿™ä¸ Transformer ä¸­ â€œè¯å‘é‡ç»´åº¦â€ æˆ– â€œéšè—å±‚ç»´åº¦â€ çš„æ¦‚å¿µç›¸åŒã€‚\n",
    "d_model = 8\n",
    "\n",
    "# 3. ç”Ÿæˆä½ç½®çŸ©é˜µ pos_m\n",
    "# ---------------------------------------------------------\n",
    "# np.arange(position) ä¼šç”Ÿæˆä¸€ä¸ªä¸€ç»´æ•°ç»„ [0, 1, 2, 3]\n",
    "# ä»£è¡¨æ¯ä¸ªå•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ã€‚\n",
    "# [:, np.newaxis] ç”¨äºå°†å…¶ä»å½¢çŠ¶ (4,) å˜ä¸º (4, 1)ï¼Œ\n",
    "# å³å˜ä¸ºâ€œåˆ—å‘é‡â€å½¢å¼ï¼Œä»¥ä¾¿ä¸ç»´åº¦çŸ©é˜µè¿›è¡Œå¹¿æ’­è®¡ç®—ã€‚\n",
    "pos_m = np.arange(position)[:, np.newaxis]\n",
    "\n",
    "# æ‰“å°éªŒè¯\n",
    "# print(pos_m)\n",
    "# è¾“å‡ºï¼š\n",
    "# [[0]\n",
    "#  [1]\n",
    "#  [2]\n",
    "#  [3]]\n",
    "\n",
    "# 4. ç”Ÿæˆç»´åº¦çŸ©é˜µ dims\n",
    "# ---------------------------------------------------------\n",
    "# np.arange(d_model) ç”Ÿæˆ [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "# ä»£è¡¨æ¯ä¸ªç¼–ç ç»´åº¦çš„ç´¢å¼•ã€‚\n",
    "# [np.newaxis, :] å°†å…¶å˜ä¸º (1, 8) çš„â€œè¡Œå‘é‡â€ï¼Œ\n",
    "# ä»¥ä¾¿ä¸ (4, 1) çš„ pos_m åœ¨è®¡ç®—æ—¶è‡ªåŠ¨å¹¿æ’­æˆ (4, 8)ã€‚\n",
    "dims = np.arange(d_model)[np.newaxis, :]\n",
    "\n",
    "# æ‰“å°éªŒè¯\n",
    "# print(dims)\n",
    "# è¾“å‡ºï¼š\n",
    "# [[0 1 2 3 4 5 6 7]]\n",
    "\n",
    "# 5. è°ƒç”¨ get_angles å‡½æ•°\n",
    "# ---------------------------------------------------------\n",
    "# ä¼ å…¥ä½ç½®çŸ©é˜µ pos_mï¼ˆå½¢çŠ¶ 4Ã—1ï¼‰ã€ç»´åº¦çŸ©é˜µ dimsï¼ˆå½¢çŠ¶ 1Ã—8ï¼‰ã€ç»´åº¦æ•° d_model=8\n",
    "# å‡½æ•°å†…éƒ¨ä¼šæ‰§è¡Œå¹¿æ’­æœºåˆ¶ï¼š\n",
    "#   pos_m çš„å½¢çŠ¶ (4,1)\n",
    "#   dims  çš„å½¢çŠ¶ (1,8)\n",
    "# è‡ªåŠ¨æ‰©å±•æˆ (4,8) çŸ©é˜µï¼Œé€å…ƒç´ è®¡ç®—è§’åº¦å…¬å¼ï¼š\n",
    "#   angle(pos, 2i) = pos / 10000^(2i / d_model)\n",
    "angles = get_angles(pos_m, dims, d_model)\n",
    "\n",
    "# 6. æ‰“å°ç»“æœ\n",
    "print(\"è§’åº¦çŸ©é˜µ angles çš„å½¢çŠ¶:\", angles.shape)\n",
    "print(angles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - æ­£å¼¦ä¸ä½™å¼¦ä½ç½®ç¼–ç  (Sine and Cosine Positional Encodings)\n",
    "\n",
    "ç°åœ¨ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¹‹å‰è®¡ç®—çš„è§’åº¦æ¥è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç ã€‚\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= \\sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= \\cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### ç»ƒä¹  2 - positional_encoding\n",
    "\n",
    "å®ç°å‡½æ•° `positional_encoding()` æ¥è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç ã€‚\n",
    "\n",
    "**æé†’ï¼š** å½“ $i$ ä¸ºå¶æ•°æ—¶ä½¿ç”¨æ­£å¼¦å…¬å¼ï¼Œå½“ $i$ ä¸ºå¥‡æ•°æ—¶ä½¿ç”¨ä½™å¼¦å…¬å¼ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y78txxoHQtwG"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# ä½ç½®ç¼–ç å‡½æ•°ï¼ˆPositional Encodingï¼‰\n",
    "# ----------------------------\n",
    "def positional_encoding(positions, d_model):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆ Transformer ä½¿ç”¨çš„ä½ç½®ç¼–ç çŸ©é˜µ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        positions (int): åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼Œå³å¥å­ä¸­ token çš„æ•°é‡\n",
    "        d_model (int): æ¯ä¸ª token çš„å‘é‡ç»´åº¦ï¼ˆembedding ç»´åº¦ï¼‰\n",
    "        \n",
    "    è¿”å›:\n",
    "        pos_encoding (torch.Tensor): ä½ç½®ç¼–ç ï¼Œå½¢çŠ¶ä¸º (1, positions, d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1. åˆ›å»ºä½ç½®ç´¢å¼•å‘é‡\n",
    "    # ----------------------------\n",
    "    # torch.arange(positions) -> [0, 1, 2, ..., positions-1]\n",
    "    # unsqueeze(1) -> ä»å½¢çŠ¶ [positions] å˜ä¸º [positions, 1]ï¼Œä¾¿äºå¹¿æ’­è®¡ç®—\n",
    "    # float() -> è½¬ä¸ºæµ®ç‚¹æ•°å¼ é‡\n",
    "    pos = torch.arange(positions).unsqueeze(1).float()       # [positions, 1]\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. åˆ›å»ºç»´åº¦ç´¢å¼•å‘é‡\n",
    "    # ----------------------------\n",
    "    # torch.arange(d_model) -> [0, 1, 2, ..., d_model-1]\n",
    "    # unsqueeze(0) -> ä»å½¢çŠ¶ [d_model] å˜ä¸º [1, d_model]ï¼Œä¾¿äºå¹¿æ’­è®¡ç®—\n",
    "    dims = torch.arange(d_model).unsqueeze(0).float()        # [1, d_model]\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. è®¡ç®—è§’åº¦çŸ©é˜µ\n",
    "    # ----------------------------\n",
    "    # ä½¿ç”¨è‡ªå®šä¹‰ get_angles å‡½æ•°ï¼š\n",
    "    # å¯¹æ¯ä¸ªä½ç½® pos[i] ä¸æ¯ä¸ªç»´åº¦ dims[j] è®¡ç®—å…¬å¼ï¼š\n",
    "    # angle_rads[i,j] = pos[i] / (10000^(2*(j//2)/d_model))\n",
    "    # å¾—åˆ°ä½ç½®ä¸ç»´åº¦å¯¹åº”çš„åŸå§‹è§’åº¦\n",
    "    angle_rads = get_angles(pos, dims, d_model)             # [positions, d_model]\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. å¶æ•°ç»´ä½¿ç”¨æ­£å¼¦å‡½æ•°ç¼–ç \n",
    "    # ----------------------------\n",
    "    # è§’åº¦çŸ©é˜µçš„å¶æ•°åˆ—ï¼ˆ0,2,4,...ï¼‰åº”ç”¨ sin å‡½æ•°\n",
    "    angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. å¥‡æ•°ç»´ä½¿ç”¨ä½™å¼¦å‡½æ•°ç¼–ç \n",
    "    # ----------------------------\n",
    "    # è§’åº¦çŸ©é˜µçš„å¥‡æ•°åˆ—ï¼ˆ1,3,5,...ï¼‰åº”ç”¨ cos å‡½æ•°\n",
    "    angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6. å¢åŠ  batch ç»´åº¦\n",
    "    # ----------------------------\n",
    "    # unsqueeze(0) -> [1, positions, d_model]ï¼Œç¬¦åˆ Transformer è¾“å…¥è¦æ±‚\n",
    "    pos_encoding = angle_rads.unsqueeze(0)                  # [1, positions, d_model]\n",
    "\n",
    "    # è¿”å›æœ€ç»ˆä½ç½®ç¼–ç å¼ é‡\n",
    "    return pos_encoding  # PyTorch Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_encoding shape: torch.Size([1, 50, 512])\n",
      "ç¬¬0ä¸ªä½ç½®ç¼–ç å‘é‡: tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "ç¬¬10ä¸ªä½ç½®ä¸ç¬¬11ä¸ªä½ç½®çš„è·ç¦»: 3.7142698764801025\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯•ä½ç½®ç¼–ç \n",
    "# ----------------------------\n",
    "\n",
    "# è®¾ç½®åºåˆ—é•¿åº¦å’Œ embedding ç»´åº¦\n",
    "positions = 50      # åºåˆ—é•¿åº¦ï¼šå¥å­ä¸­ token çš„æ•°é‡\n",
    "d_model = 512       # æ¯ä¸ª token çš„å‘é‡ç»´åº¦ï¼ˆembedding ç»´åº¦ï¼‰\n",
    "\n",
    "# è°ƒç”¨å‰é¢å®šä¹‰çš„ä½ç½®ç¼–ç å‡½æ•°\n",
    "pos_enc = positional_encoding(positions, d_model)  # è¿”å›å½¢çŠ¶ [1, positions, d_model]\n",
    "\n",
    "# ----------------------------\n",
    "# è¾“å‡ºä½ç½®ç¼–ç çš„å½¢çŠ¶\n",
    "# ----------------------------\n",
    "# pos_enc.shape -> [1, positions, d_model]\n",
    "print(\"positional_encoding shape:\", pos_enc.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# æŸ¥çœ‹ç¬¬ 0 ä¸ªä½ç½®çš„ç¼–ç å‘é‡\n",
    "# ----------------------------\n",
    "# pos_enc[0,0,:] -> ç¬¬ 0 ä¸ª token çš„ç¼–ç å‘é‡ï¼Œé•¿åº¦ä¸º d_model\n",
    "print(\"ç¬¬0ä¸ªä½ç½®ç¼–ç å‘é‡:\", pos_enc[0,0,:])\n",
    "\n",
    "# ----------------------------\n",
    "# è®¡ç®—ç¬¬ 10 ä¸ªä½ç½®ä¸ç¬¬ 11 ä¸ªä½ç½®ç¼–ç å‘é‡çš„æ¬§æ°è·ç¦»\n",
    "# ----------------------------\n",
    "# torch.norm(tensor) -> è®¡ç®—å¼ é‡çš„ L2 èŒƒæ•°ï¼ˆæ¬§æ°è·ç¦»ï¼‰\n",
    "# pos_enc[0,10,:]-pos_enc[0,11,:] -> ä¸¤ä¸ªä½ç½®ç¼–ç å‘é‡çš„å·®\n",
    "distance = torch.norm(pos_enc[0,10,:] - pos_enc[0,11,:]).item()  # .item() å°†æ ‡é‡å¼ é‡è½¬ä¸º Python float\n",
    "print(\"ç¬¬10ä¸ªä½ç½®ä¸ç¬¬11ä¸ªä½ç½®çš„è·ç¦»:\", distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¾ˆå¥½ï¼Œä½ å·²ç»æˆåŠŸè®¡ç®—å‡ºä½ç½®ç¼–ç ï¼ç°åœ¨ï¼Œä½ å¯ä»¥å°†å®ƒä»¬å¯è§†åŒ–ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªä½ç½®ç¼–ç â€”â€”æ³¨æ„æ¯ä¸€è¡Œéƒ½ä¸ç›¸åŒï¼ä½ ä¸ºæ¯ä¸ªå•è¯åˆ›å»ºäº†ç‹¬ç‰¹çš„ä½ç½®ç¼–ç ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - æ©ç  (Masking)\n",
    "\n",
    "åœ¨æ„å»º Transformer ç½‘ç»œæ—¶ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„æ©ç éå¸¸æœ‰ç”¨ï¼š*padding mask*ï¼ˆå¡«å……æ©ç ï¼‰å’Œ *look-ahead mask*ï¼ˆå‰ç»æ©ç ï¼‰ã€‚å®ƒä»¬éƒ½å¯ä»¥å¸®åŠ© softmax è®¡ç®—ä¸ºè¾“å…¥å¥å­ä¸­çš„è¯åˆ†é…åˆé€‚çš„æƒé‡ã€‚\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - å¡«å……æ©ç  (Padding Mask)\n",
    "\n",
    "é€šå¸¸æƒ…å†µä¸‹ï¼Œä½ çš„è¾“å…¥åºåˆ—å¯èƒ½è¶…è¿‡ç½‘ç»œèƒ½å¤Ÿå¤„ç†çš„æœ€å¤§é•¿åº¦ã€‚å‡è®¾æ¨¡å‹çš„æœ€å¤§é•¿åº¦ä¸º 5ï¼Œå®ƒè¾“å…¥å¦‚ä¸‹åºåˆ—ï¼š\n",
    "\n",
    "\n",
    "\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "å‘é‡åŒ–åå¯èƒ½å˜æˆï¼š\n",
    "\n",
    "\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]\n",
    "    ]\n",
    "    \n",
    "\n",
    "å½“å°†åºåˆ—ä¼ å…¥ Transformer æ¨¡å‹æ—¶ï¼Œåºåˆ—é•¿åº¦å¿…é¡»ç»Ÿä¸€ã€‚å¯ä»¥é€šè¿‡åœ¨åºåˆ—æœ«å°¾å¡«å……é›¶æˆ–æˆªæ–­è¶…è¿‡æœ€å¤§é•¿åº¦çš„å¥å­æ¥å®ç°ï¼š\n",
    "\n",
    "\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0],\n",
    "    ]\n",
    "    \n",
    "\n",
    "é•¿åº¦è¶…è¿‡æœ€å¤§å€¼çš„åºåˆ—ä¼šè¢«æˆªæ–­ï¼Œè€Œæˆªæ–­åçš„åºåˆ—æœ«å°¾ä¼šåŠ ä¸Šé›¶ä»¥ä¿æŒç»Ÿä¸€é•¿åº¦ã€‚å¯¹äºé•¿åº¦ä¸è¶³çš„åºåˆ—ï¼Œä¹Ÿä¼šåœ¨æœ«å°¾å¡«å……é›¶ã€‚ç„¶è€Œï¼Œè¿™äº›é›¶ä¼šå½±å“ softmax çš„è®¡ç®—â€”â€”è¿™æ—¶å¡«å……æ©ç å°±æ´¾ä¸Šç”¨åœºäº†ï¼ä½ éœ€è¦å®šä¹‰ä¸€ä¸ªå¸ƒå°”æ©ç ï¼ŒæŒ‡å®šå“ªäº›å…ƒç´ éœ€è¦å…³æ³¨(1)ï¼Œå“ªäº›å…ƒç´ éœ€è¦å¿½ç•¥(0)ã€‚ä¹‹åï¼Œä½ å¯ä»¥ä½¿ç”¨è¯¥æ©ç å°†åºåˆ—ä¸­çš„é›¶è®¾ç½®ä¸ºæ¥è¿‘è´Ÿæ— ç©·å¤§ï¼ˆ-1e9ï¼‰çš„å€¼ã€‚æˆ‘ä»¬ä¼šä¸ºä½ å®ç°è¿™éƒ¨åˆ†é€»è¾‘ï¼Œè®©ä½ å¯ä»¥å°½å¿«è¿›å…¥æ„å»º Transformer ç½‘ç»œçš„ä¹è¶£ ğŸ˜‡ã€‚åªéœ€ç¡®ä¿ç†è§£ä»£ç ï¼Œè¿™æ ·åœ¨æ„å»ºæ¨¡å‹æ—¶æ‰èƒ½æ­£ç¡®å®ç°å¡«å……ã€‚\n",
    "\n",
    "æ©ç å¤„ç†åï¼Œè¾“å…¥ `[87, 600, 0, 0, 0]` ä¼šå˜æˆ `[87, 600, -1e9, -1e9, -1e9]`ï¼Œè¿™æ ·åœ¨è®¡ç®— softmax æ—¶ï¼Œé›¶å°±ä¸ä¼šå½±å“å¾—åˆ†ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JOL9XWsFQxxo"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# åˆ›å»º Padding Mask\n",
    "# ----------------------------\n",
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    ä¸ºè§£ç å™¨è¾“å…¥åºåˆ—åˆ›å»º Padding Mask\n",
    "    \n",
    "    å‚æ•°:\n",
    "        decoder_token_ids -- è¾“å…¥çš„ token id å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len)\n",
    "                             é€šå¸¸ padding token ç”¨ 0 è¡¨ç¤º\n",
    "    \n",
    "    è¿”å›:\n",
    "        mask -- Padding maskï¼Œå½¢çŠ¶ä¸º (batch_size, 1, seq_len)\n",
    "                mask ä¸­é padding ä½ç½®ä¸º 1ï¼Œpadding ä½ç½®ä¸º 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 1. åˆ¤æ–­ token æ˜¯å¦ä¸º padding\n",
    "    # ----------------------------\n",
    "    # decoder_token_ids != 0 -> True è¡¨ç¤ºé paddingï¼ŒFalse è¡¨ç¤º padding\n",
    "    # .float() -> å°† bool è½¬ä¸º float ç±»å‹ï¼Œé padding ä¸º 1.0ï¼Œpadding ä¸º 0.0\n",
    "    seq = (decoder_token_ids != 0).float()  # shape: (batch_size, seq_len)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 2. å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œæ–¹ä¾¿åç»­å¹¿æ’­\n",
    "    # ----------------------------\n",
    "    # mask å½¢çŠ¶å˜ä¸º (batch_size, 1, seq_len)\n",
    "    mask = seq.unsqueeze(1)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 3. è¿”å› mask\n",
    "    # ----------------------------\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Mask shape: torch.Size([3, 1, 5])\n",
      "tensor([[[1., 1., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯•åˆ›å»º Padding Mask å‡½æ•°\n",
    "# ----------------------------\n",
    "\n",
    "# 1. æ„é€ ä¸€ä¸ªç¤ºä¾‹å¼ é‡ xï¼Œå½¢çŠ¶ä¸º (batch_size=3, seq_len=5)\n",
    "#    æ•°å€¼ 0 è¡¨ç¤º paddingï¼Œéé›¶å€¼è¡¨ç¤ºæœ‰æ•ˆ token\n",
    "x = torch.tensor([[7., 6., 0., 0., 1.],   # ç¬¬ 0 è¡Œï¼Œç¬¬ 2ã€3 åˆ—ä¸º padding\n",
    "                  [1., 2., 3., 0., 0.],   # ç¬¬ 1 è¡Œï¼Œç¬¬ 3ã€4 åˆ—ä¸º padding\n",
    "                  [0., 0., 0., 4., 5.]])  # ç¬¬ 2 è¡Œï¼Œç¬¬ 0ã€1ã€2 åˆ—ä¸º padding\n",
    "\n",
    "# 2. è°ƒç”¨ create_padding_mask å‡½æ•°ç”Ÿæˆ mask\n",
    "#    mask å½¢çŠ¶ä¸º (batch_size, 1, seq_len)\n",
    "#    mask ä¸­é padding ä¸º 1ï¼Œpadding ä¸º 0\n",
    "mask = create_padding_mask(x)\n",
    "\n",
    "# 3. æ‰“å° mask çš„å½¢çŠ¶\n",
    "print(\"Padding Mask shape:\", mask.shape)  # è¾“å‡º: torch.Size([3, 1, 5])\n",
    "\n",
    "# 4. æ‰“å° mask çš„å†…å®¹\n",
    "#    å¯è§‚å¯Ÿåˆ°åŸåºåˆ—ä¸­éé›¶ä½ç½®è¢«æ ‡è®°ä¸º 1ï¼Œé›¶ä½ç½®è¢«æ ‡è®°ä¸º 0\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœæˆ‘ä»¬å°† `(1 - mask)` ä¹˜ä»¥ `-1e9` å¹¶åŠ åˆ°ç¤ºä¾‹è¾“å…¥åºåˆ—ä¸­ï¼Œåºåˆ—ä¸­çš„é›¶å°±ä¼šè¢«è®¾ç½®ä¸ºæ¥è¿‘è´Ÿæ— ç©·å¤§ã€‚æ³¨æ„åŸå§‹åºåˆ—ä¸æ©ç åºåˆ—åœ¨è®¡ç®— softmax æ—¶çš„åŒºåˆ«ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax without mask:\n",
      "tensor([[7.2877e-01, 2.6810e-01, 6.6455e-04, 6.6455e-04, 1.8064e-03],\n",
      "        [8.4437e-02, 2.2952e-01, 6.2391e-01, 3.1063e-02, 3.1063e-02],\n",
      "        [4.8541e-03, 4.8541e-03, 4.8541e-03, 2.6503e-01, 7.2041e-01]])\n",
      "\n",
      "Softmax with padding mask applied:\n",
      "tensor([[[7.2974e-01, 2.6845e-01, 0.0000e+00, 0.0000e+00, 1.8088e-03],\n",
      "         [2.4473e-01, 6.6524e-01, 0.0000e+00, 0.0000e+00, 9.0031e-02],\n",
      "         [6.6484e-03, 6.6484e-03, 0.0000e+00, 0.0000e+00, 9.8670e-01]],\n",
      "\n",
      "        [[7.3057e-01, 2.6876e-01, 6.6620e-04, 0.0000e+00, 0.0000e+00],\n",
      "         [9.0031e-02, 2.4473e-01, 6.6524e-01, 0.0000e+00, 0.0000e+00],\n",
      "         [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6894e-01, 7.3106e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0000e-01, 5.0000e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6894e-01, 7.3106e-01]]])\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• Softmax ä¸ Padding Mask\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# 1. æ™®é€š Softmaxï¼ˆä¸åŠ  maskï¼‰\n",
    "# ----------------------------\n",
    "# dim=-1 è¡¨ç¤ºåœ¨æ¯è¡Œï¼ˆæ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªç»´åº¦ï¼‰è¿›è¡Œ softmax\n",
    "softmax_x = F.softmax(x, dim=-1)\n",
    "\n",
    "print(\"Softmax without mask:\")\n",
    "print(softmax_x)\n",
    "# è¾“å‡ºï¼š\n",
    "# æ¯è¡Œçš„å…ƒç´ è¢«è½¬æ¢æˆæ¦‚ç‡ï¼Œæ‰€æœ‰å…ƒç´ ä¹‹å’Œä¸º 1\n",
    "# padding ä½ç½®ä¹Ÿä¼šè¢«è€ƒè™‘ï¼Œä¼šå½±å“æ¦‚ç‡åˆ†å¸ƒï¼ˆä¸åˆç†ï¼‰\n",
    "\n",
    "# ----------------------------\n",
    "# 2. å¸¦ padding mask çš„ Softmax\n",
    "# ----------------------------\n",
    "# 2.1 ç”Ÿæˆ padding mask\n",
    "# create_padding_mask å‡½æ•°è¿”å›é padding ä¸º1ï¼Œpadding ä¸º0\n",
    "mask = create_padding_mask(x)  # shape (batch_size, 1, seq_len)\n",
    "\n",
    "# 2.2 è½¬æ¢ä¸ºæ³¨æ„åŠ› mask\n",
    "# padding ä½ç½®è®¾ä¸º -1e9ï¼Œé padding ä¿æŒåŸå€¼\n",
    "# mask-1 -> padding ä½ç½®å˜ä¸º -1ï¼Œå…¶ä½™ä½ç½®ä¸º 0\n",
    "# x + (mask-1)*1e9 -> padding ä½ç½®å˜ä¸º -1e9ï¼Œsoftmax åæ¥è¿‘ 0\n",
    "masked_x = x + (mask - 1) * 1e9\n",
    "\n",
    "# 2.3 å¯¹å¸¦ mask çš„å¼ é‡åš softmax\n",
    "softmax_masked_x = F.softmax(masked_x, dim=-1)\n",
    "\n",
    "print(\"\\nSoftmax with padding mask applied:\")\n",
    "print(softmax_masked_x)\n",
    "# è¾“å‡ºï¼š\n",
    "# padding ä½ç½®æ¦‚ç‡æ¥è¿‘ 0ï¼Œé padding å…ƒç´ çš„æ¦‚ç‡é‡æ–°å½’ä¸€åŒ–\n",
    "# è¿™æ ·å°±é¿å…äº† padding å¯¹æ¦‚ç‡åˆ†å¸ƒçš„å¹²æ‰°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - å‰ç»æ©ç  (Look-ahead Mask)\n",
    "\n",
    "å‰ç»æ©ç çš„åŸç†ç±»ä¼¼ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œä½ å¯ä»¥è®¿é—®è®­ç»ƒæ ·æœ¬çš„å®Œæ•´æ­£ç¡®è¾“å‡ºã€‚å‰ç»æ©ç å¸®åŠ©æ¨¡å‹æ¨¡æ‹Ÿâ€œå·²æ­£ç¡®é¢„æµ‹éƒ¨åˆ†è¾“å‡ºâ€ï¼Œç„¶åæ£€æŸ¥åœ¨**ä¸çœ‹æœªæ¥ä¿¡æ¯**çš„æƒ…å†µä¸‹ï¼Œæ˜¯å¦èƒ½æ­£ç¡®é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œå¦‚æœæœŸæœ›çš„æ­£ç¡®è¾“å‡ºæ˜¯ `[1, 2, 3]`ï¼Œä½ æƒ³è¦æ£€æŸ¥åœ¨æ¨¡å‹å·²æ­£ç¡®é¢„æµ‹ç¬¬ä¸€ä¸ªå€¼çš„æƒ…å†µä¸‹ï¼Œèƒ½å¦é¢„æµ‹ç¬¬äºŒä¸ªå€¼ï¼Œé‚£ä¹ˆå°±ä¼šå°†ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªå€¼å±è”½æ‰ã€‚è¾“å…¥çš„æ©ç åºåˆ—ä¸º `[1, -1e9, -1e9]`ï¼Œç„¶åè§‚å¯Ÿæ¨¡å‹æ˜¯å¦èƒ½ç”Ÿæˆ `[1, 2, -1e9]`ã€‚\n",
    "\n",
    "å› ä¸ºä½ åŠªåŠ›äº†ï¼Œæˆ‘ä»¬ä¹Ÿä¼šä¸ºä½ å®ç°è¿™ä¸ªæ©ç  ğŸ˜‡ğŸ˜‡ã€‚å†æ¬¡æé†’ï¼Œè¯·ä»”ç»†æŸ¥çœ‹ä»£ç ï¼Œè¿™æ ·ä½ ä»¥åæ‰èƒ½æœ‰æ•ˆåœ°å®ç°å®ƒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9O9UbM31Q3hK"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# åˆ›å»º Look-Ahead Mask\n",
    "# ----------------------------\n",
    "def create_look_ahead_mask(seq_len):\n",
    "    \"\"\"\n",
    "    è¿”å›ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼ˆLower Triangular Matrixï¼‰ä½œä¸º Look-Ahead Maskã€‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "        seq_len -- åºåˆ—é•¿åº¦\n",
    "    \n",
    "    è¿”å›:\n",
    "        mask -- (seq_len, seq_len) çš„å¼ é‡\n",
    "                ä¸‹ä¸‰è§’åŠå¯¹è§’çº¿ä¸º1ï¼Œä¸Šä¸‰è§’ä¸º0\n",
    "                ç”¨äº Transformer decoder çš„è‡ªæ³¨æ„åŠ›ï¼Œå±è”½æœªæ¥ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    # 1. torch.ones((seq_len, seq_len)) -> åˆ›å»ºä¸€ä¸ªå…¨1çŸ©é˜µ\n",
    "    #    å½¢çŠ¶: (seq_len, seq_len)\n",
    "    full_ones = torch.ones((seq_len, seq_len))\n",
    "    \n",
    "    # 2. torch.tril(...) -> å–ä¸‹ä¸‰è§’çŸ©é˜µï¼ŒåŒ…æ‹¬å¯¹è§’çº¿\n",
    "    #    ä¸‹ä¸‰è§’åŠå¯¹è§’çº¿ä½ç½®ä¸º1ï¼Œä¸Šä¸‰è§’ä¸º0\n",
    "    mask = torch.tril(full_ones)\n",
    "    \n",
    "    # 3. è¿”å› mask å¼ é‡\n",
    "    return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look-Ahead Mask for sequence length 3\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nè¾“å‡ºè§£é‡Šï¼š\\nLook-Ahead Mask ä¼šæ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼š\\ntensor([[1., 0., 0.],\\n        [1., 1., 0.],\\n        [1., 1., 1.]])\\n- æ¯ä¸€è¡Œè¡¨ç¤ºå½“å‰ token å¯ä»¥çœ‹åˆ°å“ªäº›ä½ç½®ï¼š\\n  - ç¬¬0ä¸ª token åªèƒ½çœ‹åˆ°è‡ªå·±\\n  - ç¬¬1ä¸ª token å¯ä»¥çœ‹åˆ°å‰ä¸¤ä¸ª token\\n  - ç¬¬2ä¸ª token å¯ä»¥çœ‹åˆ°å‰ä¸‰ä¸ª token\\n- ä¸Šä¸‰è§’ä¸º0çš„ä½ç½®è¡¨ç¤ºè¢«å±è”½ï¼Œé˜²æ­¢ decoder çœ‹æœªæ¥\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• Look-Ahead Mask\n",
    "# ----------------------------\n",
    "\n",
    "# 1. æ„é€ ä¸€ä¸ªéšæœºåºåˆ—å¼ é‡ï¼Œå½¢çŠ¶ (batch_size=1, seq_len=3)\n",
    "x = torch.rand(1, 3)  \n",
    "# ä¾‹å­è¾“å‡ºå¯èƒ½æ˜¯ï¼š\n",
    "# tensor([[0.2372, 0.5411, 0.8733]])\n",
    "\n",
    "# 2. è·å–åºåˆ—é•¿åº¦\n",
    "seq_len = x.shape[1]  # seq_len = 3\n",
    "\n",
    "# 3. è°ƒç”¨ create_look_ahead_mask ç”Ÿæˆä¸‹ä¸‰è§’ Mask\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "# 4. è¾“å‡º mask ä¿¡æ¯\n",
    "print(\"Look-Ahead Mask for sequence length\", seq_len)\n",
    "print(look_ahead_mask)\n",
    "\n",
    "\"\"\"\n",
    "è¾“å‡ºè§£é‡Šï¼š\n",
    "Look-Ahead Mask ä¼šæ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼š\n",
    "tensor([[1., 0., 0.],\n",
    "        [1., 1., 0.],\n",
    "        [1., 1., 1.]])\n",
    "- æ¯ä¸€è¡Œè¡¨ç¤ºå½“å‰ token å¯ä»¥çœ‹åˆ°å“ªäº›ä½ç½®ï¼š\n",
    "  - ç¬¬0ä¸ª token åªèƒ½çœ‹åˆ°è‡ªå·±\n",
    "  - ç¬¬1ä¸ª token å¯ä»¥çœ‹åˆ°å‰ä¸¤ä¸ª token\n",
    "  - ç¬¬2ä¸ª token å¯ä»¥çœ‹åˆ°å‰ä¸‰ä¸ª token\n",
    "- ä¸Šä¸‰è§’ä¸º0çš„ä½ç½®è¡¨ç¤ºè¢«å±è”½ï¼Œé˜²æ­¢ decoder çœ‹æœªæ¥\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG0gPyv0oDBi"
   },
   "source": [
    "<a name='3'></a>\n",
    "## 3 - è‡ªæ³¨æ„åŠ› (Self-Attention)\n",
    "\n",
    "æ­£å¦‚ Transformer è®ºæ–‡çš„ä½œè€…æ‰€è¯´ï¼Œâ€œAttention is All You Needâ€ã€‚\n",
    "\n",
    "<img src=\"self-attention.png\" alt=\"Encoder\" width=\"600\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 1: è‡ªæ³¨æ„åŠ›è®¡ç®—å¯è§†åŒ–</b></font></center></caption>\n",
    "\n",
    "è‡ªæ³¨æ„åŠ›ä¸ä¼ ç»Ÿå·ç§¯ç½‘ç»œç»“åˆä½¿ç”¨å¯ä»¥å®ç°å¹¶è¡Œå¤„ç†ï¼Œä»è€ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä½ å°†å®ç° **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (scaled dot-product attention)**ï¼Œå®ƒæ¥å—æŸ¥è¯¢ (query)ã€é”® (key)ã€å€¼ (value) ä»¥åŠæ©ç  (mask) ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›åºåˆ—ä¸­æ¯ä¸ªå•è¯çš„ä¸°å¯Œçš„åŸºäºæ³¨æ„åŠ›çš„å‘é‡è¡¨ç¤ºã€‚å…¶æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_k}} + M \\right) V \\tag{4}\n",
    "$$\n",
    "\n",
    "* $Q$ æ˜¯æŸ¥è¯¢çŸ©é˜µ (queries matrix)\n",
    "* $K$ æ˜¯é”®çŸ©é˜µ (keys matrix)\n",
    "* $V$ æ˜¯å€¼çŸ©é˜µ (values matrix)\n",
    "* $M$ æ˜¯å¯é€‰çš„æ©ç \n",
    "* $d_k$ æ˜¯é”®çš„ç»´åº¦ï¼Œç”¨äºç¼©æ”¾ï¼Œä»¥é˜² softmax å€¼è¿‡å¤§\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### ç»ƒä¹  3 - scaled_dot_product_attention \n",
    "\n",
    "å®ç°å‡½æ•° `scaled_dot_product_attention()` æ¥åˆ›å»ºåŸºäºæ³¨æ„åŠ›çš„è¡¨ç¤ºã€‚\n",
    "\n",
    "**æç¤º**ï¼šå¸ƒå°”æ©ç å‚æ•°å¯ä»¥ä¼ å…¥ `None`ï¼Œæˆ–è€…æ˜¯ padding æ©ç æˆ–å‰ç»æ©ç ã€‚  \n",
    "\n",
    "åœ¨åº”ç”¨ softmax ä¹‹å‰ï¼Œå°† `(1. - mask)` ä¹˜ä»¥ `-1e9`ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CSysk_rjQ7lp"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Scaled Dot-Product Attention å‡½æ•°\n",
    "# ----------------------------\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ³¨æ„åŠ›æƒé‡å¹¶è¾“å‡ºåŠ æƒåçš„ç»“æœ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        q -- Query å¼ é‡ï¼Œå½¢çŠ¶ (..., seq_len_q, depth)\n",
    "        k -- Key å¼ é‡ï¼Œå½¢çŠ¶ (..., seq_len_k, depth)\n",
    "        v -- Value å¼ é‡ï¼Œå½¢çŠ¶ (..., seq_len_v, depth_v)\n",
    "        mask -- å¯å¹¿æ’­çš„ mask å¼ é‡ï¼Œå½¢çŠ¶ (..., seq_len_q, seq_len_k)\n",
    "                mask ä¸­ä¸º 0 çš„ä½ç½®ä¼šè¢«å±è”½\n",
    "    \n",
    "    è¿”å›å€¼:\n",
    "        output -- åŠ æƒåçš„ Value å¼ é‡ï¼Œå½¢çŠ¶ (..., seq_len_q, depth_v)\n",
    "        attn   -- æ³¨æ„åŠ›æƒé‡å¼ é‡ï¼Œå½¢çŠ¶ (..., seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. è®¡ç®— QÂ·K^Tï¼Œå¾—åˆ°æ³¨æ„åŠ› logits\n",
    "    # q: (..., seq_len_q, depth)\n",
    "    # k.transpose(-2, -1): (..., depth, seq_len_k)\n",
    "    # matmul_qk: (..., seq_len_q, seq_len_k)\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "    # 2. æŒ‰ sqrt(dk) ç¼©æ”¾ï¼Œé¿å…ç‚¹ç§¯è¿‡å¤§å¯¼è‡´ softmax æ¢¯åº¦è¿‡å°\n",
    "    # dk = Q çš„æœ€åä¸€ç»´å¤§å°ï¼Œå³ depth\n",
    "    dk = q.size(-1)\n",
    "    scaled_logits = matmul_qk / math.sqrt(dk)\n",
    "\n",
    "    # 3. å¦‚æœæä¾›äº† maskï¼Œåˆ™å°† mask=0 çš„ä½ç½®è®¾ç½®ä¸º -inf\n",
    "    # è¿™æ · softmax åå¯¹åº”ä½ç½®æ¦‚ç‡ä¸º 0\n",
    "    if mask is not None:\n",
    "        # masked_fill: mask==0 çš„ä½ç½®å¡«å…… -inf\n",
    "        scaled_logits = scaled_logits.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # 4. å¯¹æœ€åä¸€ç»´ (seq_len_k) åš softmaxï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡\n",
    "    attn = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "    # 5. ç”¨æ³¨æ„åŠ›æƒé‡ä¹˜ä»¥ Vï¼Œå¾—åˆ°åŠ æƒè¾“å‡º\n",
    "    output = torch.matmul(attn, v)\n",
    "\n",
    "    return output, attn  # è¾“å‡ºåŠ æƒç»“æœå’Œæ³¨æ„åŠ›æƒé‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot-Product Attention æµ‹è¯•é€šè¿‡ï¼\n",
      "è¾“å‡ºåŠ æƒç»“æœ Output:\n",
      " tensor([[[0.4241, 0.7168, 0.2380, 0.2408],\n",
      "         [0.4604, 0.7211, 0.2376, 0.2195],\n",
      "         [0.4286, 0.7173, 0.2380, 0.2381]],\n",
      "\n",
      "        [[0.6454, 0.4965, 0.4517, 0.5569],\n",
      "         [0.6720, 0.5441, 0.4592, 0.5694],\n",
      "         [0.6279, 0.4778, 0.4598, 0.5622]]])\n",
      "æ³¨æ„åŠ›æƒé‡ Attention weights:\n",
      " tensor([[[0.5029, 0.4971, 0.0000],\n",
      "         [0.4423, 0.5577, 0.0000],\n",
      "         [0.4954, 0.5046, 0.0000]],\n",
      "\n",
      "        [[0.3084, 0.3968, 0.2948],\n",
      "         [0.2713, 0.4670, 0.2616],\n",
      "         [0.2902, 0.3767, 0.3332]]])\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• Scaled Dot-Product Attention\n",
    "# ----------------------------\n",
    "\n",
    "# 1. æ„é€  Q, K, V å¼ é‡\n",
    "# è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨éšæœºå¼ é‡æ¨¡æ‹Ÿ\n",
    "# batch_size = 2, seq_len = 3, depth = 4\n",
    "q = k = v = torch.rand(2, 3, 4)\n",
    "\n",
    "# 2. æ„é€  mask å¼ é‡\n",
    "# mask.shape = (batch_size, seq_len, seq_len)\n",
    "# ç¬¬ä¸€æ¡åºåˆ—å±è”½æœ€åä¸€ä¸ªä½ç½®ï¼ˆpadding/å¿½ç•¥ï¼‰\n",
    "# ç¬¬äºŒæ¡åºåˆ—å…¨éƒ¨å¯è§\n",
    "mask = torch.tensor([[[1, 1, 0],   # ç¬¬ä¸€æ¡åºåˆ—\n",
    "                      [1, 1, 0],\n",
    "                      [1, 1, 0]],\n",
    "                     [[1, 1, 1],   # ç¬¬äºŒæ¡åºåˆ—\n",
    "                      [1, 1, 1],\n",
    "                      [1, 1, 1]]], dtype=torch.float32)\n",
    "\n",
    "# 3. è°ƒç”¨ scaled_dot_product_attention å‡½æ•°\n",
    "# è¿”å›åŠ æƒåçš„è¾“å‡ºå’Œæ³¨æ„åŠ›æƒé‡\n",
    "out, attn = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "# 4. æ£€æŸ¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦æ­£ç¡®\n",
    "# out.shape åº”ä¸º (batch_size, seq_len, depth) -> (2,3,4)\n",
    "# attn.shape åº”ä¸º (batch_size, seq_len, seq_len) -> (2,3,3)\n",
    "assert out.shape == (2, 3, 4) and attn.shape == (2, 3, 3)\n",
    "\n",
    "# 5. æ‰“å°éªŒè¯ä¿¡æ¯\n",
    "print(\"Scaled Dot-Product Attention æµ‹è¯•é€šè¿‡ï¼\")\n",
    "print(\"è¾“å‡ºåŠ æƒç»“æœ Output:\\n\", out)\n",
    "print(\"æ³¨æ„åŠ›æƒé‡ Attention weights:\\n\", attn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¹²å¾—å¥½ï¼ä½ ç°åœ¨å·²ç»å¯ä»¥å®ç°è‡ªæ³¨æ„åŠ›äº†ã€‚æŒæ¡è‡ªæ³¨æ„åŠ›åï¼Œä½ å°±å¯ä»¥å¼€å§‹æ„å»ºç¼–ç å™¨ (encoder) æ¨¡å—äº†ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blS0pEpTqRVI"
   },
   "source": [
    "<a name='4'></a>\n",
    "## 4 - ç¼–ç å™¨ (Encoder)\n",
    "\n",
    "Transformer ç¼–ç å™¨å±‚å°†è‡ªæ³¨æ„åŠ› (self-attention) ä¸å·ç§¯ç¥ç»ç½‘ç»œé£æ ¼çš„å¤„ç†ç»“åˆï¼Œä»¥æå‡è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶å°† K å’Œ V çŸ©é˜µä¼ é€’ç»™è§£ç å™¨ (Decoder)ï¼Œè¯¥è§£ç å™¨å°†åœ¨ä½œä¸šåé¢æ„å»ºã€‚åœ¨æœ¬èŠ‚ä½œä¸šä¸­ï¼Œä½ å°†é€šè¿‡ç»“åˆå¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œæ¥å®ç°ç¼–ç å™¨ (å¦‚å›¾ 2a æ‰€ç¤º)ã€‚\n",
    "\n",
    "<img src=\"encoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 2aï¼šTransformer ç¼–ç å™¨å±‚</b></font></center></caption>\n",
    "\n",
    "* `MultiHeadAttention` å¯ä»¥ç†è§£ä¸ºå¤šæ¬¡è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼Œç”¨äºæ£€æµ‹ä¸åŒç‰¹å¾ã€‚\n",
    "* å‰é¦ˆç¥ç»ç½‘ç»œ (Feed Forward Neural Network) åŒ…å«ä¸¤ä¸ª Dense å±‚ï¼Œæˆ‘ä»¬å°†å…¶å®ç°ä¸º `FullyConnected` å‡½æ•°ã€‚\n",
    "\n",
    "è¾“å…¥å¥å­é¦–å…ˆé€šè¿‡*å¤šå¤´æ³¨æ„åŠ›å±‚*ï¼Œç¼–ç å™¨åœ¨ç¼–ç æŸä¸ªå•è¯æ—¶ä¼šæŸ¥çœ‹è¾“å…¥å¥å­ä¸­çš„å…¶ä»–å•è¯ã€‚å¤šå¤´æ³¨æ„åŠ›å±‚çš„è¾“å‡ºéšåä¼ å…¥*å‰é¦ˆç¥ç»ç½‘ç»œ*ã€‚å®Œå…¨ç›¸åŒçš„å‰é¦ˆç½‘ç»œä¼šç‹¬ç«‹åº”ç”¨äºæ¯ä¸ªä½ç½®ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sC5vJhz29vZR"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# å…¨è¿æ¥å‰é¦ˆç½‘ç»œ (Feed Forward Network)\n",
    "# ----------------------------\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, embedding_dim, fully_connected_dim):\n",
    "        \"\"\"\n",
    "        æ„é€ å‡½æ•°\n",
    "        Arguments:\n",
    "            embedding_dim       -- è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ï¼Œä¹Ÿå°±æ˜¯ Transformer çš„ d_model\n",
    "            fully_connected_dim -- ä¸­é—´éšè—å±‚çš„ç»´åº¦ï¼Œä¹Ÿå°±æ˜¯ dff\n",
    "        \"\"\"\n",
    "        super().__init__()  # è°ƒç”¨çˆ¶ç±» nn.Module çš„æ„é€ å‡½æ•°\n",
    "\n",
    "        # ç¬¬ä¸€å±‚å…¨è¿æ¥ï¼Œå°† d_model æŠ•å½±åˆ°æ›´é«˜ç»´åº¦ dff\n",
    "        # è¾“å…¥: (batch_size, seq_len, embedding_dim)\n",
    "        # è¾“å‡º: (batch_size, seq_len, fully_connected_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, fully_connected_dim)\n",
    "\n",
    "        # ç¬¬äºŒå±‚å…¨è¿æ¥ï¼Œå°† dff æŠ•å½±å› d_model\n",
    "        # è¾“å…¥: (batch_size, seq_len, fully_connected_dim)\n",
    "        # è¾“å‡º: (batch_size, seq_len, embedding_dim)\n",
    "        self.linear2 = nn.Linear(fully_connected_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        Arguments:\n",
    "            x -- è¾“å…¥å¼ é‡, shape: (batch_size, seq_len, embedding_dim)\n",
    "        Returns:\n",
    "            è¾“å‡ºå¼ é‡, shape: (batch_size, seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # ç¬¬ä¸€å±‚å…¨è¿æ¥ + ReLU æ¿€æ´»å‡½æ•°\n",
    "        hidden = F.relu(self.linear1(x))  # shape: (batch_size, seq_len, fully_connected_dim)\n",
    "\n",
    "        # ç¬¬äºŒå±‚å…¨è¿æ¥ï¼ŒæŠ•å½±å›åŸç»´åº¦\n",
    "        output = self.linear2(hidden)     # shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnected passed!\n",
      "Input:\n",
      " tensor([[[0.3919, 0.5033, 0.1868, 0.6499, 0.0126, 0.1006, 0.7525, 0.3018],\n",
      "         [0.6971, 0.1701, 0.8324, 0.2828, 0.9059, 0.7647, 0.0724, 0.2604],\n",
      "         [0.3393, 0.8557, 0.0226, 0.2186, 0.4615, 0.7450, 0.2443, 0.6304],\n",
      "         [0.7754, 0.6210, 0.4196, 0.7816, 0.8926, 0.0512, 0.6946, 0.0292],\n",
      "         [0.8638, 0.2941, 0.6957, 0.9145, 0.8763, 0.5925, 0.6828, 0.8468]],\n",
      "\n",
      "        [[0.1961, 0.0830, 0.6025, 0.6870, 0.0359, 0.9980, 0.9553, 0.9489],\n",
      "         [0.7603, 0.8144, 0.7763, 0.2747, 0.1012, 0.5966, 0.6554, 0.3688],\n",
      "         [0.1311, 0.4456, 0.2000, 0.0555, 0.4747, 0.0290, 0.8260, 0.5776],\n",
      "         [0.2783, 0.4763, 0.0859, 0.8004, 0.6699, 0.0103, 0.9909, 0.5333],\n",
      "         [0.7310, 0.7724, 0.7076, 0.1668, 0.7090, 0.1803, 0.2235, 0.7402]]])\n",
      "Output:\n",
      " tensor([[[-0.0458, -0.1047,  0.0249, -0.1991, -0.0319, -0.1535,  0.2535,\n",
      "           0.2084],\n",
      "         [-0.0400,  0.0851, -0.0790, -0.3572, -0.2309, -0.0093,  0.3958,\n",
      "           0.1602],\n",
      "         [-0.0548,  0.0389, -0.0433, -0.1893, -0.0947, -0.0977,  0.3093,\n",
      "           0.1477],\n",
      "         [-0.0128, -0.0236, -0.0792, -0.2944, -0.0936, -0.1132,  0.2883,\n",
      "           0.1698],\n",
      "         [-0.1344,  0.0139,  0.0126, -0.4191, -0.2097,  0.0159,  0.3908,\n",
      "           0.2491]],\n",
      "\n",
      "        [[-0.0234, -0.1026,  0.0289, -0.4841, -0.1975, -0.0822,  0.3235,\n",
      "           0.3765],\n",
      "         [-0.0867, -0.0210,  0.0078, -0.3052, -0.1436, -0.0550,  0.3494,\n",
      "           0.2157],\n",
      "         [-0.0921,  0.0374,  0.0051, -0.1643, -0.0910, -0.0873,  0.3465,\n",
      "           0.1797],\n",
      "         [-0.0637, -0.0623,  0.0129, -0.2096, -0.0643, -0.1328,  0.2860,\n",
      "           0.2198],\n",
      "         [-0.1265,  0.0938, -0.0568, -0.2037, -0.2013, -0.0173,  0.3669,\n",
      "           0.1550]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• FullyConnected å‰é¦ˆç½‘ç»œ\n",
    "# ----------------------------\n",
    "\n",
    "# 1. å®ä¾‹åŒ– FullyConnected\n",
    "# embedding_dim=8 : è¾“å…¥è¾“å‡ºçš„ embedding ç»´åº¦ d_model\n",
    "# fully_connected_dim=16 : å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦ dff\n",
    "fc = FullyConnected(embedding_dim=8, fully_connected_dim=16)\n",
    "\n",
    "# 2. æ„é€ éšæœºè¾“å…¥å¼ é‡ x\n",
    "# shape: (batch_size=2, seq_len=5, embedding_dim=8)\n",
    "x = torch.rand(2, 5, 8)\n",
    "\n",
    "# 3. å‰å‘ä¼ æ’­\n",
    "# è¾“å‡º shape åº”ä¸è¾“å…¥ç›¸åŒ: (batch_size, seq_len, embedding_dim)\n",
    "y = fc(x)\n",
    "\n",
    "# 4. éªŒè¯è¾“å‡ºå½¢çŠ¶\n",
    "# ç¡®è®¤å‰é¦ˆç½‘ç»œçš„è¾“å‡ºä¸è¾“å…¥ shape åŒ¹é…\n",
    "assert y.shape == x.shape\n",
    "\n",
    "# 5. æ‰“å°æµ‹è¯•ç»“æœ\n",
    "print(\"FullyConnected passed!\")\n",
    "print(\"Input:\\n\", x)   # æ‰“å°è¾“å…¥å¼ é‡\n",
    "print(\"Output:\\n\", y)  # æ‰“å°è¾“å‡ºå¼ é‡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R65WbX5wqYYH"
   },
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 ç¼–ç å™¨å±‚ (Encoder Layer)\n",
    "\n",
    "ç°åœ¨ä½ å¯ä»¥åœ¨ä¸€ä¸ªç¼–ç å™¨å±‚ä¸­å°†å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œç»“åˆèµ·æ¥ï¼ä½ è¿˜å°†ä½¿ç”¨æ®‹å·®è¿æ¥ (residual connections) å’Œå±‚å½’ä¸€åŒ– (layer normalization) æ¥åŠ é€Ÿè®­ç»ƒ (å›¾ 2a)ã€‚\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### ç»ƒä¹  4 - EncoderLayer\n",
    "\n",
    "ä½¿ç”¨ `forward()` æ–¹æ³•å®ç° `EncoderLayer()`ã€‚\n",
    "\n",
    "åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†å®ç°ä¸€ä¸ªç¼–ç å™¨å—ï¼ˆå›¾ 2ï¼‰ï¼Œä½¿ç”¨ `forward()` æ–¹æ³•ã€‚è¯¥å‡½æ•°åº”æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. å°†è¾“å…¥å¼ é‡ `x` å’Œå¯é€‰çš„ padding mask è¾“å…¥å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ã€‚è¯·è®°ä½ï¼Œä¸ºäº†è®¡ç®—*è‡ªæ³¨æ„åŠ› (self-attention)*ï¼Œqueryã€key å’Œ value éƒ½ç›¸åŒã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿˜éœ€åœ¨å¤šå¤´æ³¨æ„åŠ›å±‚è¾“å‡ºåæ‰§è¡Œ Dropoutã€‚\n",
    "2. æ·»åŠ è·³è·ƒè¿æ¥ (skip connection)ï¼Œå°†åŸå§‹è¾“å…¥ `x` ä¸å¤šå¤´æ³¨æ„åŠ›å±‚è¾“å‡ºç›¸åŠ ã€‚\n",
    "3. æ·»åŠ è·³è·ƒè¿æ¥åï¼Œå°†è¾“å‡ºä¼ å…¥ç¬¬ä¸€ä¸ªå½’ä¸€åŒ–å±‚ (LayerNorm)ã€‚\n",
    "4. æœ€åé‡å¤æ­¥éª¤ 1-3ï¼Œä½†ä½¿ç”¨å‰é¦ˆç¥ç»ç½‘ç»œå’Œ Dropout å±‚ä»£æ›¿å¤šå¤´æ³¨æ„åŠ›å±‚ã€‚\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"darkgreen\"><b>é¢å¤–æç¤º (ç‚¹å‡»å±•å¼€)</b></font></summary>\n",
    "    \n",
    "* `__init__` æ–¹æ³•åˆ›å»ºæ‰€æœ‰å°†åœ¨ `forward` æ–¹æ³•ä¸­è®¿é—®çš„å±‚ã€‚æ— è®ºä½•æ—¶åœ¨ `forward` æ–¹æ³•ä¸­ä½¿ç”¨ `__init__` ä¸­å®šä¹‰çš„å±‚ï¼Œéƒ½å¿…é¡»ä½¿ç”¨è¯­æ³• `self.[å±‚å]`ã€‚\n",
    "* å¯ä»¥æŸ¥é˜… [torch.nn.MultiheadAttention æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) è·å–å¸®åŠ©ã€‚*æ³¨æ„ï¼Œå¦‚æœ queryã€key å’Œ value ç›¸åŒï¼Œåˆ™è¯¥å‡½æ•°æ‰§è¡Œè‡ªæ³¨æ„åŠ› (self-attention)ã€‚*\n",
    "* `self.mha` çš„ forward å‚æ•°å¦‚ä¸‹ï¼š\n",
    "  - `query`ï¼šå½¢çŠ¶ä¸º (batch_size, seq_len, embedding_dim) çš„æŸ¥è¯¢å¼ é‡\n",
    "  - `key`ï¼šå½¢çŠ¶ä¸º (batch_size, seq_len, embedding_dim) çš„é”®å¼ é‡ï¼ˆé€šå¸¸ä¸ query ç›¸åŒï¼‰\n",
    "  - `value`ï¼šå½¢çŠ¶ä¸º (batch_size, seq_len, embedding_dim) çš„å€¼å¼ é‡ï¼ˆé€šå¸¸ä¸ query ç›¸åŒï¼‰\n",
    "  - `key_padding_mask`ï¼šå½¢çŠ¶ä¸º (batch_size, seq_len) çš„å¸ƒå°” maskï¼Œmask ä¸­ä¸º True çš„ä½ç½®å°†è¢«å¿½ç•¥ï¼ˆpaddingï¼‰\n",
    "  - è¿”å›å€¼ï¼š`attn_output, attn_weights`ï¼Œå…¶ä¸­ `attn_output` ä¸ºæ³¨æ„åŠ›è¾“å‡ºå¼ é‡ï¼Œ`attn_weights` ä¸ºæ³¨æ„åŠ›æƒé‡\n",
    "* æ³¨æ„æ®‹å·®è¿æ¥å’Œ LayerNorm çš„ä½¿ç”¨é¡ºåºï¼šå…ˆæ®‹å·®ç›¸åŠ ï¼Œå†å½’ä¸€åŒ–ã€‚\n",
    "* Dropout åœ¨è®­ç»ƒæ¨¡å¼ä¸‹åº”ç”¨ï¼Œå¯ä½¿ç”¨ `self.training` åˆ¤æ–­ã€‚\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tIufbrc-9_2u"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Encoder Layer\n",
    "# ----------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Layer: å¤šå¤´è‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # ----------------------------\n",
    "        # 1. å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚\n",
    "        # embed_dim = embedding_dim : è¾“å…¥è¾“å‡º embedding ç»´åº¦\n",
    "        # num_heads = num_heads : æ³¨æ„åŠ›å¤´æ•°é‡\n",
    "        # batch_first=True : è¾“å…¥ shape ä¸º (batch, seq_len, embedding_dim)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # 2. å‰é¦ˆç½‘ç»œ\n",
    "        # embedding_dim -> fully_connected_dim -> embedding_dim\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # 3. ä¸¤ä¸ª LayerNormï¼Œç”¨äºæ®‹å·®è¿æ¥åçš„å½’ä¸€åŒ–\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)  # è‡ªæ³¨æ„åŠ›å\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)  # å‰é¦ˆç½‘ç»œå\n",
    "        \n",
    "        # ----------------------------\n",
    "        # 4. Dropout å±‚ï¼Œç”¨äºæ­£åˆ™åŒ–\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x -- è¾“å…¥å¼ é‡ (batch_size, seq_len, embedding_dim)\n",
    "            mask -- padding mask (batch_size, seq_len), å¯é€‰\n",
    "        Returns:\n",
    "            out -- ç¼–ç å™¨å±‚è¾“å‡ºï¼Œshape åŒè¾“å…¥\n",
    "        \"\"\"\n",
    "        # ----------------------------\n",
    "        # 1. å¤šå¤´è‡ªæ³¨æ„åŠ›\n",
    "        # q, k, v éƒ½æ˜¯ x\n",
    "        # key_padding_mask: mask ä¸­ä¸º True çš„ä½ç½®å°†è¢«å¿½ç•¥ï¼ˆpaddingï¼‰\n",
    "        attn_out, _ = self.mha(x, x, x, key_padding_mask=mask)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # 2. æ®‹å·®è¿æ¥ + LayerNorm\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # 3. å‰é¦ˆç½‘ç»œ + Dropout\n",
    "        ffn_out = self.dropout(self.ffn(x))\n",
    "        \n",
    "        # ----------------------------\n",
    "        # 4. æ®‹å·®è¿æ¥ + LayerNorm\n",
    "        out = self.norm2(x + ffn_out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLayer passed!\n",
      "Input:\n",
      " tensor([[[0.7947, 0.9531, 0.4589, 0.6945, 0.4668, 0.4361, 0.0973, 0.1205],\n",
      "         [0.3888, 0.5288, 0.4888, 0.6794, 0.2912, 0.0418, 0.8516, 0.4749],\n",
      "         [0.6194, 0.0618, 0.9253, 0.4245, 0.7067, 0.3102, 0.6887, 0.8965],\n",
      "         [0.2681, 0.5815, 0.5684, 0.6029, 0.8595, 0.5352, 0.4130, 0.8486],\n",
      "         [0.9073, 0.5362, 0.4928, 0.3184, 0.6954, 0.2466, 0.5180, 0.7157]],\n",
      "\n",
      "        [[0.4113, 0.3603, 0.1968, 0.7512, 0.1381, 0.5056, 0.9400, 0.6953],\n",
      "         [0.4676, 0.9834, 0.7910, 0.0837, 0.7162, 0.1024, 0.5559, 0.7923],\n",
      "         [0.8730, 0.2485, 0.3912, 0.7760, 0.0479, 0.5188, 0.0596, 0.1548],\n",
      "         [0.2840, 0.5443, 0.9917, 0.5454, 0.4684, 0.2163, 0.9661, 0.7396],\n",
      "         [0.6313, 0.9254, 0.5091, 0.5744, 0.8824, 0.8933, 0.9690, 0.2545]]])\n",
      "Output:\n",
      " tensor([[[ 0.3008,  1.4027,  0.2056,  1.3513, -0.4654, -0.7725, -0.2318,\n",
      "          -1.7907],\n",
      "         [-0.5861, -0.0544,  0.2431,  0.8553, -0.0947, -1.6568,  1.9259,\n",
      "          -0.6324],\n",
      "         [-0.4679, -1.5238,  1.4557,  0.0515, -0.1143, -1.1839,  1.2997,\n",
      "           0.4830],\n",
      "         [-2.0143,  0.4078,  0.4358,  1.1447,  0.3602, -1.2750,  0.6678,\n",
      "           0.2730],\n",
      "         [ 0.9970, -0.1465,  0.5198, -0.3294,  0.5597, -2.2396,  1.0519,\n",
      "          -0.4129]],\n",
      "\n",
      "        [[-0.7437, -0.5634, -0.7363,  1.1489, -0.4468, -0.5999,  2.1588,\n",
      "          -0.2177],\n",
      "         [-0.7424,  1.1788,  0.6648, -0.8744,  0.7880, -1.7990,  0.9784,\n",
      "          -0.1942],\n",
      "         [ 0.5580, -0.4353,  0.2733,  2.1303, -1.0651, -0.0325, -0.1102,\n",
      "          -1.3185],\n",
      "         [-1.1337, -0.3129,  0.8138,  0.1585,  0.3317, -1.5204,  1.8622,\n",
      "          -0.1993],\n",
      "         [-0.5522,  0.4938, -0.4552,  0.1269,  0.7988, -0.3238,  1.7560,\n",
      "          -1.8443]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• EncoderLayer\n",
    "# ----------------------------\n",
    "\n",
    "# 1. å®ä¾‹åŒ–ä¸€ä¸ª EncoderLayer\n",
    "# embedding_dim=8: è¾“å…¥/è¾“å‡º embedding ç»´åº¦\n",
    "# num_heads=2: å¤šå¤´æ³¨æ„åŠ›å¤´æ•°é‡\n",
    "# fully_connected_dim=16: å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦\n",
    "enc_layer = EncoderLayer(embedding_dim=8, num_heads=2, fully_connected_dim=16)\n",
    "\n",
    "# 2. åˆ›å»ºè¾“å…¥å¼ é‡\n",
    "# batch_size=2, seq_len=5, embedding_dim=8\n",
    "# éšæœºåˆå§‹åŒ–è¾“å…¥æ•°æ®ï¼Œæ¨¡æ‹Ÿåºåˆ—åµŒå…¥\n",
    "x = torch.rand(2, 5, 8)\n",
    "\n",
    "# 3. åˆ›å»º padding mask\n",
    "# None è¡¨ç¤ºä¸ä½¿ç”¨ mask\n",
    "# ä¹Ÿå¯ä»¥ä½¿ç”¨ mask = torch.zeros(2,5) æ¥æµ‹è¯• padding æ•ˆæœ\n",
    "mask = None\n",
    "\n",
    "# 4. å‰å‘è®¡ç®—\n",
    "# å°†è¾“å…¥ x å’Œ mask ä¼ å…¥ EncoderLayer\n",
    "out = enc_layer(x, mask)\n",
    "\n",
    "# 5. éªŒè¯è¾“å‡ºå½¢çŠ¶\n",
    "# EncoderLayer è¾“å‡º shape åº”è¯¥ä¸è¾“å…¥ç›¸åŒ\n",
    "assert out.shape == x.shape\n",
    "\n",
    "# 6. è¾“å‡ºç»“æœ\n",
    "print(\"EncoderLayer passed!\")\n",
    "print(\"Input:\\n\", x)   # è¾“å…¥å¼ é‡\n",
    "print(\"Output:\\n\", out)  # EncoderLayer è¾“å‡ºå¼ é‡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - å®Œæ•´ç¼–ç å™¨ (Full Encoder)\n",
    "\n",
    "å¤ªæ£’äº†ï¼ä½ ç°åœ¨å·²ç»æˆåŠŸå®ç°äº†ä½ç½®ç¼–ç  (positional encoding)ã€è‡ªæ³¨æ„åŠ› (self-attention) å’Œç¼–ç å™¨å±‚ (encoder layer) â€”â€” ç»™è‡ªå·±ä¸€ä¸ªé¼“åŠ±çš„æŒå£°ğŸ‘ã€‚ç°åœ¨ä½ å‡†å¤‡å¥½æ„å»ºå®Œæ•´çš„ Transformer ç¼–ç å™¨ï¼ˆå›¾ 2bï¼‰äº†ï¼Œä½ å°†å¯¹è¾“å…¥è¿›è¡ŒåµŒå…¥ï¼Œå¹¶æ·»åŠ ä½ è®¡ç®—å¥½çš„ä½ç½®ç¼–ç ã€‚ç„¶åå°†ç¼–ç åçš„åµŒå…¥é€å…¥å¤šå±‚ç¼–ç å™¨å †æ ˆä¸­ã€‚\n",
    "\n",
    "<img src=\"encoder.png\" alt=\"Encoder\" width=\"330\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 2b: Transformer ç¼–ç å™¨</b></font></center></caption>\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### ç»ƒä¹  5 - Encoder\n",
    "\n",
    "ä½¿ç”¨ `forward()` æ–¹æ³•å®Œæˆ `Encoder()` ç±»ï¼Œä»¥å¯¹è¾“å…¥è¿›è¡ŒåµŒå…¥ã€æ·»åŠ ä½ç½®ç¼–ç ï¼Œå¹¶å®ç°å¤šä¸ªç¼–ç å™¨å±‚ã€‚\n",
    "\n",
    "åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†ç”¨ Embedding å±‚ã€ä½ç½®ç¼–ç ä»¥åŠå¤šä¸ª EncoderLayer åˆå§‹åŒ–ç¼–ç å™¨ã€‚ä½ çš„ `forward()` æ–¹æ³•åº”æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. å°†è¾“å…¥ token id å¼ é‡ `x` ä¼ å…¥ Embedding å±‚ï¼Œå¾—åˆ°å½¢çŠ¶ä¸º `(batch_size, seq_len, embedding_dim)` çš„åµŒå…¥ã€‚\n",
    "2. å¯¹åµŒå…¥è¿›è¡Œç¼©æ”¾ (scale)ï¼šä¹˜ä»¥åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹ `sqrt(embedding_dim)`ï¼Œä»¥æé«˜æ¢¯åº¦ç¨³å®šæ€§ã€‚\n",
    "3. æ·»åŠ ä½ç½®ç¼–ç ï¼šå°† `self.pos_encoding[:, :seq_len, :]` åŠ åˆ°åµŒå…¥ä¸Šï¼Œå¹¶ç¡®ä¿ä½ç½®ç¼–ç å’ŒåµŒå…¥åœ¨åŒä¸€ deviceã€‚\n",
    "4. å°†ç¼–ç åçš„åµŒå…¥ä¼ å…¥ Dropout å±‚è¿›è¡Œéšæœºå±è”½ï¼Œè®­ç»ƒæ¨¡å¼å¯é€šè¿‡ `self.training` è‡ªåŠ¨æ§åˆ¶ã€‚\n",
    "5. ä½¿ç”¨ for å¾ªç¯å°† Dropout å±‚è¾“å‡ºä¾æ¬¡ä¼ å…¥ç¼–ç å™¨å †æ ˆä¸­çš„å¤šå±‚ `EncoderLayer`ï¼Œæ¯å±‚ä½¿ç”¨å¯é€‰çš„ padding maskã€‚\n",
    "6. è¿”å›æœ€ç»ˆç¼–ç å™¨è¾“å‡ºï¼Œå½¢çŠ¶ä¸º `(batch_size, seq_len, embedding_dim)`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7j2Tjr0K0t0I"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Encoder æ¨¡å—\n",
    "# ----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„ Encoder æ¨¡å—:\n",
    "    åŒ…å«ï¼š\n",
    "    1. Embedding å±‚\n",
    "    2. Positional Encoding\n",
    "    3. å¤šä¸ª EncoderLayer å †å \n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ----------------------------\n",
    "        # ä¿å­˜è¶…å‚æ•°\n",
    "        # ----------------------------\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # ----------------------------\n",
    "        # 1. Token embedding å±‚\n",
    "        # input_vocab_size: è¯è¡¨å¤§å°\n",
    "        # embedding_dim: è¯å‘é‡ç»´åº¦\n",
    "        # è¾“å‡º shape: (batch_size, seq_len, embedding_dim)\n",
    "        # ----------------------------\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "\n",
    "        # ----------------------------\n",
    "        # 2. ä½ç½®ç¼–ç \n",
    "        # positional_encoding è¿”å› numpy array [1, max_pos, embedding_dim]\n",
    "        # è½¬æ¢ä¸º torch tensor\n",
    "        # ----------------------------\n",
    "        self.pos_encoding = torch.tensor(\n",
    "            positional_encoding(maximum_position_encoding, embedding_dim),\n",
    "            dtype=torch.float32\n",
    "        )  # shape: [1, maximum_position_encoding, embedding_dim]\n",
    "\n",
    "        # ----------------------------\n",
    "        # 3. EncoderLayer å †å \n",
    "        # ä½¿ç”¨ ModuleList ä¿å­˜å¤šä¸ª EncoderLayer\n",
    "        # æ¯ä¸ª EncoderLayer åŒ…å«:\n",
    "        #   multi-head attention + feed-forward + residual + layernorm\n",
    "        # ----------------------------\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                embedding_dim=embedding_dim,\n",
    "                num_heads=num_heads,\n",
    "                fully_connected_dim=fully_connected_dim,\n",
    "                dropout_rate=dropout_rate\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ----------------------------\n",
    "        # 4. Dropout å±‚\n",
    "        # ----------------------------\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Encoder å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            x: (batch_size, seq_len) -> token id åºåˆ—\n",
    "            mask: (batch_size, seq_len) -> padding maskï¼Œ0 è¡¨ç¤º padding\n",
    "            \n",
    "        è¿”å›:\n",
    "            x: (batch_size, seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)  # åºåˆ—é•¿åº¦\n",
    "\n",
    "        # ----------------------------\n",
    "        # 1. Embedding\n",
    "        # ----------------------------\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embedding_dim)\n",
    "        x *= math.sqrt(self.embedding_dim)  # ç¼©æ”¾ embeddingï¼Œæé«˜æ¢¯åº¦ç¨³å®šæ€§\n",
    "\n",
    "        # ----------------------------\n",
    "        # 2. æ·»åŠ ä½ç½®ç¼–ç \n",
    "        # ----------------------------\n",
    "        x = x + self.pos_encoding[:, :seq_len, :].to(x.device)  # ä¿è¯åœ¨åŒä¸€ device ä¸Š\n",
    "        x = self.dropout(x)  # dropout éšæœºå±è”½éƒ¨åˆ†ç¥ç»å…ƒ\n",
    "\n",
    "        # ----------------------------\n",
    "        # 3. é€šè¿‡æ¯ä¸€å±‚ EncoderLayer\n",
    "        # ----------------------------\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, mask)  # mask ç”¨äº padding\n",
    "\n",
    "        # è¾“å‡º shape: (batch_size, seq_len, embedding_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder passed!\n",
      "Input token ids:\n",
      " tensor([[18, 15, 20, 34,  3,  6, 20, 31, 33, 22],\n",
      "        [20, 19, 30, 45, 19, 40,  3, 41,  8, 31]])\n",
      "Encoder output:\n",
      " tensor([[[ 1.7026, -0.6184,  0.7938,  0.2587, -1.4241,  0.8039, -0.4441,\n",
      "          -1.0724],\n",
      "         [ 1.1894,  1.9279, -0.1774, -1.4647, -0.4269, -0.1308, -0.6480,\n",
      "          -0.2695],\n",
      "         [ 0.0239,  0.1147, -0.3023,  0.3296,  2.0046, -1.9088,  0.0816,\n",
      "          -0.3433],\n",
      "         [-0.1425, -1.3909, -1.4283,  0.3300,  0.0694,  1.2380,  1.5229,\n",
      "          -0.1985],\n",
      "         [-0.4435,  0.4189,  0.8504, -0.5401, -0.3779,  2.1049, -0.9220,\n",
      "          -1.0906],\n",
      "         [-0.8367,  0.2064,  0.1647,  0.6553,  0.2442,  1.8635, -0.5877,\n",
      "          -1.7097],\n",
      "         [-0.4595,  0.6335, -0.0876,  0.3368,  1.9118, -1.8551, -0.0741,\n",
      "          -0.4058],\n",
      "         [ 0.8256,  0.8318,  1.8912, -0.6672, -0.8028, -0.1164, -1.0842,\n",
      "          -0.8780],\n",
      "         [-0.9251, -0.3453, -0.7092,  1.5367, -0.9252,  0.2768, -0.6017,\n",
      "           1.6929],\n",
      "         [ 0.5557, -0.3558, -1.7067,  0.0573, -0.1592,  1.8867,  0.5767,\n",
      "          -0.8548]],\n",
      "\n",
      "        [[ 0.5318, -0.0870, -0.6533,  0.8184,  1.5932, -1.8478,  0.3689,\n",
      "          -0.7242],\n",
      "         [ 1.1240, -1.4967,  0.8106,  1.5463, -0.3052, -0.0435, -0.7287,\n",
      "          -0.9068],\n",
      "         [ 0.1784,  0.0993, -1.4626,  0.5285, -0.0572,  2.0914, -0.3625,\n",
      "          -1.0154],\n",
      "         [-0.8272,  1.4682, -0.5815,  0.2205,  0.4850,  0.1938, -1.9008,\n",
      "           0.9420],\n",
      "         [ 0.6527, -1.9737,  0.3589,  1.6718, -0.4150,  0.3680,  0.0058,\n",
      "          -0.6686],\n",
      "         [-0.7654,  1.5449,  1.1335, -0.7975,  0.1550, -1.2651, -0.8635,\n",
      "           0.8582],\n",
      "         [-0.3450,  0.4975,  0.6363, -0.8263, -0.4061,  2.1982, -0.9466,\n",
      "          -0.8080],\n",
      "         [-0.1033,  0.6846, -0.9073,  0.3068, -0.9155,  0.2671, -1.3201,\n",
      "           1.9877],\n",
      "         [-0.4544,  1.1719,  0.0879,  0.2216, -0.7895,  1.8731, -1.0090,\n",
      "          -1.1016],\n",
      "         [ 0.4716,  0.2470,  1.2456, -0.6094, -1.1708,  1.6074, -0.5532,\n",
      "          -1.2382]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luxia\\AppData\\Local\\Temp\\ipykernel_20508\\1819583349.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pos_encoding = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• Encoder\n",
    "# ----------------------------\n",
    "\n",
    "# 1. å®ä¾‹åŒ– Encoder\n",
    "# num_layers=2 -> å †å  2 å±‚ EncoderLayer\n",
    "# embedding_dim=8 -> è¯å‘é‡ç»´åº¦ 8\n",
    "# num_heads=2 -> å¤šå¤´æ³¨æ„åŠ›å¤´æ•° 2\n",
    "# fully_connected_dim=16 -> å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦ 16\n",
    "# input_vocab_size=50 -> è¯è¡¨å¤§å° 50\n",
    "# maximum_position_encoding=100 -> æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦ 100\n",
    "encoder = Encoder(\n",
    "    num_layers=2,\n",
    "    embedding_dim=8,\n",
    "    num_heads=2,\n",
    "    fully_connected_dim=16,\n",
    "    input_vocab_size=50,\n",
    "    maximum_position_encoding=100\n",
    ")\n",
    "\n",
    "# 2. æ„é€ éšæœºè¾“å…¥ token id åºåˆ—\n",
    "# torch.randint(low, high, size) -> ç”Ÿæˆ [low, high) ä¹‹é—´çš„æ•´æ•°\n",
    "# shape = (batch_size=2, seq_len=10)\n",
    "x_inp = torch.randint(0, 50, (2, 10))\n",
    "\n",
    "# 3. å‰å‘ä¼ æ’­\n",
    "# è¾“å‡º shape: (batch_size, seq_len, embedding_dim) = (2, 10, 8)\n",
    "out = encoder(x_inp)\n",
    "\n",
    "# 4. æ–­è¨€æ£€æŸ¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦æ­£ç¡®\n",
    "assert out.shape == (2, 10, 8)\n",
    "\n",
    "# 5. æ‰“å°ç»“æœï¼Œä¾¿äºè§‚å¯Ÿ\n",
    "print(\"Encoder passed!\")\n",
    "print(\"Input token ids:\\n\", x_inp)\n",
    "print(\"Encoder output:\\n\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - è§£ç å™¨ (Decoder)\n",
    "\n",
    "è§£ç å™¨å±‚æ¥æ”¶ç¼–ç å™¨ç”Ÿæˆçš„ K å’Œ V çŸ©é˜µï¼Œå¹¶ä½¿ç”¨è¾“å‡ºçš„ Q çŸ©é˜µè®¡ç®—ç¬¬äºŒä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ (multi-head attention)ï¼ˆå›¾ 3aï¼‰ã€‚\n",
    "\n",
    "<img src=\"decoder_layer.png\" alt=\"Decoder Layer\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 3a: Transformer è§£ç å™¨å±‚</b></font></center></caption>\n",
    "\n",
    "<a name='5-1'></a>    \n",
    "### 5.1 - è§£ç å™¨å±‚ (Decoder Layer)\n",
    "\n",
    "åŒæ ·ï¼Œä½ å°†å¤šå¤´æ³¨æ„åŠ›ä¸å‰é¦ˆç¥ç»ç½‘ç»œç»“åˆï¼Œä½†è¿™æ¬¡éœ€è¦å®ç°ä¸¤å±‚å¤šå¤´æ³¨æ„åŠ›å±‚ã€‚ä½ è¿˜ä¼šä½¿ç”¨æ®‹å·®è¿æ¥ (residual connections) å’Œå±‚å½’ä¸€åŒ– (layer normalization) æ¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆå›¾ 3aï¼‰ã€‚\n",
    "\n",
    "<a name='ex-6'></a>    \n",
    "### ç»ƒä¹  6 - DecoderLayer\n",
    "\n",
    "ä½¿ç”¨ `forward()` æ–¹æ³•å®ç° `DecoderLayer()`ï¼š\n",
    "\n",
    "1. **Block 1: Masked Multi-Head Self-Attention**  \n",
    "   - è¾“å…¥ `x`ï¼Œè¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼ˆQ=K=V=xï¼‰ã€‚  \n",
    "   - ä½¿ç”¨å‰ç»æ©ç  (look-ahead mask) é®ç›–æœªæ¥ä½ç½®ï¼Œé˜²æ­¢å½“å‰ token æ³¨æ„æœªæ¥ tokenã€‚  \n",
    "   - è¾“å‡ºç»è¿‡æ®‹å·®è¿æ¥å’Œ LayerNormã€‚  \n",
    "   - è¿”å›æ³¨æ„åŠ›æƒé‡ `attn_weights_block1` ä»¥ä¾¿å¯è§†åŒ–æˆ–åˆ†æã€‚\n",
    "\n",
    "2. **Block 2: Encoder-Decoder Multi-Head Attention**  \n",
    "   - è¾“å…¥æ¥è‡ª Block 1 çš„è¾“å‡º `x1` ä½œä¸º Qï¼Œç¼–ç å™¨è¾“å‡º `enc_output` ä½œä¸º K å’Œ Vã€‚  \n",
    "   - ä½¿ç”¨ padding mask é®ç›–ç¼–ç å™¨çš„ padding tokenã€‚  \n",
    "   - è¾“å‡ºç»è¿‡æ®‹å·®è¿æ¥å’Œ LayerNormã€‚  \n",
    "   - è¿”å›æ³¨æ„åŠ›æƒé‡ `attn_weights_block2`ã€‚\n",
    "\n",
    "3. **Block 3: Feed-Forward Network**  \n",
    "   - å¯¹ Block 2 çš„è¾“å‡ºä¼ å…¥å‰é¦ˆå…¨è¿æ¥ç½‘ç»œï¼Œå¹¶åº”ç”¨ Dropoutã€‚  \n",
    "   - è¾“å‡ºç»è¿‡æ®‹å·®è¿æ¥å’Œ LayerNormï¼Œå¾—åˆ°æœ€ç»ˆ Decoder Layer è¾“å‡º `out3`ã€‚\n",
    "\n",
    "**é¢å¤–æç¤ºï¼š**  \n",
    "* Dropout å±‚åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è‡ªåŠ¨å¯ç”¨ï¼Œå¯é€šè¿‡ `self.training` åˆ¤æ–­ã€‚  \n",
    "* Masked Self-Attention å’Œ Encoder-Decoder Attention éƒ½éœ€è¦è¿”å›æ³¨æ„åŠ›æƒé‡ï¼Œä¾¿äºå¯è§†åŒ–å’Œè°ƒè¯•ã€‚  \n",
    "* æ®‹å·®è¿æ¥é¡ºåºï¼šå…ˆåŠ ä¸ŠåŸå§‹è¾“å…¥ï¼Œå†å½’ä¸€åŒ–ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wEouNFvCzMeT"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Decoder Layer (PyTorch)\n",
    "# ----------------------------\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Layer ç»“æ„ï¼š\n",
    "    1. Masked è‡ªæ³¨æ„åŠ›å±‚ï¼ˆMasked Self-Attentionï¼‰\n",
    "    2. Encoder-Decoder æ³¨æ„åŠ›å±‚\n",
    "    3. å‰é¦ˆå…¨è¿æ¥ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰\n",
    "    æ¯ä¸ªå­å±‚åŒ…å«æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰å’Œå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # BLOCK 1: Masked Multi-Head Self-Attention\n",
    "        # embed_dim=d_model -> è¾“å…¥/è¾“å‡ºç‰¹å¾ç»´åº¦\n",
    "        # num_heads=num_heads -> æ³¨æ„åŠ›å¤´æ•°\n",
    "        # batch_first=True -> è¾“å…¥ shape ä¸º (batch, seq_len, d_model)\n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # BLOCK 2: Encoder-Decoder Multi-Head Attention\n",
    "        # query=x1, key=enc_output, value=enc_output\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # å‰é¦ˆå…¨è¿æ¥ç½‘ç»œ\n",
    "        self.ffn = FullyConnected(d_model, dff)\n",
    "        \n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout å±‚\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "        x -- Decoder è¾“å…¥ï¼Œshape=(batch_size, seq_len, d_model)\n",
    "        enc_output -- Encoder è¾“å‡ºï¼Œshape=(batch_size, enc_seq_len, d_model)\n",
    "        look_ahead_mask -- Mask ç”¨äºé˜²æ­¢å½“å‰ token æ³¨æ„æœªæ¥ tokenï¼Œshape å¯å¹¿æ’­åˆ° (batch_size, seq_len, seq_len)\n",
    "        padding_mask -- Mask ç”¨äº Encoder è¾“å‡ºçš„ padding tokenï¼Œshape å¯å¹¿æ’­åˆ° (batch_size, seq_len, enc_seq_len)\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "        out3 -- Decoder Layer è¾“å‡ºï¼Œshape=(batch_size, seq_len, d_model)\n",
    "        attn_weights_block1 -- Masked Self-Attention æƒé‡\n",
    "        attn_weights_block2 -- Encoder-Decoder Attention æƒé‡\n",
    "        \"\"\"\n",
    "        \n",
    "        # ----------------------------\n",
    "        # BLOCK 1: Masked Self-Attention\n",
    "        # ----------------------------\n",
    "        # q=k=v=xï¼Œè¿›è¡Œè‡ªæ³¨æ„åŠ›\n",
    "        # key_padding_mask=look_ahead_mask -> é®ç›–æœªæ¥ä½ç½®\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, key_padding_mask=look_ahead_mask)\n",
    "        # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–\n",
    "        x1 = self.norm1(x + attn1)\n",
    "\n",
    "        # ----------------------------\n",
    "        # BLOCK 2: Encoder-Decoder Attention\n",
    "        # ----------------------------\n",
    "        # query=x1, key/value=enc_output\n",
    "        # key_padding_mask=padding_mask -> é®ç›– Encoder çš„ padding token\n",
    "        attn2, attn_weights_block2 = self.mha2(x1, enc_output, enc_output, key_padding_mask=padding_mask)\n",
    "        # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–\n",
    "        x2 = self.norm2(x1 + attn2)\n",
    "\n",
    "        # ----------------------------\n",
    "        # BLOCK 3: Feed-Forward Network\n",
    "        # ----------------------------\n",
    "        ffn_out = self.dropout(self.ffn(x2))\n",
    "        # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–\n",
    "        out3 = self.norm3(x2 + ffn_out)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderLayer output shape: torch.Size([2, 5, 8])\n",
      "Attention1 shape: torch.Size([2, 5, 5])\n",
      "Attention2 shape: torch.Size([2, 5, 10])\n",
      "DecoderLayer passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• DecoderLayer\n",
    "# ----------------------------\n",
    "\n",
    "# åˆå§‹åŒ– DecoderLayer\n",
    "# d_model=8 -> embedding/hidden ç»´åº¦\n",
    "# num_heads=2 -> å¤šå¤´æ³¨æ„åŠ›å¤´æ•°\n",
    "# dff=16 -> å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦\n",
    "dec_layer = DecoderLayer(d_model=8, num_heads=2, dff=16)\n",
    "\n",
    "# æ¨¡æ‹Ÿ Encoder è¾“å‡º (batch_size=2, seq_len_enc=10, d_model=8)\n",
    "enc_out = torch.rand(2, 10, 8)\n",
    "\n",
    "# æ¨¡æ‹Ÿ Decoder è¾“å…¥ (batch_size=2, seq_len_dec=5, d_model=8)\n",
    "x_tar = torch.rand(2, 5, 8)\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "# ä¸ä½¿ç”¨ maskï¼Œè¿”å›ï¼š\n",
    "# out -> decoder layer è¾“å‡º\n",
    "# attn1 -> masked self-attention æƒé‡\n",
    "# attn2 -> encoder-decoder attention æƒé‡\n",
    "out, attn1, attn2 = dec_layer(x_tar, enc_out)\n",
    "\n",
    "# è¾“å‡ºå½¢çŠ¶ä¿¡æ¯\n",
    "print(\"DecoderLayer output shape:\", out.shape)      # (batch_size, seq_len_dec, d_model) -> (2,5,8)\n",
    "print(\"Attention1 shape:\", attn1.shape)            # masked self-attention æƒé‡ -> (batch_size, seq_len_dec, seq_len_dec)\n",
    "print(\"Attention2 shape:\", attn2.shape)            # encoder-decoder attention æƒé‡ -> (batch_size, seq_len_dec, seq_len_enc)\n",
    "print(\"DecoderLayer passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-2'></a> \n",
    "### 5.2 - å®Œæ•´è§£ç å™¨ (Full Decoder)\n",
    "\n",
    "ä½ å·²ç»æ¥è¿‘å®Œæˆäº†ï¼ç°åœ¨ä½¿ç”¨è§£ç å™¨å±‚ (DecoderLayer) æ¥æ„å»ºå®Œæ•´çš„ Transformer è§£ç å™¨ï¼ˆå›¾ 3bï¼‰ã€‚ä½ éœ€è¦å¯¹è¾“å‡ºè¿›è¡ŒåµŒå…¥ (embedding) å¹¶æ·»åŠ ä½ç½®ç¼–ç ï¼Œç„¶åå°†ç¼–ç åçš„åµŒå…¥è¾“å…¥åˆ°å¤šå±‚è§£ç å™¨å †æ ˆä¸­ã€‚\n",
    "\n",
    "<img src=\"decoder.png\" alt=\"Decoder\" width=\"300\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 3b: Transformer è§£ç å™¨</b></font></center></caption>\n",
    "\n",
    "<a name='ex-7'></a>     \n",
    "### ç»ƒä¹  7 - Decoder\n",
    "\n",
    "ä½¿ç”¨ `forward()` æ–¹æ³•å®ç° `Decoder()`ï¼Œå¯¹è¾“å‡ºè¿›è¡ŒåµŒå…¥ã€æ·»åŠ ä½ç½®ç¼–ç ï¼Œå¹¶å®ç°å¤šå±‚è§£ç å™¨ã€‚\n",
    "\n",
    "åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†åˆå§‹åŒ–è§£ç å™¨ï¼ŒåŒ…å«åµŒå…¥å±‚ã€ä½ç½®ç¼–ç å’Œå¤šä¸ª `DecoderLayer`ã€‚ä½ çš„ `forward()` æ–¹æ³•åº”æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. å°†ç›®æ ‡åºåˆ— token IDs ä¼ å…¥åµŒå…¥å±‚ (Embedding layer)ï¼Œå¾—åˆ° `(batch_size, target_seq_len, d_model)` çš„å‘é‡è¡¨ç¤ºã€‚\n",
    "2. å¯¹åµŒå…¥å‘é‡è¿›è¡Œç¼©æ”¾ (scale)ï¼šä¹˜ä»¥åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹ `sqrt(d_model)`ï¼Œä»¥æé«˜æ¢¯åº¦ç¨³å®šæ€§ã€‚\n",
    "3. æ·»åŠ ä½ç½®ç¼–ç ï¼šå°† `self.pos_encoding[:, :seq_len, :]` åŠ åˆ°åµŒå…¥å‘é‡ä¸Šï¼Œå¹¶ç¡®ä¿ä¸åµŒå…¥åœ¨åŒä¸€ deviceã€‚\n",
    "4. å°†ç¼–ç åçš„åµŒå…¥é€šè¿‡ Dropout å±‚ï¼Œè®­ç»ƒæ¨¡å¼ç”± `self.training` è‡ªåŠ¨æ§åˆ¶ã€‚\n",
    "5. ä½¿ç”¨ for å¾ªç¯å°† Dropout å±‚è¾“å‡ºä¾æ¬¡ä¼ å…¥è§£ç å™¨å †æ ˆä¸­çš„å¤šå±‚ `DecoderLayer`ï¼Œæ¯å±‚è¿”å›ï¼š\n",
    "   - å½“å‰å±‚è¾“å‡º `x`\n",
    "   - è‡ªæ³¨æ„åŠ›æƒé‡ `block1`\n",
    "   - ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æƒé‡ `block2`\n",
    "6. å°†æ¯å±‚çš„æ³¨æ„åŠ›æƒé‡ä¿å­˜åˆ°å­—å…¸ `attention_weights` ä¸­ï¼Œç”¨äºå¯è§†åŒ–æˆ–åˆ†æã€‚\n",
    "7. è¿”å›æœ€ç»ˆè§£ç å™¨è¾“å‡º `x` å’Œæ³¨æ„åŠ›æƒé‡å­—å…¸ `attention_weights`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "McS3by6k4pnP"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Decoderï¼ˆè§£ç å™¨ï¼‰\n",
    "# ----------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                 target_vocab_size, max_pos_enc, dropout=0.1):\n",
    "        \"\"\"\n",
    "        è§£ç å™¨ç»“æ„ï¼š\n",
    "        ç”±åµŒå…¥å±‚ï¼ˆembeddingï¼‰+ ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰+ å¤šå±‚è§£ç å™¨å±‚ï¼ˆDecoderLayerï¼‰ç»„æˆã€‚\n",
    "        \n",
    "        å‚æ•°è¯´æ˜ï¼š\n",
    "        num_layers: intï¼Œè§£ç å™¨å±‚ï¼ˆDecoderLayerï¼‰çš„æ•°é‡ã€‚\n",
    "        d_model: intï¼Œembedding çš„ç»´åº¦ï¼ˆæ¨¡å‹ç‰¹å¾ç»´åº¦ï¼‰ã€‚\n",
    "        num_heads: intï¼Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•°ã€‚\n",
    "        dff: intï¼Œå‰é¦ˆç¥ç»ç½‘ç»œçš„éšè—å±‚ç»´åº¦ï¼ˆfeed-forward network çš„ä¸­é—´å±‚å¤§å°ï¼‰ã€‚\n",
    "        target_vocab_size: intï¼Œç›®æ ‡è¯­è¨€çš„è¯æ±‡è¡¨å¤§å°ï¼Œç”¨äº nn.Embeddingã€‚\n",
    "        max_pos_enc: intï¼Œä½ç½®ç¼–ç çš„æœ€å¤§é•¿åº¦ï¼ˆå¥å­æœ€é•¿é•¿åº¦ï¼‰ã€‚\n",
    "        dropout: floatï¼Œdropout éšæœºå¤±æ´»æ¯”ä¾‹ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n",
    "        \"\"\"\n",
    "        super().__init__()  # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•° nn.Module.__init__()\n",
    "\n",
    "        # ä¿å­˜ç»´åº¦ä¸å±‚æ•°ä¿¡æ¯\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # ----------------------------\n",
    "        # (1) è¯åµŒå…¥å±‚ï¼šå°†è¯IDè½¬æ¢ä¸ºå‘é‡\n",
    "        # ----------------------------\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # ----------------------------\n",
    "        # (2) ä½ç½®ç¼–ç ï¼šä¸ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®æ·»åŠ ä½ç½®ä¿¡æ¯\n",
    "        #    positional_encoding() é€šå¸¸è¿”å›å½¢çŠ¶ [1, max_pos_enc, d_model]\n",
    "        # ----------------------------\n",
    "        self.pos_encoding = positional_encoding(max_pos_enc, d_model)\n",
    "\n",
    "        # ----------------------------\n",
    "        # (3) æ„å»ºå¤šå±‚ DecoderLayer\n",
    "        # æ¯ä¸ª DecoderLayer åŒ…å«ï¼š\n",
    "        #   - Masked Self-Attention\n",
    "        #   - Encoder-Decoder Attention\n",
    "        #   - Feed Forward Network\n",
    "        #   - æ®‹å·®è¿æ¥ + LayerNorm\n",
    "        # ----------------------------\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, dff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ----------------------------\n",
    "        # (4) dropout å±‚\n",
    "        # ----------------------------\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # ----------------------------\n",
    "    # å‰å‘ä¼ æ’­å‡½æ•°\n",
    "    # ----------------------------\n",
    "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼š\n",
    "        è¾“å…¥ç›®æ ‡åºåˆ— token IDsï¼Œå¹¶é€å±‚ç”Ÿæˆè§£ç ç»“æœã€‚\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "        x: Tensorï¼Œå½¢çŠ¶ä¸º (batch_size, target_seq_len)ï¼Œç›®æ ‡åºåˆ—çš„ token IDsã€‚\n",
    "        enc_output: Tensorï¼Œå½¢çŠ¶ä¸º (batch_size, input_seq_len, d_model)ï¼Œæ¥è‡ª Encoder çš„è¾“å‡ºã€‚\n",
    "        look_ahead_mask: Tensorï¼Œé˜²æ­¢æ¨¡å‹çœ‹åˆ°æœªæ¥çš„è¯ï¼ˆç”¨äºè‡ªå›å½’ç”Ÿæˆï¼‰ã€‚\n",
    "        padding_mask: Tensorï¼Œæ©ç ï¼Œç”¨äºå¿½ç•¥å¡«å……ï¼ˆpaddingï¼‰éƒ¨åˆ†ã€‚\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "        x: Tensorï¼Œ(batch_size, target_seq_len, d_model)ï¼Œè§£ç å™¨è¾“å‡ºç‰¹å¾ã€‚\n",
    "        attention_weights: dictï¼ŒåŒ…å«æ¯ä¸€å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¯ç”¨äºå¯è§†åŒ–ã€‚\n",
    "        \"\"\"\n",
    "        # å½“å‰ç›®æ ‡åºåˆ—é•¿åº¦\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # å­˜å‚¨æ³¨æ„åŠ›æƒé‡çš„å­—å…¸\n",
    "        attention_weights = {}\n",
    "\n",
    "        # ----------------------------\n",
    "        # (1) åµŒå…¥ + ä½ç½®ç¼–ç \n",
    "        # ----------------------------\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        x = x * math.sqrt(self.d_model)  # ç¼©æ”¾ï¼Œé˜²æ­¢ embedding å€¼è¿‡å°\n",
    "        x = x + self.pos_encoding[:, :seq_len, :].to(x.device)  # æ·»åŠ ä½ç½®ç¼–ç \n",
    "        x = self.dropout(x)  # dropout é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "\n",
    "        # ----------------------------\n",
    "        # (2) é€šè¿‡å¤šä¸ª DecoderLayer å †å è®¡ç®—\n",
    "        # ----------------------------\n",
    "        for i, layer in enumerate(self.dec_layers):\n",
    "            # æ¯å±‚è¿”å›ä¸‰ä¸ªç»“æœï¼š\n",
    "            # x: å½“å‰å±‚è¾“å‡º\n",
    "            # block1: è‡ªæ³¨æ„åŠ›ï¼ˆMasked Self-Attentionï¼‰æƒé‡\n",
    "            # block2: ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼ˆEnc-Dec Attentionï¼‰æƒé‡\n",
    "            x, block1, block2 = layer(x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "            # å°†æ¯å±‚çš„æ³¨æ„åŠ›æƒé‡ä¿å­˜èµ·æ¥ï¼ˆç”¨äºåç»­åˆ†ææˆ–å¯è§†åŒ–ï¼‰\n",
    "            attention_weights[f'decoder_layer{i+1}_block1_self_att'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2_decenc_att'] = block2\n",
    "\n",
    "        # ----------------------------\n",
    "        # (3) è¾“å‡º\n",
    "        # ----------------------------\n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: torch.Size([2, 5, 8])\n",
      "decoder_layer1_block1_self_att shape: torch.Size([2, 5, 5])\n",
      "decoder_layer1_block2_decenc_att shape: torch.Size([2, 5, 10])\n",
      "decoder_layer2_block1_self_att shape: torch.Size([2, 5, 5])\n",
      "decoder_layer2_block2_decenc_att shape: torch.Size([2, 5, 10])\n",
      "Decoder passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• Decoder\n",
    "# ----------------------------\n",
    "\n",
    "# å®ä¾‹åŒ– Decoder\n",
    "decoder = Decoder(\n",
    "    num_layers=2,       # è§£ç å™¨å±‚æ•°ï¼ˆå³ DecoderLayer å †å æ•°é‡ï¼‰\n",
    "    d_model=8,          # embedding ç»´åº¦\n",
    "    num_heads=2,        # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°\n",
    "    dff=16,             # å‰é¦ˆå±‚éšè—ç»´åº¦\n",
    "    target_vocab_size=50,  # ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°ï¼ˆå³ embedding è¾“å…¥èŒƒå›´ï¼‰\n",
    "    max_pos_enc=100     # ä½ç½®ç¼–ç æœ€å¤§é•¿åº¦\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# åˆ›å»ºéšæœºç›®æ ‡åºåˆ—\n",
    "# ----------------------------\n",
    "x_tar_ids = torch.randint(0, 50, (2, 5))\n",
    "# è¯´æ˜ï¼š\n",
    "#   batch_size = 2\n",
    "#   target_seq_len = 5\n",
    "#   æ¯ä¸ª token id èŒƒå›´ [0, 49]\n",
    "# ä¾‹å¦‚ï¼š[[12, 3, 48, 7, 1], [9, 17, 0, 33, 2]]\n",
    "\n",
    "# ----------------------------\n",
    "# å‰å‘ä¼ æ’­ï¼šè¾“å…¥è§£ç å™¨\n",
    "# ----------------------------\n",
    "# enc_out æ¥è‡ª Encoder æµ‹è¯•éƒ¨åˆ†ï¼Œæ˜¯ (2, 10, 8)\n",
    "# ä»£è¡¨ç¼–ç å™¨è¾“å‡ºçš„è¯­ä¹‰ç‰¹å¾åºåˆ—\n",
    "out, attention_weights = decoder(x_tar_ids, enc_out)\n",
    "\n",
    "# ----------------------------\n",
    "# éªŒè¯è¾“å‡ºç»´åº¦\n",
    "# ----------------------------\n",
    "print(\"Decoder output shape:\", out.shape)\n",
    "# é¢„æœŸç»“æœï¼š\n",
    "#   (batch_size=2, target_seq_len=5, d_model=8)\n",
    "#   æ¯ä¸ªç›®æ ‡ token å¯¹åº”ä¸€ä¸ª 8ç»´è¯­ä¹‰å‘é‡\n",
    "\n",
    "# ----------------------------\n",
    "# æ‰“å°æ³¨æ„åŠ›æƒé‡å½¢çŠ¶\n",
    "# ----------------------------\n",
    "for k, v in attention_weights.items():\n",
    "    print(f\"{k} shape: {v.shape}\")\n",
    "\n",
    "# è¾“å‡ºç¤ºä¾‹ï¼š\n",
    "# decoder_layer1_block1_self_att shape: torch.Size([2, 5, 5])\n",
    "# decoder_layer1_block2_decenc_att shape: torch.Size([2, 5, 10])\n",
    "# decoder_layer2_block1_self_att shape: torch.Size([2, 5, 5])\n",
    "# decoder_layer2_block2_decenc_att shape: torch.Size([2, 5, 10])\n",
    "\n",
    "# ----------------------------\n",
    "# æˆåŠŸæç¤º\n",
    "# ----------------------------\n",
    "print(\"Decoder passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a> \n",
    "## 6 - Transformer\n",
    "\n",
    "å‘¼ï¼è¿™æ¬¡ä½œä¸šçœŸçš„å¾ˆé•¿ï¼Œç°åœ¨ä½ å·²ç»æ¥åˆ°äº†æ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹çš„æœ€åä¸€ä¸ªç»ƒä¹ ã€‚æ­å–œï¼ä½ å·²ç»å®Œæˆäº†æ‰€æœ‰çš„æ ¸å¿ƒå·¥ä½œï¼Œç°åœ¨æ˜¯æ—¶å€™æŠŠæ‰€æœ‰æ¨¡å—æ•´åˆèµ·æ¥äº†ã€‚  \n",
    "\n",
    "<img src=\"transformer.png\" alt=\"Transformer\" width=\"550\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 4: Transformer</b></font></center></caption>\n",
    "    \n",
    "Transformer æ¶æ„çš„æ•°æ®æµå¦‚ä¸‹ï¼š\n",
    "\n",
    "* **ç¼–ç å™¨ (Encoder)**  \n",
    "    - è¾“å…¥é€šè¿‡åµŒå…¥å±‚å’Œä½ç½®ç¼–ç è¿›è¡Œå¤„ç†  \n",
    "    - å¤šå±‚ EncoderLayer å †å ï¼Œæ‰§è¡Œè‡ªæ³¨æ„åŠ› (Multi-Head Self-Attention)  \n",
    "    - å‰é¦ˆç¥ç»ç½‘ç»œ (Feed Forward Network) æå–ç‰¹å¾  \n",
    "    - è¾“å‡ºç¼–ç åçš„ç‰¹å¾ `enc_output`  \n",
    "\n",
    "* **è§£ç å™¨ (Decoder)**  \n",
    "    - ç›®æ ‡åºåˆ—é€šè¿‡åµŒå…¥å±‚å’Œä½ç½®ç¼–ç å¤„ç†  \n",
    "    - Block 1: Masked Self-Attentionï¼Œé˜²æ­¢é¢„æµ‹æœªæ¥ä¿¡æ¯  \n",
    "    - Block 2: Encoder-Decoder Attentionï¼Œä½¿ç”¨æ¥è‡ªç¼–ç å™¨çš„ Kã€V å’Œ Block 1 çš„ Q  \n",
    "    - Block 3: å‰é¦ˆç¥ç»ç½‘ç»œå¸®åŠ©æå–ç‰¹å¾  \n",
    "    - å¤šå±‚ DecoderLayer å †å ï¼Œè¾“å‡ºè§£ç å™¨ç‰¹å¾ `dec_output`  \n",
    "    - æ³¨æ„åŠ›æƒé‡è¢«ä¿å­˜åˆ°å­—å…¸ `attention_weights`ï¼Œå¯ç”¨äºå¯è§†åŒ–  \n",
    "\n",
    "* **è¾“å‡ºå±‚**  \n",
    "    - å°† Decoder è¾“å‡ºé€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°  \n",
    "    - ä½¿ç”¨ softmax è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œç”Ÿæˆåºåˆ—ä¸­æ¯ä¸ª token çš„é¢„æµ‹  \n",
    "\n",
    "<a name='ex-8'></a> \n",
    "### ç»ƒä¹  8 - Transformer\n",
    "\n",
    "ä½¿ç”¨ `forward()` æ–¹æ³•å®ç° `Transformer()`ï¼š\n",
    "\n",
    "1. å°†è¾“å…¥ token IDs é€šè¿‡ Encoderï¼Œä½¿ç”¨é€‚å½“çš„ padding maskã€‚  \n",
    "2. å°† Encoder è¾“å‡ºå’Œç›®æ ‡åºåˆ— token IDs é€šè¿‡ Decoderï¼Œä½¿ç”¨å‰ç» mask å’Œ Encoder padding maskã€‚  \n",
    "3. å°† Decoder è¾“å‡ºé€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ°ç›®æ ‡è¯æ±‡è¡¨ç»´åº¦ï¼Œå¹¶ä½¿ç”¨ softmax ç”Ÿæˆé¢„æµ‹æ¦‚ç‡ã€‚  \n",
    "4. è¿”å›æœ€ç»ˆè¾“å‡ºæ¦‚ç‡ `final_output` å’Œæ³¨æ„åŠ›æƒé‡å­—å…¸ `attention_weights`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Transformer æ¨¡å—\n",
    "# ----------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    å®Œæ•´ Transformer æ¨¡å‹ï¼š\n",
    "    - åŒ…å« Encoder + Decoder\n",
    "    - è¾“å‡ºå±‚å°† decoder çš„è¾“å‡ºæ˜ å°„åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°\n",
    "    - é€‚ç”¨äºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆä¾‹å¦‚ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆï¼‰\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size,\n",
    "                 max_pos_enc_input, max_pos_enc_target):\n",
    "        super().__init__()\n",
    "\n",
    "        # ===================== ç¼–ç å™¨ =====================\n",
    "        # Encoder ç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆï¼š\n",
    "        # 1) è¾“å…¥ token embedding\n",
    "        # 2) ä½ç½®ç¼–ç  (Positional Encoding)\n",
    "        # 3) å¤šå±‚ EncoderLayer å †å ï¼Œæ¯å±‚åŒ…æ‹¬ï¼š\n",
    "        #    - Multi-Head Self Attention\n",
    "        #    - å‰é¦ˆå…¨è¿æ¥ç½‘ç»œ (Feed-Forward Network)\n",
    "        #    - æ®‹å·®è¿æ¥ + LayerNorm\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_layers,               # Encoder å±‚æ•°ï¼ˆNï¼‰\n",
    "            embedding_dim=d_model,               # embedding ç»´åº¦ d_model\n",
    "            num_heads=num_heads,                 # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°\n",
    "            fully_connected_dim=dff,             # å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦\n",
    "            input_vocab_size=input_vocab_size,   # è¾“å…¥è¯è¡¨å¤§å°\n",
    "            maximum_position_encoding=max_pos_enc_input  # æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦\n",
    "        )\n",
    "\n",
    "        # ===================== è§£ç å™¨ =====================\n",
    "        # Decoder ç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆï¼š\n",
    "        # 1) ç›®æ ‡ token embedding\n",
    "        # 2) ä½ç½®ç¼–ç \n",
    "        # 3) å¤šå±‚ DecoderLayer å †å ï¼Œæ¯å±‚åŒ…æ‹¬ï¼š\n",
    "        #    - Masked Multi-Head Self Attention (é˜²æ­¢é¢„æµ‹æœªæ¥ä¿¡æ¯)\n",
    "        #    - Multi-Head Attention over Encoder Output (Encoder-Decoder Attention)\n",
    "        #    - å‰é¦ˆå…¨è¿æ¥ç½‘ç»œ\n",
    "        #    - æ®‹å·®è¿æ¥ + LayerNorm\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_layers,               # Decoder å±‚æ•°\n",
    "            d_model=d_model,                     # embedding ç»´åº¦\n",
    "            num_heads=num_heads,                 # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°\n",
    "            dff=dff,                             # å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦\n",
    "            target_vocab_size=target_vocab_size, # ç›®æ ‡è¯è¡¨å¤§å°\n",
    "            max_pos_enc=max_pos_enc_target       # æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦\n",
    "        )\n",
    "\n",
    "        # ===================== è¾“å‡ºå±‚ =====================\n",
    "        # å°† decoder è¾“å‡ºçš„ç‰¹å¾æ˜ å°„åˆ°è¯è¡¨ç»´åº¦\n",
    "        # (batch_size, target_seq_len, d_model) -> (batch_size, target_seq_len, target_vocab_size)\n",
    "        self.final_linear = nn.Linear(d_model, target_vocab_size)\n",
    "        # å¯¹æ¯ä¸ª token çš„è¾“å‡º logits è¿›è¡Œ softmax è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    # ----------------------------\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    # ----------------------------\n",
    "    def forward(self, inp, tar, enc_padding_mask=None,\n",
    "                look_ahead_mask=None, dec_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Transformer å‰å‘ä¼ æ’­\n",
    "        Arguments:\n",
    "            inp: è¾“å…¥ token ids, å½¢çŠ¶ (batch_size, input_seq_len)\n",
    "            tar: ç›®æ ‡ token ids, å½¢çŠ¶ (batch_size, target_seq_len)\n",
    "            enc_padding_mask: Encoder è¾“å…¥ padding mask, (batch_size, 1, input_seq_len)\n",
    "            look_ahead_mask: Decoder å‰ç» mask, (target_seq_len, target_seq_len)\n",
    "            dec_padding_mask: Decoder padding mask, (batch_size, 1, input_seq_len)\n",
    "        Returns:\n",
    "            final_output: è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ, (batch_size, target_seq_len, target_vocab_size)\n",
    "            attention_weights: æ³¨æ„åŠ›æƒé‡å­—å…¸ï¼ŒåŒ…å«æ¯ä¸€å±‚çš„è‡ªæ³¨æ„åŠ›å’Œç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›\n",
    "        \"\"\"\n",
    "\n",
    "        # --------------------\n",
    "        # 1. ç¼–ç å™¨\n",
    "        # --------------------\n",
    "        # å°†è¾“å…¥ token ids é€å…¥ Encoder\n",
    "        # è¾“å‡º enc_output: (batch_size, input_seq_len, d_model)\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)\n",
    "\n",
    "        # --------------------\n",
    "        # 2. è§£ç å™¨\n",
    "        # --------------------\n",
    "        # å°†ç›®æ ‡ token ids å’Œ encoder è¾“å‡ºé€å…¥ Decoder\n",
    "        # è¾“å‡º dec_output: (batch_size, target_seq_len, d_model)\n",
    "        # attention_weights: å­—å…¸ï¼Œå­˜å‚¨æ¯ä¸€å±‚æ³¨æ„åŠ›æƒé‡ï¼Œå¯ç”¨äºå¯è§†åŒ–\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, look_ahead_mask, dec_padding_mask\n",
    "        )\n",
    "\n",
    "        # --------------------\n",
    "        # 3. è¾“å‡ºå±‚\n",
    "        # --------------------\n",
    "        # Linear: å°†è§£ç å™¨è¾“å‡ºæ˜ å°„åˆ°ç›®æ ‡è¯è¡¨å¤§å°\n",
    "        final_output = self.final_linear(dec_output)      # (batch_size, target_seq_len, target_vocab_size)\n",
    "        # Softmax: è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ\n",
    "        final_output = self.softmax(final_output)\n",
    "\n",
    "        # è¿”å›è¾“å‡ºæ¦‚ç‡åŠæ³¨æ„åŠ›æƒé‡\n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥åºåˆ—ç´¢å¼• (inp): tensor([[47, 21, 29, 17, 28, 45, 27, 46,  5, 40],\n",
      "        [13,  4, 23, 37, 19, 14, 12, 40, 21,  6]])\n",
      "ç›®æ ‡åºåˆ—ç´¢å¼• (tar): tensor([[43, 20, 38, 11, 42, 28, 26],\n",
      "        [15, 21, 34,  2, 39, 26, 41]])\n",
      "\n",
      "Transformer è¾“å‡ºå½¢çŠ¶: torch.Size([2, 7, 50])\n",
      "Transformer è¾“å‡ºç¤ºä¾‹ï¼ˆç¬¬ä¸€æ¡åºåˆ—ï¼Œå‰ä¸‰ä¸ª tokenï¼Œå‰äº”ä¸ª vocab æ¦‚ç‡ï¼‰:\n",
      "tensor([[0.0186, 0.0100, 0.0132, 0.0056, 0.0149],\n",
      "        [0.0109, 0.0131, 0.0142, 0.0105, 0.0521],\n",
      "        [0.0259, 0.0327, 0.0132, 0.0133, 0.0101]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "è¾“å‡ºç»Ÿè®¡ä¿¡æ¯:\n",
      "æœ€å¤§å€¼: 0.09744833409786224\n",
      "æœ€å°å€¼: 0.0032348392996937037\n",
      "å¹³å‡å€¼: 0.019999999552965164\n",
      "ç¬¬ä¸€ä¸ª token çš„ vocab æ¦‚ç‡å’Œï¼ˆåº”ä¸º 1ï¼‰: 1.0\n",
      "\n",
      "æ³¨æ„åŠ›æƒé‡ key åˆ—è¡¨: dict_keys(['decoder_layer1_block1_self_att', 'decoder_layer1_block2_decenc_att', 'decoder_layer2_block1_self_att', 'decoder_layer2_block2_decenc_att'])\n",
      "\n",
      "ç¬¬ä¸€å±‚è‡ªæ³¨æ„åŠ›ç¤ºä¾‹ (decoder_layer1_block1_self_att)ï¼Œç¬¬ä¸€æ¡åºåˆ—ï¼Œå‰ä¸‰ä¸ª query çš„æ³¨æ„åŠ›åˆ†å¸ƒ:\n",
      "tensor([[1.7475e-02, 4.3847e-02, 4.7683e-01, 1.5862e-02, 1.6917e-02, 9.9759e-02,\n",
      "         3.2931e-01],\n",
      "        [2.4769e-01, 6.4847e-02, 4.3777e-01, 6.2865e-02, 2.8386e-02, 1.4510e-01,\n",
      "         1.3342e-02],\n",
      "        [2.0507e-02, 1.4190e-03, 7.0171e-05, 9.9357e-02, 2.9329e-01, 9.6701e-02,\n",
      "         4.8865e-01]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "ç¬¬ä¸€å±‚ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ç¤ºä¾‹ (decoder_layer1_block2_decenc_att)ï¼Œç¬¬ä¸€æ¡åºåˆ—ï¼Œå‰ä¸‰ä¸ª query çš„æ³¨æ„åŠ›åˆ†å¸ƒ:\n",
      "tensor([[0.0927, 0.0983, 0.1082, 0.0779, 0.0956, 0.0932, 0.1110, 0.1195, 0.0819,\n",
      "         0.1218],\n",
      "        [0.0905, 0.1004, 0.1336, 0.0895, 0.1077, 0.0625, 0.0758, 0.1079, 0.1224,\n",
      "         0.1097],\n",
      "        [0.0838, 0.1155, 0.1192, 0.0804, 0.1560, 0.0481, 0.0902, 0.0834, 0.1184,\n",
      "         0.1049]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Transformer æµ‹è¯•å®Œæˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luxia\\AppData\\Local\\Temp\\ipykernel_20508\\1819583349.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pos_encoding = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# æµ‹è¯• Transformer\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    import math\n",
    "\n",
    "    # --------------------\n",
    "    # è¶…å‚æ•°è®¾ç½®\n",
    "    # --------------------\n",
    "    num_layers = 2                  # ç¼–ç å™¨/è§£ç å™¨å †å å±‚æ•°\n",
    "    d_model = 8                     # embedding ç»´åº¦\n",
    "    num_heads = 2                   # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°\n",
    "    dff = 16                        # å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦\n",
    "    input_vocab_size = 50           # è¾“å…¥è¯è¡¨å¤§å°\n",
    "    target_vocab_size = 50          # ç›®æ ‡è¯è¡¨å¤§å°\n",
    "    max_pos_enc_input = 100         # è¾“å…¥æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦\n",
    "    max_pos_enc_target = 100        # è¾“å‡ºæœ€å¤§ä½ç½®ç¼–ç é•¿åº¦\n",
    "    batch_size = 2                  # æ‰¹é‡å¤§å°\n",
    "    inp_seq_len = 10                # è¾“å…¥åºåˆ—é•¿åº¦\n",
    "    tar_seq_len = 7                 # è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆç›®æ ‡åºåˆ—ï¼‰\n",
    "\n",
    "    # --------------------\n",
    "    # æ¨¡æ‹Ÿè¾“å…¥ token IDs\n",
    "    # --------------------\n",
    "    inp = torch.randint(0, input_vocab_size, (batch_size, inp_seq_len))   # è¾“å…¥ token id éšæœºç”Ÿæˆ\n",
    "    tar = torch.randint(0, target_vocab_size, (batch_size, tar_seq_len))  # ç›®æ ‡ token id éšæœºç”Ÿæˆ\n",
    "\n",
    "    print(\"è¾“å…¥åºåˆ—ç´¢å¼• (inp):\", inp)\n",
    "    print(\"ç›®æ ‡åºåˆ—ç´¢å¼• (tar):\", tar)\n",
    "\n",
    "    # --------------------\n",
    "    # åˆå§‹åŒ– Transformer\n",
    "    # --------------------\n",
    "    transformer = Transformer(\n",
    "        num_layers=num_layers,                       # å±‚æ•°\n",
    "        d_model=d_model,                             # embedding ç»´åº¦\n",
    "        num_heads=num_heads,                         # æ³¨æ„åŠ›å¤´æ•°\n",
    "        dff=dff,                                     # å‰é¦ˆç½‘ç»œç»´åº¦\n",
    "        input_vocab_size=input_vocab_size,           # è¾“å…¥è¯è¡¨å¤§å°\n",
    "        target_vocab_size=target_vocab_size,         # è¾“å‡ºè¯è¡¨å¤§å°\n",
    "        max_pos_enc_input=max_pos_enc_input,         # æœ€å¤§è¾“å…¥ä½ç½®ç¼–ç \n",
    "        max_pos_enc_target=max_pos_enc_target        # æœ€å¤§è¾“å‡ºä½ç½®ç¼–ç \n",
    "    )\n",
    "\n",
    "    # --------------------\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    # --------------------\n",
    "    # è¾“å…¥ inp (batch_size, inp_seq_len)\n",
    "    # ç›®æ ‡ tar (batch_size, tar_seq_len)\n",
    "    # è¿”å›:\n",
    "    #   out: Transformer è¾“å‡ºæ¦‚ç‡ (batch_size, tar_seq_len, target_vocab_size)\n",
    "    #   attn: æ³¨æ„åŠ›æƒé‡å­—å…¸\n",
    "    out, attn = transformer(inp, tar)\n",
    "\n",
    "    # --------------------\n",
    "    # è¾“å‡ºå½¢çŠ¶\n",
    "    # --------------------\n",
    "    print(\"\\nTransformer è¾“å‡ºå½¢çŠ¶:\", out.shape)  \n",
    "    # è¾“å‡ºç¤ºä¾‹ (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    print(\"Transformer è¾“å‡ºç¤ºä¾‹ï¼ˆç¬¬ä¸€æ¡åºåˆ—ï¼Œå‰ä¸‰ä¸ª tokenï¼Œå‰äº”ä¸ª vocab æ¦‚ç‡ï¼‰:\")\n",
    "    # æ‰“å°ç¬¬ä¸€æ¡åºåˆ—çš„å‰ä¸‰ä¸ª token çš„å‰äº”ä¸ªè¯è¡¨æ¦‚ç‡\n",
    "    print(out[0, :3, :5])\n",
    "\n",
    "    # --------------------\n",
    "    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯\n",
    "    # --------------------\n",
    "    print(\"\\nè¾“å‡ºç»Ÿè®¡ä¿¡æ¯:\")\n",
    "    print(\"æœ€å¤§å€¼:\", out.max().item())          # è¾“å‡ºæ¦‚ç‡æœ€å¤§å€¼\n",
    "    print(\"æœ€å°å€¼:\", out.min().item())          # è¾“å‡ºæ¦‚ç‡æœ€å°å€¼\n",
    "    print(\"å¹³å‡å€¼:\", out.mean().item())        # è¾“å‡ºæ¦‚ç‡å‡å€¼\n",
    "    print(\"ç¬¬ä¸€ä¸ª token çš„ vocab æ¦‚ç‡å’Œï¼ˆåº”ä¸º 1ï¼‰:\", out[0,0,:].sum().item())  # softmax å’Œåº”ä¸º1\n",
    "\n",
    "    # --------------------\n",
    "    # æ³¨æ„åŠ›æƒé‡ç¤ºä¾‹\n",
    "    # --------------------\n",
    "    print(\"\\næ³¨æ„åŠ›æƒé‡ key åˆ—è¡¨:\", attn.keys())  # æ‰“å°æ³¨æ„åŠ›æƒé‡å­—å…¸çš„ key\n",
    "\n",
    "    # æ‰“å°ç¬¬ä¸€å±‚ Decoder è‡ªæ³¨æ„åŠ›ç¤ºä¾‹\n",
    "    first_layer_key = 'decoder_layer1_block1_self_att'\n",
    "    if first_layer_key in attn:\n",
    "        attn_tensor = attn[first_layer_key]  # shape: (batch_size, tar_seq_len, tar_seq_len)\n",
    "        print(f\"\\nç¬¬ä¸€å±‚è‡ªæ³¨æ„åŠ›ç¤ºä¾‹ ({first_layer_key})ï¼Œç¬¬ä¸€æ¡åºåˆ—ï¼Œå‰ä¸‰ä¸ª query çš„æ³¨æ„åŠ›åˆ†å¸ƒ:\")\n",
    "        print(attn_tensor[0, :3, :])  # æ‰“å°å‰ä¸‰ä¸ª query çš„æ³¨æ„åŠ›åˆ†å¸ƒ\n",
    "\n",
    "    # æ‰“å°ç¬¬ä¸€å±‚ Decoder ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ç¤ºä¾‹\n",
    "    first_layer_encdec_key = 'decoder_layer1_block2_decenc_att'\n",
    "    if first_layer_encdec_key in attn:\n",
    "        attn_tensor = attn[first_layer_encdec_key]  # shape: (batch_size, tar_seq_len, inp_seq_len)\n",
    "        print(f\"\\nç¬¬ä¸€å±‚ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ç¤ºä¾‹ ({first_layer_encdec_key})ï¼Œç¬¬ä¸€æ¡åºåˆ—ï¼Œå‰ä¸‰ä¸ª query çš„æ³¨æ„åŠ›åˆ†å¸ƒ:\")\n",
    "        print(attn_tensor[0, :3, :])  # æ‰“å°å‰ä¸‰ä¸ª query çš„æ³¨æ„åŠ›åˆ†å¸ƒ\n",
    "\n",
    "    print(\"\\nTransformer æµ‹è¯•å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»“è®º\n",
    "\n",
    "ä½ å·²ç»å®Œæˆäº†æœ¬æ¬¡ä½œä¸šçš„è¯„åˆ†éƒ¨åˆ†ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»æŒæ¡äº†ä»¥ä¸‹å†…å®¹ï¼š\n",
    "\n",
    "* åˆ›å»ºä½ç½®ç¼–ç  (Positional Encodings) æ¥æ•æ‰æ•°æ®ä¸­çš„åºåˆ—å…³ç³»\n",
    "* ä½¿ç”¨è¯åµŒå…¥ (Word Embeddings) è®¡ç®—ç¼©æ”¾ç‚¹ç§¯è‡ªæ³¨æ„åŠ› (Scaled Dot-Product Self-Attention)\n",
    "* å®ç°å¸¦æ©ç çš„å¤šå¤´æ³¨æ„åŠ› (Masked Multi-Head Attention)\n",
    "* æ„å»ºå¹¶è®­ç»ƒä¸€ä¸ª Transformer æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    <b>ä½ éœ€è¦è®°ä½çš„å†…å®¹</b>ï¼š\n",
    "\n",
    "- è‡ªæ³¨æ„åŠ› (Self-Attention) ä¸å·ç§¯ç½‘ç»œå±‚ (Convolutional Network Layers) çš„ç»“åˆèƒ½å¤Ÿå®ç°è®­ç»ƒçš„å¹¶è¡ŒåŒ–ï¼Œä»è€ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚\n",
    "- è‡ªæ³¨æ„åŠ›æ˜¯é€šè¿‡ç”Ÿæˆçš„æŸ¥è¯¢çŸ©é˜µ Qã€é”®çŸ©é˜µ K å’Œå€¼çŸ©é˜µ V æ¥è®¡ç®—çš„ã€‚\n",
    "- å°†ä½ç½®ç¼–ç  (Positional Encoding) æ·»åŠ åˆ°è¯åµŒå…¥ (Word Embeddings) ä¸­ï¼Œæ˜¯åœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­å¼•å…¥åºåˆ—ä¿¡æ¯çš„æœ‰æ•ˆæ–¹æ³•ã€‚\n",
    "- å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention) æœ‰åŠ©äºæ£€æµ‹å¥å­ä¸­çš„å¤šä¸ªç‰¹å¾ã€‚\n",
    "- æ©ç  (Masking) é˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­â€œå·çœ‹â€æœªæ¥ä¿¡æ¯ï¼Œæˆ–è€…åœ¨å¤„ç†æˆªæ–­å¥å­æ—¶è¿‡åº¦åŠ æƒé›¶å€¼ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ä½ å·²ç»å®Œæˆäº† Transformer ä½œä¸šã€‚\n",
    "\n",
    "# æ­å–œä½ å®Œæˆæ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹ï¼ï¼ï¼ï¼ ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
    "\n",
    "\n",
    "<a name='7'></a> \n",
    "## 7 - å‚è€ƒæ–‡çŒ®\n",
    "\n",
    "Transformer ç®—æ³•ç”± Vaswani ç­‰äººæå‡º (2017)ã€‚\n",
    "\n",
    "- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
