{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbzZLqIPv6b7",
    "outputId": "19f2fc2b-6f1d-4b43-fd50-4c513e3936fd"
   },
   "source": [
    "# Transformer ç½‘ç»œ\n",
    "\n",
    "æ¬¢è¿æ¥åˆ°ç¬¬ 4 å‘¨çš„ä½œä¸šï¼Œè¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹ï¼ˆDeep Learning Specializationï¼‰ç¬¬ 5 é—¨è¯¾ç¨‹çš„æœ€åä¸€ä¸ªä½œä¸šï¼  \n",
    "åŒæ—¶ï¼Œæ­å–œä½ èµ°åˆ°æ•´ä¸ªæ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹çš„æœ€åä¸€ä»½ä½œä¸šâ€”â€”ä½ å·²ç»æ¥è¿‘å®Œæˆæ•´ä¸ªè¯¾ç¨‹äº†ï¼\n",
    "\n",
    "åœ¨è¯¾ç¨‹çš„å‰é¢éƒ¨åˆ†ï¼Œä½ å·²ç»å®ç°äº†é¡ºåºç¥ç»ç½‘ç»œï¼ˆsequential neural networksï¼‰ï¼Œå¦‚ RNNã€GRU å’Œ LSTMã€‚  \n",
    "åœ¨æœ¬ç¬”è®°ä¸­ï¼Œä½ å°†æ¢ç´¢ **Transformer æ¶æ„**ï¼Œè¿™æ˜¯ä¸€ç§å¯ä»¥åˆ©ç”¨å¹¶è¡Œå¤„ç†å¹¶æ˜¾è‘—åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹çš„ç¥ç»ç½‘ç»œã€‚\n",
    "\n",
    "**å®Œæˆæœ¬ä½œä¸šåï¼Œä½ å°†èƒ½å¤Ÿï¼š**\n",
    "\n",
    "* åˆ›å»ºä½ç½®ç¼–ç ï¼ˆpositional encodingsï¼‰ä»¥æ•æ‰æ•°æ®ä¸­çš„åºåˆ—å…³ç³»\n",
    "* ä½¿ç”¨è¯å‘é‡è®¡ç®—ç¼©æ”¾ç‚¹ç§¯è‡ªæ³¨æ„åŠ›ï¼ˆscaled dot-product self-attentionï¼‰\n",
    "* å®ç°æ©ç å¤šå¤´æ³¨æ„åŠ›ï¼ˆmasked multi-head attentionï¼‰\n",
    "* æ„å»ºå¹¶è®­ç»ƒä¸€ä¸ª Transformer æ¨¡å‹\n",
    "\n",
    "æœ€åä¸€æ¬¡ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_angles passed!\n",
      "positional_encoding passed!\n",
      "scaled_dot_product_attention passed!\n",
      "EncoderLayer passed!\n",
      "Encoder passed!\n",
      "DecoderLayer passed!\n",
      "Decoder passed!\n",
      "Transformer passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# UNQ_C1: get_angles\n",
    "# -------------------------------\n",
    "def get_angles(pos, k, d):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \"\"\"\n",
    "    i = k\n",
    "    angles = pos / np.power(10000, (2 * (i // 2)) / np.float32(d))\n",
    "    return angles\n",
    "\n",
    "# -------------------------------\n",
    "# get_angles æµ‹è¯•\n",
    "# -------------------------------\n",
    "pos_m = np.arange(4)[:, np.newaxis]\n",
    "dims = np.arange(8)[np.newaxis, :]\n",
    "angles = get_angles(pos_m, dims, 8)\n",
    "assert angles.shape == (4, 8)\n",
    "print(\"get_angles passed!\")\n",
    "\n",
    "# -------------------------------\n",
    "# UNQ_C2: positional_encoding\n",
    "# -------------------------------\n",
    "def positional_encoding(positions, d):\n",
    "    angle_rads = get_angles(np.arange(positions)[:, np.newaxis],\n",
    "                            np.arange(d)[np.newaxis, :],\n",
    "                            d)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# positional_encoding æµ‹è¯•\n",
    "# -------------------------------\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "assert pos_encoding.shape == (1, 50, 512)\n",
    "print(\"positional_encoding passed!\")\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "# plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "# plt.xlabel('d')\n",
    "# plt.ylabel('Position')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# UNQ_C3: scaled_dot_product_attention\n",
    "# -------------------------------\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "    dk = k.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, float('-1e9'))\n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "# -------------------------------\n",
    "# scaled_dot_product_attention æµ‹è¯•\n",
    "# -------------------------------\n",
    "q = torch.rand(1, 3, 4)\n",
    "k = torch.rand(1, 3, 4)\n",
    "v = torch.rand(1, 3, 4)\n",
    "out, attn = scaled_dot_product_attention(q, k, v)\n",
    "assert out.shape == (1, 3, 4)\n",
    "assert attn.shape == (1, 3, 3)\n",
    "print(\"scaled_dot_product_attention passed!\")\n",
    "\n",
    "# -------------------------------\n",
    "# FullyConnected\n",
    "# -------------------------------\n",
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(embedding_dim, fully_connected_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(fully_connected_dim, embedding_dim)\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# UNQ_C4: EncoderLayer\n",
    "# -------------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim, eps=layernorm_eps)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "# -------------------------------\n",
    "# EncoderLayer æµ‹è¯•\n",
    "# -------------------------------\n",
    "x = torch.rand(2, 5, 32)\n",
    "layer = EncoderLayer(32, 4, 64)\n",
    "out = layer(x)\n",
    "assert out.shape == (2, 5, 32)\n",
    "print(\"EncoderLayer passed!\")\n",
    "\n",
    "# -------------------------------\n",
    "# UNQ_C5: Encoder\n",
    "# -------------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, embedding_dim)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(embedding_dim, num_heads, fully_connected_dim, dropout_rate) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) * np.sqrt(self.embedding.embedding_dim)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# Encoder æµ‹è¯•\n",
    "# -------------------------------\n",
    "x = torch.randint(0, 50, (2, 10))\n",
    "enc = Encoder(num_layers=2, embedding_dim=32, num_heads=4, fully_connected_dim=64, input_vocab_size=50, maximum_position_encoding=100)\n",
    "out = enc(x)\n",
    "assert out.shape == (2, 10, 32)\n",
    "print(\"Encoder passed!\")\n",
    "\n",
    "# -------------------------------\n",
    "# UNQ_C6: DecoderLayer\n",
    "# -------------------------------\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha1 = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.layernorm3 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        attn1, attn_w1 = self.mha1(x, x, x)\n",
    "        x1 = self.layernorm1(x + attn1)\n",
    "        attn2, attn_w2 = self.mha2(x1, enc_output, enc_output)\n",
    "        x2 = self.layernorm2(x1 + attn2)\n",
    "        ffn_output = self.ffn(x2)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        out = self.layernorm3(x2 + ffn_output)\n",
    "        return out, attn_w1, attn_w2\n",
    "\n",
    "# -------------------------------\n",
    "# DecoderLayer æµ‹è¯•\n",
    "# -------------------------------\n",
    "x = torch.rand(2, 5, 32)\n",
    "enc_out = torch.rand(2, 6, 32)\n",
    "dec_layer = DecoderLayer(32, 4, 64)\n",
    "out, a1, a2 = dec_layer(x, enc_out)\n",
    "assert out.shape == (2, 5, 32)\n",
    "print(\"DecoderLayer passed!\")\n",
    "\n",
    "# -------------------------------\n",
    "# UNQ_C7: Decoder\n",
    "# -------------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(target_vocab_size, embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, embedding_dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(embedding_dim, num_heads, fully_connected_dim, dropout_rate) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) * np.sqrt(self.embedding.embedding_dim)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        attn_weights = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, attn1, attn2 = layer(x, enc_output)\n",
    "            attn_weights[f'decoder_layer{i+1}_block1'] = attn1\n",
    "            attn_weights[f'decoder_layer{i+1}_block2'] = attn2\n",
    "        return x, attn_weights\n",
    "\n",
    "# -------------------------------\n",
    "# Decoder æµ‹è¯•\n",
    "# -------------------------------\n",
    "x = torch.randint(0, 60, (2, 5))\n",
    "enc_out = torch.rand(2, 6, 32)\n",
    "dec = Decoder(num_layers=2, embedding_dim=32, num_heads=4, fully_connected_dim=64, target_vocab_size=60, maximum_position_encoding=100)\n",
    "out, attn = dec(x, enc_out)\n",
    "assert out.shape == (2, 5, 32)\n",
    "print(\"Decoder passed!\")\n",
    "\n",
    "# -------------------------------\n",
    "# UNQ_C8: Transformer\n",
    "# -------------------------------\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 input_vocab_size, target_vocab_size, max_pos_enc_input, max_pos_enc_target, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "                               input_vocab_size, max_pos_enc_input, dropout_rate)\n",
    "        self.decoder = Decoder(num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "                               target_vocab_size, max_pos_enc_target, dropout_rate)\n",
    "        self.final_layer = nn.Linear(embedding_dim, target_vocab_size)\n",
    "\n",
    "    def forward(self, inp, tar):\n",
    "        enc_output = self.encoder(inp)\n",
    "        dec_output, attn_weights = self.decoder(tar, enc_output)\n",
    "        final_output = F.softmax(self.final_layer(dec_output), dim=-1)\n",
    "        return final_output, attn_weights\n",
    "\n",
    "# -------------------------------\n",
    "# Transformer æµ‹è¯•\n",
    "# -------------------------------\n",
    "inp = torch.randint(0, 50, (2, 10))\n",
    "tar = torch.randint(0, 60, (2, 8))\n",
    "model = Transformer(num_layers=2, embedding_dim=32, num_heads=4, fully_connected_dim=64,\n",
    "                    input_vocab_size=50, target_vocab_size=60,\n",
    "                    max_pos_enc_input=100, max_pos_enc_target=100)\n",
    "\n",
    "final_output, attn = model(inp, tar)\n",
    "assert final_output.shape == (2, 8, 60)\n",
    "print(\"Transformer passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_angles & positional_encoding passed!\n",
      "Mask functions passed!\n",
      "Scaled Dot-Product Attention passed!\n",
      "FullyConnected passed!\n",
      "EncoderLayer passed!\n",
      "Encoder passed!\n",
      "DecoderLayer passed!\n",
      "Decoder passed!\n",
      "Transformer passed! Forward output shape: torch.Size([2, 5, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# ä½ç½®ç¼–ç \n",
    "# ----------------------------\n",
    "def get_angles(pos, i, d_model):\n",
    "    return pos / torch.pow(10000, (2 * (i//2)) / d_model)\n",
    "\n",
    "def positional_encoding(positions, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        torch.arange(positions).unsqueeze(1).float(),\n",
    "        torch.arange(d_model).unsqueeze(0).float(),\n",
    "        d_model\n",
    "    )\n",
    "    # sin on even indices; cos on odd indices\n",
    "    angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "    return angle_rads.unsqueeze(0)  # (1, positions, d_model)\n",
    "\n",
    "# æµ‹è¯•ä½ç½®ç¼–ç \n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "assert pos_encoding.shape == (1, 50, 512)\n",
    "print(\"get_angles & positional_encoding passed!\")\n",
    "\n",
    "# ----------------------------\n",
    "# Mask\n",
    "# ----------------------------\n",
    "def create_padding_mask(seq):\n",
    "    mask = (seq == 0).float()\n",
    "    return mask.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "# æµ‹è¯• Mask\n",
    "x = torch.tensor([[7, 6, 0, 0, 1],[1, 2, 3, 0, 0],[0, 0, 0, 4, 5]])\n",
    "pad_mask = create_padding_mask(x)\n",
    "look_mask = create_look_ahead_mask(5)\n",
    "assert pad_mask.shape == (3,1,5)\n",
    "assert look_mask.shape == (5,5)\n",
    "print(\"Mask functions passed!\")\n",
    "\n",
    "# ----------------------------\n",
    "# Scaled Dot-Product Attention\n",
    "# ----------------------------\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    dk = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(dk)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==1, float('-inf'))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, v)\n",
    "    return output, attn\n",
    "\n",
    "# æµ‹è¯• attention\n",
    "q = k = v = torch.rand(2,3,4)\n",
    "out, attn = scaled_dot_product_attention(q,k,v)\n",
    "assert out.shape == (2,3,4) and attn.shape == (2,3,3)\n",
    "print(\"Scaled Dot-Product Attention passed!\")\n",
    "\n",
    "# ----------------------------\n",
    "# å‰é¦ˆç½‘ç»œ\n",
    "# ----------------------------\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dff)\n",
    "        self.linear2 = nn.Linear(dff, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "# æµ‹è¯•å‰é¦ˆç½‘ç»œ\n",
    "fc = FullyConnected(8, 16)\n",
    "x = torch.rand(2,5,8)\n",
    "y = fc(x)\n",
    "assert y.shape == x.shape\n",
    "print(\"FullyConnected passed!\")\n",
    "\n",
    "# ----------------------------\n",
    "# Encoder Layer\n",
    "# ----------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        self.ffn = FullyConnected(d_model, dff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out,_ = self.mha(x,x,x, key_padding_mask=mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.dropout(self.ffn(x))\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# æµ‹è¯• EncoderLayer\n",
    "enc_layer = EncoderLayer(8,2,16)\n",
    "x = torch.rand(2,5,8)\n",
    "mask = torch.zeros(2,5)\n",
    "out = enc_layer(x, mask)\n",
    "assert out.shape == x.shape\n",
    "print(\"EncoderLayer passed!\")\n",
    "\n",
    "# ----------------------------\n",
    "# Encoder\n",
    "# ----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, max_pos_enc):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_pos_enc, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model,num_heads,dff) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:,:x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "# æµ‹è¯• Encoder\n",
    "encoder = Encoder(2,8,2,16,50,100)\n",
    "x_inp = torch.randint(0,50,(2,10))\n",
    "out = encoder(x_inp)\n",
    "assert out.shape == (2,10,8)\n",
    "print(\"Encoder passed!\")\n",
    "\n",
    "# ----------------------------\n",
    "# Decoder Layer\n",
    "# ----------------------------\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff):\n",
    "        super().__init__()\n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        self.ffn = FullyConnected(d_model,dff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        attn1,_ = self.mha1(x,x,x,key_padding_mask=look_ahead_mask)\n",
    "        x = self.norm1(x + attn1)\n",
    "        attn2,_ = self.mha2(x, enc_output, enc_output, key_padding_mask=padding_mask)\n",
    "        x = self.norm2(x + attn2)\n",
    "        ffn_out = self.dropout(self.ffn(x))\n",
    "        x = self.norm3(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# æµ‹è¯• DecoderLayer\n",
    "dec_layer = DecoderLayer(8,2,16)\n",
    "enc_out = torch.rand(2,10,8)\n",
    "x_tar = torch.rand(2,5,8)\n",
    "out = dec_layer(x_tar, enc_out)\n",
    "assert out.shape == x_tar.shape\n",
    "print(\"DecoderLayer passed!\")\n",
    "\n",
    "# ----------------------------\n",
    "# Decoder\n",
    "# ----------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_pos_enc):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_pos_enc, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model,num_heads,dff) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x, enc_output):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = x + self.pos_encoding[:,:x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output)\n",
    "        return x\n",
    "\n",
    "# æµ‹è¯• Decoder\n",
    "decoder = Decoder(2,8,2,16,50,100)\n",
    "x_tar = torch.randint(0,50,(2,5))\n",
    "out = decoder(x_tar, enc_out)\n",
    "assert out.shape == (2,5,8)\n",
    "print(\"Decoder passed!\")\n",
    "\n",
    "# ----------------------------\n",
    "# Transformer\n",
    "# ----------------------------\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, max_pos_enc_input, max_pos_enc_target):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers,d_model,num_heads,dff,input_vocab_size,max_pos_enc_input)\n",
    "        self.decoder = Decoder(num_layers,d_model,num_heads,dff,target_vocab_size,max_pos_enc_target)\n",
    "        self.final_layer = nn.Linear(d_model,target_vocab_size)\n",
    "    def forward(self, inp, tar):\n",
    "        enc_out = self.encoder(inp)\n",
    "        dec_out = self.decoder(tar, enc_out)\n",
    "        return F.softmax(self.final_layer(dec_out), dim=-1)\n",
    "\n",
    "# æµ‹è¯• Transformer\n",
    "model = Transformer(2,8,2,16,50,50,100,100)\n",
    "x_inp = torch.randint(0,50,(2,10))\n",
    "x_tar = torch.randint(0,50,(2,5))\n",
    "out = model(x_inp,x_tar)\n",
    "assert out.shape == (2,5,50)\n",
    "print(\"Transformer passed! Forward output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç›®å½•\n",
    "\n",
    "- [Packages](#0) è½¯ä»¶åŒ…\n",
    "- [1 - ä½ç½®ç¼–ç  (Positional Encoding)](#1)\n",
    "    - [1.1 - æ­£å¼¦å’Œä½™å¼¦è§’åº¦ (Sine and Cosine Angles)](#1-1)\n",
    "        - [ç»ƒä¹  1 - get_angles](#ex-1)\n",
    "    - [1.2 - æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç  (Sine and Cosine Positional Encodings)](#1-2)\n",
    "        - [ç»ƒä¹  2 - positional_encoding](#ex-2)\n",
    "- [2 - Masking æ©ç ](#2)\n",
    "    - [2.1 - å¡«å……æ©ç  (Padding Mask)](#2-1)\n",
    "    - [2.2 - å‰ç»æ©ç  (Look-ahead Mask)](#2-2)\n",
    "- [3 - è‡ªæ³¨æ„åŠ› (Self-Attention)](#3)\n",
    "    - [ç»ƒä¹  3 - scaled_dot_product_attention](#ex-3)\n",
    "- [4 - ç¼–ç å™¨ (Encoder)](#4)\n",
    "    - [4.1 ç¼–ç å™¨å±‚ (Encoder Layer)](#4-1)\n",
    "        - [ç»ƒä¹  4 - EncoderLayer](#ex-4)\n",
    "    - [4.2 å®Œæ•´ç¼–ç å™¨ (Full Encoder)](#4-2)\n",
    "        - [ç»ƒä¹  5 - Encoder](#ex-5)\n",
    "- [5 - è§£ç å™¨ (Decoder)](#5)\n",
    "    - [5.1 è§£ç å™¨å±‚ (Decoder Layer)](#5-1)\n",
    "        - [ç»ƒä¹  6 - DecoderLayer](#ex-6)\n",
    "    - [5.2 å®Œæ•´è§£ç å™¨ (Full Decoder)](#5-2)\n",
    "        - [ç»ƒä¹  7 - Decoder](#ex-7)\n",
    "- [6 - Transformer](#6)\n",
    "    - [ç»ƒä¹  8 - Transformer](#ex-8)\n",
    "- [7 - å‚è€ƒæ–‡çŒ® (References)](#7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## è½¯ä»¶åŒ… (Packages)\n",
    "\n",
    "è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒä»¥åŠ è½½æ‰€éœ€çš„è½¯ä»¶åŒ…ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_OpwqWL2QH5G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - ä½ç½®ç¼–ç  (Positional Encoding)\n",
    "\n",
    "åœ¨åºåˆ—åˆ°åºåˆ—ï¼ˆsequence-to-sequenceï¼‰ä»»åŠ¡ä¸­ï¼Œæ•°æ®çš„ç›¸å¯¹é¡ºåºå¯¹å…¶å«ä¹‰éå¸¸é‡è¦ã€‚  \n",
    "å½“ä½ è®­ç»ƒé¡ºåºç¥ç»ç½‘ç»œï¼ˆå¦‚ RNNï¼‰æ—¶ï¼Œè¾“å…¥æ•°æ®æŒ‰é¡ºåºä¼ å…¥ç½‘ç»œï¼Œæ¨¡å‹è‡ªåŠ¨è·å¾—æ•°æ®é¡ºåºçš„ä¿¡æ¯ã€‚  \n",
    "\n",
    "ç„¶è€Œï¼Œå½“ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼ˆmulti-head attentionï¼‰è®­ç»ƒ Transformer ç½‘ç»œæ—¶ï¼Œä½ ä¼šä¸€æ¬¡æ€§å°†æ‰€æœ‰æ•°æ®è¾“å…¥æ¨¡å‹ã€‚  \n",
    "è™½ç„¶è¿™æ ·å¤§å¤§å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼Œä½†æ¨¡å‹æ— æ³•ç›´æ¥è·å–æ•°æ®é¡ºåºä¿¡æ¯ã€‚  \n",
    "\n",
    "è¿™æ—¶ï¼Œ**ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰**å°±éå¸¸æœ‰ç”¨â€”â€”ä½ å¯ä»¥ä¸“é—¨å¯¹è¾“å…¥ä½ç½®è¿›è¡Œç¼–ç ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹æ­£å¼¦å’Œä½™å¼¦å…¬å¼å°†å…¶ä¼ å…¥ç½‘ç»œï¼š\n",
    "    \n",
    "$$\n",
    "PE_{(pos, 2i)}= \\sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{1}$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= \\cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{2}$$\n",
    "\n",
    "* $d$ ä¸ºè¯å‘é‡ï¼ˆword embeddingï¼‰å’Œä½ç½®ç¼–ç çš„ç»´åº¦\n",
    "* $pos$ ä¸ºå•è¯çš„ä½ç½®\n",
    "* $i$ è¡¨ç¤ºä½ç½®ç¼–ç çš„å„ä¸ªç»´åº¦\n",
    "\n",
    "ä¸ºäº†ç†è§£ä½ç½®ç¼–ç ï¼Œä½ å¯ä»¥å°†å…¶çœ‹ä½œåŒ…å«å•è¯ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„ç‰¹å¾ã€‚  \n",
    "æœ€ç»ˆï¼Œ**è¯å‘é‡ä¸ä½ç½®ç¼–ç çš„å’Œ**ä¼šè¢«è¾“å…¥æ¨¡å‹ã€‚  \n",
    "\n",
    "å¦‚æœä½ åªæ˜¯ç¡¬ç¼–ç ä½ç½®ï¼Œæ¯”å¦‚å°†ä¸€ä¸ªå…¨ 1 æˆ–æ•´æ•°çŸ©é˜µç›´æ¥åŠ åˆ°è¯å‘é‡ä¸Šï¼Œä¼šæ‰­æ›²è¯­ä¹‰ä¿¡æ¯ã€‚  \n",
    "è€Œæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å€¼åœ¨ [-1, 1] ä¹‹é—´ï¼ŒåŠ å…¥åˆ°è¯å‘é‡åï¼Œä¸ä¼šæ˜¾è‘—æ‰­æ›²è¯å‘é‡ï¼Œè€Œæ˜¯ä¸ºå…¶å¢åŠ äº†ä½ç½®ä¿¡æ¯ã€‚  \n",
    "ä½¿ç”¨è¿™ä¸¤ä¸ªå…¬å¼çš„ç»„åˆï¼Œå¯ä»¥å¸®åŠ© Transformer ç½‘ç»œå…³æ³¨è¾“å…¥æ•°æ®çš„ç›¸å¯¹ä½ç½®ã€‚  \n",
    "\n",
    "> è¿™åªæ˜¯å¯¹ä½ç½®ç¼–ç çš„ç®€è¦ä»‹ç»ï¼Œè‹¥æƒ³è¿›ä¸€æ­¥ç†è§£ï¼Œå¯ä»¥å‚è€ƒ *Positional Encoding Ungraded Lab*ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼š** åœ¨è¯¾ç¨‹è®²è§£ä¸­ Andrew ä½¿ç”¨çš„æ˜¯ç«–å‘å‘é‡ï¼Œä½†æœ¬ä½œä¸šä¸­æ‰€æœ‰å‘é‡å‡ä¸ºæ¨ªå‘ã€‚æ‰€æœ‰çŸ©é˜µä¹˜æ³•è¯·ç›¸åº”è°ƒæ•´ã€‚\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - æ­£å¼¦ä¸ä½™å¼¦è§’åº¦ (Sine and Cosine Angles)\n",
    "\n",
    "æ³¨æ„ï¼Œå°½ç®¡æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç å…¬å¼è¾“å…¥çš„å‚æ•°ä¸åŒï¼ˆ`2i` ä¸ `2i+1`ï¼Œå¶æ•°ä¸å¥‡æ•°ï¼‰ï¼Œä½†å…¬å¼çš„å†…éƒ¨é¡¹ç›¸åŒï¼š\n",
    "$$\\theta(pos, i, d) = \\frac{pos}{10000^{\\frac{2i}{d}}} \\tag{3}$$\n",
    "\n",
    "åœ¨è®¡ç®—åºåˆ—ä¸­æŸä¸ªå•è¯çš„ä½ç½®ç¼–ç æ—¶ï¼Œå¯ä»¥å‚è€ƒå†…éƒ¨é¡¹ï¼š  \n",
    "$PE_{(pos, 0)}= \\sin\\left(\\frac{pos}{{10000}^{0/d}}\\right)$ï¼Œå› ä¸º `2i = 0` å¾— `i = 0`  \n",
    "$PE_{(pos, 1)}= \\cos\\left(\\frac{pos}{{10000}^{0/d}}\\right)$ï¼Œå› ä¸º `2i + 1 = 1` å¾— `i = 0`  \n",
    "\n",
    "ä¸¤è€…çš„è§’åº¦ç›¸åŒï¼  \n",
    "$PE_{(pos, 2)}$ å’Œ $PE_{(pos, 3)}$ çš„è§’åº¦ä¹Ÿç›¸åŒï¼Œå› ä¸º `i = 1`ï¼Œæ‰€ä»¥å†…éƒ¨é¡¹ä¸º $\\frac{pos}{{10000}^{1/d}}$ã€‚  \n",
    "è¿™ç§å…³ç³»å¯¹æ‰€æœ‰æˆå¯¹çš„æ­£å¼¦å’Œä½™å¼¦æ›²çº¿éƒ½æˆç«‹ï¼š\n",
    "\n",
    "|      k         | <code>0</code> | <code>1</code> | <code>2</code> | <code>3</code> | <code>...</code> | <code>d-2</code> | <code>d-1</code> |\n",
    "|----------------|:---------------:|:---------------:|:---------------:|:---------------:|:----------------:|:----------------:|:----------------:|\n",
    "| encoding(0) =  | [$\\sin(\\theta(0,0,d))$ | $\\cos(\\theta(0,0,d))$ | $\\sin(\\theta(0,1,d))$ | $\\cos(\\theta(0,1,d))$ | ... | $\\sin(\\theta(0,d//2,d))$ | $\\cos(\\theta(0,d//2,d))$] |\n",
    "| encoding(1) =  | [$\\sin(\\theta(1,0,d))$ | $\\cos(\\theta(1,0,d))$ | $\\sin(\\theta(1,1,d))$ | $\\cos(\\theta(1,1,d))$ | ... | $\\sin(\\theta(1,d//2,d))$ | $\\cos(\\theta(1,d//2,d))$] |\n",
    "| ...            | ...             | ...             | ...             | ...             | ...              | ...              | ...              |\n",
    "| encoding(pos) =| [$\\sin(\\theta(pos,0,d))$ | $\\cos(\\theta(pos,0,d))$ | $\\sin(\\theta(pos,1,d))$ | $\\cos(\\theta(pos,1,d))$ | ... | $\\sin(\\theta(pos,d//2,d))$ | $\\cos(\\theta(pos,d//2,d))$] |\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### ç»ƒä¹  1 - get_angles\n",
    "\n",
    "å®ç° `get_angles()` å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç å¯èƒ½çš„è§’åº¦ã€‚\n",
    "\n",
    "**æç¤ºï¼š**\n",
    "\n",
    "- å¦‚æœ `k = [0, 1, 2, 3, 4, 5]`ï¼Œé‚£ä¹ˆ `i` åº”ä¸º `i = [0, 0, 1, 1, 2, 2]`\n",
    "- `i = k//2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bPzwMVfcQpT-"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# ä½ç½®ç¼–ç \n",
    "# ----------------------------\n",
    "def get_angles(pos, i, d_model):\n",
    "    return pos / torch.pow(10000, (2 * (i//2)) / d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angles shape: torch.Size([4, 8])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e-01, 1.0000e-01, 1.0000e-02, 1.0000e-02,\n",
      "         1.0000e-03, 1.0000e-03],\n",
      "        [2.0000e+00, 2.0000e+00, 2.0000e-01, 2.0000e-01, 2.0000e-02, 2.0000e-02,\n",
      "         2.0000e-03, 2.0000e-03],\n",
      "        [3.0000e+00, 3.0000e+00, 3.0000e-01, 3.0000e-01, 3.0000e-02, 3.0000e-02,\n",
      "         3.0000e-03, 3.0000e-03]])\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Example\n",
    "# ----------------------------\n",
    "position = 4\n",
    "d_model = 8\n",
    "pos_m = torch.arange(position).unsqueeze(1)  # shape (4,1)\n",
    "dims = torch.arange(d_model).unsqueeze(0)    # shape (1,8)\n",
    "angles = get_angles(pos_m, dims, d_model)\n",
    "print(\"Angles shape:\", angles.shape)\n",
    "print(angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - æ­£å¼¦ä¸ä½™å¼¦ä½ç½®ç¼–ç  (Sine and Cosine Positional Encodings)\n",
    "\n",
    "ç°åœ¨ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¹‹å‰è®¡ç®—çš„è§’åº¦æ¥è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç ã€‚\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= \\sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= \\cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### ç»ƒä¹  2 - positional_encoding\n",
    "\n",
    "å®ç°å‡½æ•° `positional_encoding()` æ¥è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç ã€‚\n",
    "\n",
    "**æé†’ï¼š** å½“ $i$ ä¸ºå¶æ•°æ—¶ä½¿ç”¨æ­£å¼¦å…¬å¼ï¼Œå½“ $i$ ä¸ºå¥‡æ•°æ—¶ä½¿ç”¨ä½™å¼¦å…¬å¼ã€‚\n",
    "\n",
    "#### é¢å¤–æç¤º\n",
    "* ä½ å¯èƒ½ä¼šæ ¹æ®å®ç°æ–¹å¼ä½¿ç”¨ [np.newaxis](https://numpy.org/doc/stable/reference/arrays.indexing.html)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y78txxoHQtwG"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        torch.arange(positions).unsqueeze(1).float(),\n",
    "        torch.arange(d_model).unsqueeze(0).float(),\n",
    "        d_model\n",
    "    )\n",
    "    # sin on even indices; cos on odd indices\n",
    "    angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "    return angle_rads.unsqueeze(0)  # (1, positions, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¾ˆå¥½ï¼Œä½ å·²ç»æˆåŠŸè®¡ç®—å‡ºä½ç½®ç¼–ç ï¼ç°åœ¨ï¼Œä½ å¯ä»¥å°†å®ƒä»¬å¯è§†åŒ–ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 512])\n",
      "get_angles & positional_encoding passed!\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ä½ç½®ç¼–ç \n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "assert pos_encoding.shape == (1, 50, 512)\n",
    "print (pos_encoding.shape)\n",
    "print(\"get_angles & positional_encoding passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªä½ç½®ç¼–ç â€”â€”æ³¨æ„æ¯ä¸€è¡Œéƒ½ä¸ç›¸åŒï¼ä½ ä¸ºæ¯ä¸ªå•è¯åˆ›å»ºäº†ç‹¬ç‰¹çš„ä½ç½®ç¼–ç ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - æ©ç  (Masking)\n",
    "\n",
    "åœ¨æ„å»º Transformer ç½‘ç»œæ—¶ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„æ©ç éå¸¸æœ‰ç”¨ï¼š*padding mask*ï¼ˆå¡«å……æ©ç ï¼‰å’Œ *look-ahead mask*ï¼ˆå‰ç»æ©ç ï¼‰ã€‚å®ƒä»¬éƒ½å¯ä»¥å¸®åŠ© softmax è®¡ç®—ä¸ºè¾“å…¥å¥å­ä¸­çš„è¯åˆ†é…åˆé€‚çš„æƒé‡ã€‚\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - å¡«å……æ©ç  (Padding Mask)\n",
    "\n",
    "é€šå¸¸æƒ…å†µä¸‹ï¼Œä½ çš„è¾“å…¥åºåˆ—å¯èƒ½è¶…è¿‡ç½‘ç»œèƒ½å¤Ÿå¤„ç†çš„æœ€å¤§é•¿åº¦ã€‚å‡è®¾æ¨¡å‹çš„æœ€å¤§é•¿åº¦ä¸º 5ï¼Œå®ƒè¾“å…¥å¦‚ä¸‹åºåˆ—ï¼š\n",
    "\n",
    "\n",
    "\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "å‘é‡åŒ–åå¯èƒ½å˜æˆï¼š\n",
    "\n",
    "\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]\n",
    "    ]\n",
    "    \n",
    "\n",
    "å½“å°†åºåˆ—ä¼ å…¥ Transformer æ¨¡å‹æ—¶ï¼Œåºåˆ—é•¿åº¦å¿…é¡»ç»Ÿä¸€ã€‚å¯ä»¥é€šè¿‡åœ¨åºåˆ—æœ«å°¾å¡«å……é›¶æˆ–æˆªæ–­è¶…è¿‡æœ€å¤§é•¿åº¦çš„å¥å­æ¥å®ç°ï¼š\n",
    "\n",
    "\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0],\n",
    "    ]\n",
    "    \n",
    "\n",
    "é•¿åº¦è¶…è¿‡æœ€å¤§å€¼çš„åºåˆ—ä¼šè¢«æˆªæ–­ï¼Œè€Œæˆªæ–­åçš„åºåˆ—æœ«å°¾ä¼šåŠ ä¸Šé›¶ä»¥ä¿æŒç»Ÿä¸€é•¿åº¦ã€‚å¯¹äºé•¿åº¦ä¸è¶³çš„åºåˆ—ï¼Œä¹Ÿä¼šåœ¨æœ«å°¾å¡«å……é›¶ã€‚ç„¶è€Œï¼Œè¿™äº›é›¶ä¼šå½±å“ softmax çš„è®¡ç®—â€”â€”è¿™æ—¶å¡«å……æ©ç å°±æ´¾ä¸Šç”¨åœºäº†ï¼ä½ éœ€è¦å®šä¹‰ä¸€ä¸ªå¸ƒå°”æ©ç ï¼ŒæŒ‡å®šå“ªäº›å…ƒç´ éœ€è¦å…³æ³¨(1)ï¼Œå“ªäº›å…ƒç´ éœ€è¦å¿½ç•¥(0)ã€‚ä¹‹åï¼Œä½ å¯ä»¥ä½¿ç”¨è¯¥æ©ç å°†åºåˆ—ä¸­çš„é›¶è®¾ç½®ä¸ºæ¥è¿‘è´Ÿæ— ç©·å¤§ï¼ˆ-1e9ï¼‰çš„å€¼ã€‚æˆ‘ä»¬ä¼šä¸ºä½ å®ç°è¿™éƒ¨åˆ†é€»è¾‘ï¼Œè®©ä½ å¯ä»¥å°½å¿«è¿›å…¥æ„å»º Transformer ç½‘ç»œçš„ä¹è¶£ ğŸ˜‡ã€‚åªéœ€ç¡®ä¿ç†è§£ä»£ç ï¼Œè¿™æ ·åœ¨æ„å»ºæ¨¡å‹æ—¶æ‰èƒ½æ­£ç¡®å®ç°å¡«å……ã€‚\n",
    "\n",
    "æ©ç å¤„ç†åï¼Œè¾“å…¥ `[87, 600, 0, 0, 0]` ä¼šå˜æˆ `[87, 600, -1e9, -1e9, -1e9]`ï¼Œè¿™æ ·åœ¨è®¡ç®— softmax æ—¶ï¼Œé›¶å°±ä¸ä¼šå½±å“å¾—åˆ†ã€‚\n",
    "\n",
    "Keras ä¸­å®ç°çš„ [MultiheadAttention](https://keras.io/api/layers/attention_layers/multi_head_attention/) å±‚ä½¿ç”¨äº†è¿™ç§æ©ç é€»è¾‘ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JOL9XWsFQxxo"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Mask\n",
    "# ----------------------------\n",
    "def create_padding_mask(seq):\n",
    "    mask = (seq == 0).float()\n",
    "    return mask.unsqueeze(1)  # (batch_size, 1, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5J5FFjklQ1Fz",
    "outputId": "8319446f-3ed4-406a-cf38-ca2b08142ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 1., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 0., 0.]]])\n",
      "Padding_Mask functions passed!\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[7, 6, 0, 0, 1],[1, 2, 3, 0, 0],[0, 0, 0, 4, 5]])\n",
    "pad_mask = create_padding_mask(x)\n",
    "assert pad_mask.shape == (3,1,5)\n",
    "print(create_padding_mask(x))\n",
    "print(\"Padding_Mask functions passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœæˆ‘ä»¬å°† `(1 - mask)` ä¹˜ä»¥ `-1e9` å¹¶åŠ åˆ°ç¤ºä¾‹è¾“å…¥åºåˆ—ä¸­ï¼Œåºåˆ—ä¸­çš„é›¶å°±ä¼šè¢«è®¾ç½®ä¸ºæ¥è¿‘è´Ÿæ— ç©·å¤§ã€‚æ³¨æ„åŸå§‹åºåˆ—ä¸æ©ç åºåˆ—åœ¨è®¡ç®— softmax æ—¶çš„åŒºåˆ«ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax without mask:\n",
      "tensor([[7.2877e-01, 2.6810e-01, 6.6455e-04, 6.6455e-04, 1.8064e-03],\n",
      "        [8.4437e-02, 2.2952e-01, 6.2391e-01, 3.1063e-02, 3.1063e-02],\n",
      "        [4.8541e-03, 4.8541e-03, 4.8541e-03, 2.6503e-01, 7.2041e-01]])\n",
      "Softmax with padding mask:\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 5.0000e-01, 5.0000e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 9.5257e-01, 4.7426e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7986e-02, 9.8201e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6894e-01, 7.3106e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0000e-01, 5.0000e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6894e-01, 7.3106e-01]],\n",
      "\n",
      "        [[7.3057e-01, 2.6876e-01, 6.6620e-04, 0.0000e+00, 0.0000e+00],\n",
      "         [9.0031e-02, 2.4473e-01, 6.6524e-01, 0.0000e+00, 0.0000e+00],\n",
      "         [3.3333e-01, 3.3333e-01, 3.3333e-01, 0.0000e+00, 0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Softmax æµ‹è¯•\n",
    "# ----------------------------\n",
    "x_float = x.float()  # å…ˆè½¬æ¢ä¸ºæµ®ç‚¹æ•°\n",
    "\n",
    "# æ™®é€š softmax\n",
    "softmax_x = F.softmax(x_float, dim=-1)\n",
    "print(\"Softmax without mask:\")\n",
    "print(softmax_x)\n",
    "\n",
    "# å¸¦ padding mask çš„ softmax\n",
    "# mask ä¸º 1 çš„ä½ç½®æ˜¯è¢« maskï¼Œéœ€è¦ç”¨å¤§è´Ÿæ•°æƒ©ç½š\n",
    "masked_x = x_float + (pad_mask - 1) * 1e9  # pad_mask==1 -> -1e9ï¼Œ pad_mask==0 -> 0\n",
    "softmax_masked_x = F.softmax(masked_x, dim=-1)\n",
    "print(\"Softmax with padding mask:\")\n",
    "print(softmax_masked_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - å‰ç»æ©ç  (Look-ahead Mask)\n",
    "\n",
    "å‰ç»æ©ç çš„åŸç†ç±»ä¼¼ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œä½ å¯ä»¥è®¿é—®è®­ç»ƒæ ·æœ¬çš„å®Œæ•´æ­£ç¡®è¾“å‡ºã€‚å‰ç»æ©ç å¸®åŠ©æ¨¡å‹æ¨¡æ‹Ÿâ€œå·²æ­£ç¡®é¢„æµ‹éƒ¨åˆ†è¾“å‡ºâ€ï¼Œç„¶åæ£€æŸ¥åœ¨**ä¸çœ‹æœªæ¥ä¿¡æ¯**çš„æƒ…å†µä¸‹ï¼Œæ˜¯å¦èƒ½æ­£ç¡®é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œå¦‚æœæœŸæœ›çš„æ­£ç¡®è¾“å‡ºæ˜¯ `[1, 2, 3]`ï¼Œä½ æƒ³è¦æ£€æŸ¥åœ¨æ¨¡å‹å·²æ­£ç¡®é¢„æµ‹ç¬¬ä¸€ä¸ªå€¼çš„æƒ…å†µä¸‹ï¼Œèƒ½å¦é¢„æµ‹ç¬¬äºŒä¸ªå€¼ï¼Œé‚£ä¹ˆå°±ä¼šå°†ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªå€¼å±è”½æ‰ã€‚è¾“å…¥çš„æ©ç åºåˆ—ä¸º `[1, -1e9, -1e9]`ï¼Œç„¶åè§‚å¯Ÿæ¨¡å‹æ˜¯å¦èƒ½ç”Ÿæˆ `[1, 2, -1e9]`ã€‚\n",
    "\n",
    "å› ä¸ºä½ åŠªåŠ›äº†ï¼Œæˆ‘ä»¬ä¹Ÿä¼šä¸ºä½ å®ç°è¿™ä¸ªæ©ç  ğŸ˜‡ğŸ˜‡ã€‚å†æ¬¡æé†’ï¼Œè¯·ä»”ç»†æŸ¥çœ‹ä»£ç ï¼Œè¿™æ ·ä½ ä»¥åæ‰èƒ½æœ‰æ•ˆåœ°å®ç°å®ƒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9O9UbM31Q3hK"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look-Ahead Mask:\n",
      "tensor([[0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "Look-Ahead Mask function passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• create_look_ahead_mask\n",
    "# ----------------------------\n",
    "seq_len = 5\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "assert look_ahead_mask.shape == (seq_len, seq_len)\n",
    "print(\"Look-Ahead Mask:\")\n",
    "print(look_ahead_mask)\n",
    "print(\"Look-Ahead Mask function passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG0gPyv0oDBi"
   },
   "source": [
    "<a name='3'></a>\n",
    "## 3 - è‡ªæ³¨æ„åŠ› (Self-Attention)\n",
    "\n",
    "æ­£å¦‚ Transformer è®ºæ–‡çš„ä½œè€…æ‰€è¯´ï¼Œâ€œAttention is All You Needâ€ã€‚\n",
    "\n",
    "<img src=\"self-attention.png\" alt=\"Encoder\" width=\"600\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 1: è‡ªæ³¨æ„åŠ›è®¡ç®—å¯è§†åŒ–</b></font></center></caption>\n",
    "\n",
    "è‡ªæ³¨æ„åŠ›ä¸ä¼ ç»Ÿå·ç§¯ç½‘ç»œç»“åˆä½¿ç”¨å¯ä»¥å®ç°å¹¶è¡Œå¤„ç†ï¼Œä»è€ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä½ å°†å®ç° **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (scaled dot-product attention)**ï¼Œå®ƒæ¥å—æŸ¥è¯¢ (query)ã€é”® (key)ã€å€¼ (value) ä»¥åŠæ©ç  (mask) ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›åºåˆ—ä¸­æ¯ä¸ªå•è¯çš„ä¸°å¯Œçš„åŸºäºæ³¨æ„åŠ›çš„å‘é‡è¡¨ç¤ºã€‚å…¶æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_k}} + M \\right) V \\tag{4}\n",
    "$$\n",
    "\n",
    "* $Q$ æ˜¯æŸ¥è¯¢çŸ©é˜µ (queries matrix)\n",
    "* $K$ æ˜¯é”®çŸ©é˜µ (keys matrix)\n",
    "* $V$ æ˜¯å€¼çŸ©é˜µ (values matrix)\n",
    "* $M$ æ˜¯å¯é€‰çš„æ©ç \n",
    "* $d_k$ æ˜¯é”®çš„ç»´åº¦ï¼Œç”¨äºç¼©æ”¾ï¼Œä»¥é˜² softmax å€¼è¿‡å¤§\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### ç»ƒä¹  3 - scaled_dot_product_attention \n",
    "\n",
    "å®ç°å‡½æ•° `scaled_dot_product_attention()` æ¥åˆ›å»ºåŸºäºæ³¨æ„åŠ›çš„è¡¨ç¤ºã€‚\n",
    "\n",
    "**æç¤º**ï¼šå¸ƒå°”æ©ç å‚æ•°å¯ä»¥ä¼ å…¥ `None`ï¼Œæˆ–è€…æ˜¯ padding æ©ç æˆ–å‰ç»æ©ç ã€‚  \n",
    "\n",
    "åœ¨åº”ç”¨ softmax ä¹‹å‰ï¼Œå°† `(1. - mask)` ä¹˜ä»¥ `-1e9`ã€‚\n",
    "\n",
    "#### é¢å¤–æç¤º\n",
    "* ä½ å¯èƒ½ä¼šå‘ç° [tf.matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) å¯¹çŸ©é˜µä¹˜æ³•éå¸¸æœ‰ç”¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CSysk_rjQ7lp"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Scaled Dot-Product Attention\n",
    "# ----------------------------\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    dk = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(dk)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==1, float('-inf'))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, v)\n",
    "    return output, attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:\n",
      " tensor([[[0.8200, 0.5481, 0.7669, 0.6045],\n",
      "         [0.2528, 0.8482, 0.1522, 0.5363],\n",
      "         [0.2815, 0.8123, 0.2681, 0.8129]],\n",
      "\n",
      "        [[0.4784, 0.7652, 0.7037, 0.8443],\n",
      "         [0.4885, 0.1822, 0.9119, 0.6112],\n",
      "         [0.5726, 0.5345, 0.4922, 0.1238]]])\n",
      "k:\n",
      " tensor([[[0.8200, 0.5481, 0.7669, 0.6045],\n",
      "         [0.2528, 0.8482, 0.1522, 0.5363],\n",
      "         [0.2815, 0.8123, 0.2681, 0.8129]],\n",
      "\n",
      "        [[0.4784, 0.7652, 0.7037, 0.8443],\n",
      "         [0.4885, 0.1822, 0.9119, 0.6112],\n",
      "         [0.5726, 0.5345, 0.4922, 0.1238]]])\n",
      "v:\n",
      " tensor([[[0.8200, 0.5481, 0.7669, 0.6045],\n",
      "         [0.2528, 0.8482, 0.1522, 0.5363],\n",
      "         [0.2815, 0.8123, 0.2681, 0.8129]],\n",
      "\n",
      "        [[0.4784, 0.7652, 0.7037, 0.8443],\n",
      "         [0.4885, 0.1822, 0.9119, 0.6112],\n",
      "         [0.5726, 0.5345, 0.4922, 0.1238]]])\n",
      "Attention Scores (softmax):\n",
      " tensor([[[0.4125, 0.2747, 0.3128],\n",
      "         [0.3274, 0.3243, 0.3483],\n",
      "         [0.3350, 0.3130, 0.3520]],\n",
      "\n",
      "        [[0.4126, 0.3227, 0.2646],\n",
      "         [0.3699, 0.3601, 0.2700],\n",
      "         [0.3614, 0.3217, 0.3169]]])\n",
      "Output:\n",
      " tensor([[[0.4958, 0.7132, 0.4420, 0.6509],\n",
      "         [0.4485, 0.7375, 0.3938, 0.6549],\n",
      "         [0.4529, 0.7350, 0.3989, 0.6565]],\n",
      "\n",
      "        [[0.5066, 0.5160, 0.7149, 0.5784],\n",
      "         [0.5075, 0.4930, 0.7215, 0.5658],\n",
      "         [0.5115, 0.5045, 0.7037, 0.5410]]])\n",
      "Scaled Dot-Product Attention passed!\n",
      "\n",
      "Mask:\n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 0, 1],\n",
      "         [1, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1],\n",
      "         [1, 0, 0],\n",
      "         [0, 1, 0]]], dtype=torch.uint8)\n",
      "Attention Scores with mask (softmax):\n",
      " tensor([[[0.5687, 0.0000, 0.4313],\n",
      "         [0.5024, 0.4976, 0.0000],\n",
      "         [0.0000, 0.4707, 0.5293]],\n",
      "\n",
      "        [[0.5611, 0.4389, 0.0000],\n",
      "         [0.0000, 0.5714, 0.4286],\n",
      "         [0.5328, 0.0000, 0.4672]]])\n",
      "Output with mask:\n",
      " tensor([[[0.5878, 0.6620, 0.5518, 0.6943],\n",
      "         [0.5377, 0.6974, 0.4610, 0.5705],\n",
      "         [0.2680, 0.8292, 0.2135, 0.6827]],\n",
      "\n",
      "        [[0.4828, 0.5093, 0.7951, 0.7420],\n",
      "         [0.5246, 0.3332, 0.7320, 0.4023],\n",
      "         [0.5224, 0.6574, 0.6049, 0.5076]]])\n",
      "Scaled Dot-Product Attention with mask passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• Scaled Dot-Product Attention\n",
    "# ----------------------------\n",
    "q = k = v = torch.rand(2, 3, 4)  # batch_size=2, seq_len=3, depth=4\n",
    "out, attn = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(\"q:\\n\", q)\n",
    "print(\"k:\\n\", k)\n",
    "print(\"v:\\n\", v)\n",
    "print(\"Attention Scores (softmax):\\n\", attn)\n",
    "print(\"Output:\\n\", out)\n",
    "\n",
    "assert out.shape == (2, 3, 4) and attn.shape == (2, 3, 3)\n",
    "print(\"Scaled Dot-Product Attention passed!\")\n",
    "\n",
    "# æµ‹è¯•åŠ  mask\n",
    "mask = torch.tensor([[[0,1,0],[0,0,1],[1,0,0]], [[0,0,1],[1,0,0],[0,1,0]]], dtype=torch.uint8)\n",
    "out_masked, attn_masked = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "\n",
    "print(\"\\nMask:\\n\", mask)\n",
    "print(\"Attention Scores with mask (softmax):\\n\", attn_masked)\n",
    "print(\"Output with mask:\\n\", out_masked)\n",
    "\n",
    "assert out_masked.shape == (2, 3, 4) and attn_masked.shape == (2, 3, 3)\n",
    "print(\"Scaled Dot-Product Attention with mask passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¹²å¾—å¥½ï¼ä½ ç°åœ¨å·²ç»å¯ä»¥å®ç°è‡ªæ³¨æ„åŠ›äº†ã€‚æŒæ¡è‡ªæ³¨æ„åŠ›åï¼Œä½ å°±å¯ä»¥å¼€å§‹æ„å»ºç¼–ç å™¨ (encoder) æ¨¡å—äº†ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blS0pEpTqRVI"
   },
   "source": [
    "<a name='4'></a>\n",
    "## 4 - ç¼–ç å™¨ (Encoder)\n",
    "\n",
    "Transformer ç¼–ç å™¨å±‚å°†è‡ªæ³¨æ„åŠ› (self-attention) ä¸å·ç§¯ç¥ç»ç½‘ç»œé£æ ¼çš„å¤„ç†ç»“åˆï¼Œä»¥æå‡è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶å°† K å’Œ V çŸ©é˜µä¼ é€’ç»™è§£ç å™¨ (Decoder)ï¼Œè¯¥è§£ç å™¨å°†åœ¨ä½œä¸šåé¢æ„å»ºã€‚åœ¨æœ¬èŠ‚ä½œä¸šä¸­ï¼Œä½ å°†é€šè¿‡ç»“åˆå¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œæ¥å®ç°ç¼–ç å™¨ (å¦‚å›¾ 2a æ‰€ç¤º)ã€‚\n",
    "\n",
    "<img src=\"encoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 2aï¼šTransformer ç¼–ç å™¨å±‚</b></font></center></caption>\n",
    "\n",
    "* `MultiHeadAttention` å¯ä»¥ç†è§£ä¸ºå¤šæ¬¡è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼Œç”¨äºæ£€æµ‹ä¸åŒç‰¹å¾ã€‚\n",
    "* å‰é¦ˆç¥ç»ç½‘ç»œ (Feed Forward Neural Network) åŒ…å«ä¸¤ä¸ª Dense å±‚ï¼Œæˆ‘ä»¬å°†å…¶å®ç°ä¸º `FullyConnected` å‡½æ•°ã€‚\n",
    "\n",
    "è¾“å…¥å¥å­é¦–å…ˆé€šè¿‡*å¤šå¤´æ³¨æ„åŠ›å±‚*ï¼Œç¼–ç å™¨åœ¨ç¼–ç æŸä¸ªå•è¯æ—¶ä¼šæŸ¥çœ‹è¾“å…¥å¥å­ä¸­çš„å…¶ä»–å•è¯ã€‚å¤šå¤´æ³¨æ„åŠ›å±‚çš„è¾“å‡ºéšåä¼ å…¥*å‰é¦ˆç¥ç»ç½‘ç»œ*ã€‚å®Œå…¨ç›¸åŒçš„å‰é¦ˆç½‘ç»œä¼šç‹¬ç«‹åº”ç”¨äºæ¯ä¸ªä½ç½®ã€‚\n",
    "\n",
    "* å¯¹äº `MultiHeadAttention` å±‚ï¼Œä½ å°†ä½¿ç”¨ [Keras å®ç°](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)ã€‚å¦‚æœä½ æƒ³äº†è§£å¦‚ä½•å°†æŸ¥è¯¢çŸ©é˜µ Qã€é”®çŸ©é˜µ K å’Œå€¼çŸ©é˜µ V æ‹†åˆ†æˆä¸åŒçš„ headï¼Œå¯ä»¥æŸ¥çœ‹å®ç°ç»†èŠ‚ã€‚\n",
    "* ä½ è¿˜å°†ä½¿ç”¨ [Sequential API](https://keras.io/api/models/sequential/) æ­å»ºä¸¤ä¸ª Dense å±‚æ„æˆçš„å‰é¦ˆç¥ç»ç½‘ç»œã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sC5vJhz29vZR"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# å‰é¦ˆç½‘ç»œ\n",
    "# ----------------------------\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dff)\n",
    "        self.linear2 = nn.Linear(dff, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " tensor([[[0.3513, 0.2513, 0.2308, 0.1186, 0.6873, 0.3637, 0.5210, 0.4875],\n",
      "         [0.0917, 0.8506, 0.3968, 0.9119, 0.2783, 0.7048, 0.6517, 0.4378],\n",
      "         [0.8261, 0.2454, 0.1647, 0.0172, 0.2447, 0.1091, 0.6365, 0.1570],\n",
      "         [0.2795, 0.8758, 0.7508, 0.1421, 0.8066, 0.9451, 0.3369, 0.8978],\n",
      "         [0.1600, 0.8795, 0.0888, 0.0454, 0.8192, 0.8937, 0.1246, 0.6692]],\n",
      "\n",
      "        [[0.6951, 0.4703, 0.2725, 0.7294, 0.4237, 0.1618, 0.6358, 0.5917],\n",
      "         [0.9310, 0.5374, 0.6483, 0.4942, 0.1652, 0.0818, 0.0631, 0.5288],\n",
      "         [0.3806, 0.7728, 0.7665, 0.2217, 0.5649, 0.3021, 0.7523, 0.3003],\n",
      "         [0.5093, 0.6719, 0.2087, 0.4812, 0.4554, 0.5704, 0.2378, 0.1722],\n",
      "         [0.1584, 0.4287, 0.4379, 0.4206, 0.9504, 0.8533, 0.6630, 0.7453]]])\n",
      "After linear1 + ReLU:\n",
      " tensor([[[0.2839, 0.0000, 0.0560, 0.0479, 0.0000, 0.0000, 0.0244, 0.3960,\n",
      "          0.0000, 0.0000, 0.1330, 0.0000, 0.0000, 0.1323, 0.0000, 0.4261],\n",
      "         [0.3787, 0.1663, 0.4007, 0.6023, 0.0000, 0.0000, 0.0000, 0.4809,\n",
      "          0.0000, 0.0938, 0.0000, 0.0000, 0.0000, 0.1380, 0.0800, 0.4736],\n",
      "         [0.4860, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7018,\n",
      "          0.0455, 0.0000, 0.0000, 0.1569, 0.0000, 0.3490, 0.0000, 0.0375],\n",
      "         [0.3416, 0.0000, 0.3934, 0.4069, 0.0000, 0.0000, 0.0000, 0.0936,\n",
      "          0.0000, 0.0995, 0.2039, 0.0000, 0.0000, 0.1959, 0.0061, 1.0505],\n",
      "         [0.1565, 0.0336, 0.4209, 0.1479, 0.0000, 0.0000, 0.0000, 0.1747,\n",
      "          0.0000, 0.1888, 0.1523, 0.0000, 0.0000, 0.2839, 0.0769, 0.8780]],\n",
      "\n",
      "        [[0.4413, 0.2360, 0.1811, 0.1256, 0.0000, 0.0000, 0.0000, 0.5736,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2013, 0.0000, 0.2413],\n",
      "         [0.6039, 0.3258, 0.1092, 0.0000, 0.0000, 0.0000, 0.0000, 0.3697,\n",
      "          0.0000, 0.0000, 0.0000, 0.1520, 0.0000, 0.1156, 0.1420, 0.4347],\n",
      "         [0.4856, 0.0000, 0.0000, 0.3219, 0.0000, 0.0000, 0.0000, 0.3661,\n",
      "          0.0000, 0.0079, 0.0000, 0.0000, 0.0000, 0.2145, 0.0000, 0.5417],\n",
      "         [0.3587, 0.1081, 0.2565, 0.1091, 0.0000, 0.0000, 0.0000, 0.4901,\n",
      "          0.0378, 0.1747, 0.0000, 0.0000, 0.0000, 0.2125, 0.2222, 0.4110],\n",
      "         [0.2234, 0.0000, 0.3500, 0.4801, 0.0000, 0.0000, 0.0235, 0.3186,\n",
      "          0.0000, 0.0000, 0.3053, 0.0000, 0.0000, 0.0880, 0.0000, 0.7061]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Output y:\n",
      " tensor([[[ 0.0517,  0.1174,  0.0958,  0.0577,  0.0914,  0.1692, -0.1119,\n",
      "           0.1743],\n",
      "         [ 0.1450,  0.1778,  0.1756,  0.1304, -0.0385,  0.1855, -0.0658,\n",
      "           0.1891],\n",
      "         [ 0.0296,  0.1957,  0.1663,  0.0984,  0.0448,  0.0953, -0.1884,\n",
      "           0.0538],\n",
      "         [ 0.2326,  0.0776,  0.0129,  0.0226, -0.0692,  0.2477, -0.0281,\n",
      "           0.4037],\n",
      "         [ 0.1579,  0.0444,  0.0933,  0.0812,  0.0356,  0.1386, -0.0597,\n",
      "           0.2663]],\n",
      "\n",
      "        [[ 0.0663,  0.1610,  0.2378,  0.0709,  0.0756,  0.0883, -0.1193,\n",
      "           0.0445],\n",
      "         [ 0.2123,  0.1795,  0.2488,  0.0033,  0.0733,  0.1844, -0.0238,\n",
      "           0.2176],\n",
      "         [ 0.1590,  0.1436,  0.0704,  0.0040, -0.0656,  0.2239, -0.0968,\n",
      "           0.2705],\n",
      "         [ 0.1080,  0.1894,  0.2464,  0.1341,  0.0569,  0.1397, -0.0978,\n",
      "           0.1606],\n",
      "         [ 0.1077,  0.1038,  0.0024,  0.0947,  0.0188,  0.2457, -0.0529,\n",
      "           0.2862]]], grad_fn=<ViewBackward0>)\n",
      "FullyConnected passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯•å‰é¦ˆç½‘ç»œ\n",
    "# ----------------------------\n",
    "fc = FullyConnected(8, 16)\n",
    "x = torch.rand(2, 5, 8)  # batch_size=2, seq_len=5, d_model=8\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "y = fc(x)\n",
    "\n",
    "# æ‰“å°æ•°å­—è¾“å‡º\n",
    "print(\"Input x:\\n\", x)\n",
    "x1 = F.relu(fc.linear1(x))\n",
    "print(\"After linear1 + ReLU:\\n\", x1)\n",
    "print(\"Output y:\\n\", y)\n",
    "\n",
    "# éªŒè¯è¾“å‡ºå½¢çŠ¶\n",
    "assert y.shape == x.shape\n",
    "print(\"FullyConnected passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R65WbX5wqYYH"
   },
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 ç¼–ç å™¨å±‚ (Encoder Layer)\n",
    "\n",
    "ç°åœ¨ä½ å¯ä»¥åœ¨ä¸€ä¸ªç¼–ç å™¨å±‚ä¸­å°†å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œç»“åˆèµ·æ¥ï¼ä½ è¿˜å°†ä½¿ç”¨æ®‹å·®è¿æ¥ (residual connections) å’Œå±‚å½’ä¸€åŒ– (layer normalization) æ¥åŠ é€Ÿè®­ç»ƒ (å›¾ 2a)ã€‚\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### ç»ƒä¹  4 - EncoderLayer\n",
    "\n",
    "ä½¿ç”¨ `call()` æ–¹æ³•å®ç° `EncoderLayer()`ã€‚\n",
    "\n",
    "åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†å®ç°ä¸€ä¸ªç¼–ç å™¨å—ï¼ˆå›¾ 2ï¼‰ï¼Œä½¿ç”¨ `call()` æ–¹æ³•ã€‚è¯¥å‡½æ•°åº”æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. å°† Qã€Vã€K çŸ©é˜µä»¥åŠå¸ƒå°”æ©ç  (boolean mask) è¾“å…¥å¤šå¤´æ³¨æ„åŠ›å±‚ã€‚è¯·è®°ä½ï¼Œä¸ºäº†è®¡ç®—*è‡ªæ³¨æ„åŠ› (self-attention)*ï¼ŒQã€V å’Œ K åº”ç›¸åŒã€‚è®© `return_attention_scores` å’Œ `training` ä½¿ç”¨é»˜è®¤å€¼ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿˜éœ€åœ¨å¤šå¤´æ³¨æ„åŠ›å±‚ä¸­æ‰§è¡Œ Dropoutã€‚\n",
    "2. æ·»åŠ è·³è·ƒè¿æ¥ (skip connection)ï¼Œå°†åŸå§‹è¾“å…¥ `x` ä¸å¤šå¤´æ³¨æ„åŠ›å±‚è¾“å‡ºç›¸åŠ ã€‚\n",
    "3. æ·»åŠ è·³è·ƒè¿æ¥åï¼Œå°†è¾“å‡ºä¼ å…¥ç¬¬ä¸€ä¸ªå½’ä¸€åŒ–å±‚ (Layer Normalization)ã€‚\n",
    "4. æœ€åé‡å¤æ­¥éª¤ 1-3ï¼Œä½†ä½¿ç”¨å‰é¦ˆç¥ç»ç½‘ç»œå’Œ Dropout å±‚ä»£æ›¿å¤šå¤´æ³¨æ„åŠ›å±‚ã€‚\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"darkgreen\"><b>é¢å¤–æç¤º (ç‚¹å‡»å±•å¼€)</b></font></summary>\n",
    "    \n",
    "* `__init__` æ–¹æ³•åˆ›å»ºæ‰€æœ‰å°†åœ¨ `call` æ–¹æ³•ä¸­è®¿é—®çš„å±‚ã€‚æ— è®ºä½•æ—¶åœ¨ `call` æ–¹æ³•ä¸­ä½¿ç”¨ `__init__` ä¸­å®šä¹‰çš„å±‚ï¼Œéƒ½å¿…é¡»ä½¿ç”¨è¯­æ³• `self.[å±‚å]`ã€‚\n",
    "* å¯ä»¥æŸ¥é˜… [MultiHeadAttention æ–‡æ¡£](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) è·å–å¸®åŠ©ã€‚*æ³¨æ„ï¼Œå¦‚æœ queryã€key å’Œ value ç›¸åŒï¼Œåˆ™è¯¥å‡½æ•°æ‰§è¡Œè‡ªæ³¨æ„åŠ› (self-attention)ã€‚*\n",
    "* `self.mha` çš„ call å‚æ•°å¦‚ä¸‹ï¼ˆB ä¸º batch_sizeï¼ŒT ä¸ºç›®æ ‡åºåˆ—é•¿åº¦ï¼ŒS ä¸ºè¾“å‡ºé•¿åº¦ï¼‰ï¼š\n",
    "  - `query`ï¼šå½¢çŠ¶ä¸º (B, T, dim) çš„æŸ¥è¯¢å¼ é‡\n",
    "  - `value`ï¼šå½¢çŠ¶ä¸º (B, S, dim) çš„å€¼å¼ é‡\n",
    "  - `key`ï¼šå¯é€‰ï¼Œå½¢çŠ¶ä¸º (B, S, dim) çš„é”®å¼ é‡ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨ value ä½œä¸º key å’Œ valueï¼ˆæœ€å¸¸ç”¨æƒ…å†µï¼‰ã€‚\n",
    "  - `attention_mask`ï¼šå½¢çŠ¶ä¸º (B, T, S) çš„å¸ƒå°”æ©ç ï¼Œé˜²æ­¢æ³¨æ„åŠ›å…³æ³¨æŸäº›ä½ç½®ã€‚å¸ƒå°”æ©ç æŒ‡å®šæŸ¥è¯¢å…ƒç´ å¯ä»¥å…³æ³¨å“ªäº›é”®å…ƒç´ ï¼Œ1 è¡¨ç¤ºå…³æ³¨ï¼Œ0 è¡¨ç¤ºä¸å…³æ³¨ã€‚ç¼ºå¤±çš„ batch å’Œ head ç»´åº¦å¯è‡ªåŠ¨å¹¿æ’­ã€‚\n",
    "  - `return_attention_scores`ï¼šå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ˜¯å¦è¿”å›æ³¨æ„åŠ›è¾“å‡ºã€‚å¦‚æœä¸º Trueï¼Œåˆ™åªè¿”å›æ³¨æ„åŠ›è¾“å‡ºï¼›å¦åˆ™è¿”å› (attention_output, attention_scores)ã€‚é»˜è®¤å€¼ä¸º Falseã€‚\n",
    "  - `training`ï¼šå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºå±‚æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼ï¼ˆæ·»åŠ  dropoutï¼‰æˆ–æ¨ç†æ¨¡å¼ï¼ˆä¸æ·»åŠ  dropoutï¼‰ã€‚é»˜è®¤å€¼ä¸ºçˆ¶æ¨¡å‹/å±‚çš„è®­ç»ƒæ¨¡å¼ï¼Œå¦‚æœæ— çˆ¶å±‚ï¼Œåˆ™é»˜è®¤ä¸º Falseï¼ˆæ¨ç†æ¨¡å¼ï¼‰ã€‚\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tIufbrc-9_2u"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Encoder Layer\n",
    "# ----------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        self.ffn = FullyConnected(d_model, dff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out,_ = self.mha(x,x,x, key_padding_mask=mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ffn_out = self.dropout(self.ffn(x))\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of EncoderLayer:\n",
      " tensor([[[ 0.0314,  0.5726,  0.5395,  0.2765, -0.3497, -1.3844, -1.4577,\n",
      "           1.7719],\n",
      "         [-0.2994,  0.6061,  1.0795,  1.2569, -0.8556, -1.8855, -0.4538,\n",
      "           0.5518],\n",
      "         [ 0.2667, -1.8838, -0.0271,  1.4896,  0.2043, -0.0229, -1.0424,\n",
      "           1.0156],\n",
      "         [ 1.3583, -1.1074, -0.4414,  1.0912, -1.4507, -0.6820,  0.2888,\n",
      "           0.9433],\n",
      "         [-0.7208,  0.4611,  0.8476, -0.9587,  0.0526,  1.6104, -1.6948,\n",
      "           0.4026]],\n",
      "\n",
      "        [[ 1.4858,  1.4418, -0.4724, -0.4241, -1.5317,  0.5932, -0.4577,\n",
      "          -0.6348],\n",
      "         [ 1.7666, -1.2154, -1.0147, -0.0202, -1.1960,  0.5424,  0.5811,\n",
      "           0.5563],\n",
      "         [ 1.1912, -0.2158,  0.6246,  0.7333, -2.0517,  0.0110, -0.9691,\n",
      "           0.6766],\n",
      "         [-0.5451, -0.1484,  0.1374,  0.4691, -1.2809, -0.7139, -0.2088,\n",
      "           2.2908],\n",
      "         [ 0.6832, -1.6518, -0.3179,  1.1011, -1.2169,  1.1806, -0.4332,\n",
      "           0.6549]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "EncoderLayer passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• EncoderLayer\n",
    "# ----------------------------\n",
    "enc_layer = EncoderLayer(8, 2, 16)\n",
    "x = torch.rand(2,5,8)  # batch_size=2, seq_len=5, d_model=8\n",
    "mask = torch.zeros(2,5)  # æ²¡æœ‰padding\n",
    "out = enc_layer(x, mask)\n",
    "\n",
    "print(\"Output of EncoderLayer:\\n\", out)\n",
    "assert out.shape == x.shape\n",
    "print(\"EncoderLayer passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - å®Œæ•´ç¼–ç å™¨ (Full Encoder)\n",
    "\n",
    "å¤ªæ£’äº†ï¼ä½ ç°åœ¨å·²ç»æˆåŠŸå®ç°äº†ä½ç½®ç¼–ç  (positional encoding)ã€è‡ªæ³¨æ„åŠ› (self-attention) å’Œç¼–ç å™¨å±‚ (encoder layer) â€”â€” ç»™è‡ªå·±ä¸€ä¸ªé¼“åŠ±çš„æŒå£°ğŸ‘ã€‚ç°åœ¨ä½ å‡†å¤‡å¥½æ„å»ºå®Œæ•´çš„ Transformer ç¼–ç å™¨ï¼ˆå›¾ 2bï¼‰äº†ï¼Œä½ å°†å¯¹è¾“å…¥è¿›è¡ŒåµŒå…¥ï¼Œå¹¶æ·»åŠ ä½ è®¡ç®—å¥½çš„ä½ç½®ç¼–ç ã€‚ç„¶åå°†ç¼–ç åçš„åµŒå…¥é€å…¥å¤šå±‚ç¼–ç å™¨å †æ ˆä¸­ã€‚\n",
    "\n",
    "<img src=\"encoder.png\" alt=\"Encoder\" width=\"330\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 2b: Transformer ç¼–ç å™¨</b></font></center></caption>\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### ç»ƒä¹  5 - Encoder\n",
    "\n",
    "ä½¿ç”¨ `call()` æ–¹æ³•å®Œæˆ `Encoder()` å‡½æ•°ï¼Œä»¥å¯¹è¾“å…¥è¿›è¡ŒåµŒå…¥ã€æ·»åŠ ä½ç½®ç¼–ç ï¼Œå¹¶å®ç°å¤šä¸ªç¼–ç å™¨å±‚ã€‚\n",
    "\n",
    "åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†ç”¨ Embedding å±‚ã€ä½ç½®ç¼–ç ä»¥åŠå¤šä¸ª EncoderLayer åˆå§‹åŒ–ç¼–ç å™¨ã€‚ä½ çš„ `call()` æ–¹æ³•åº”æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. å°†è¾“å…¥ä¼ å…¥ Embedding å±‚ã€‚\n",
    "2. å°†åµŒå…¥è¿›è¡Œç¼©æ”¾ (scale)ï¼Œé€šè¿‡ä¹˜ä»¥åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹æ¥å®ç°ã€‚è®°å¾—åœ¨è®¡ç®—å¹³æ–¹æ ¹å‰å°†åµŒå…¥ç»´åº¦è½¬æ¢ä¸º `tf.float32` æ•°æ®ç±»å‹ã€‚\n",
    "3. æ·»åŠ ä½ç½®ç¼–ç ï¼š`self.pos_encoding[:, :seq_len, :]` åŠ åˆ°åµŒå…¥ä¸Šã€‚\n",
    "4. å°†ç¼–ç åçš„åµŒå…¥ä¼ å…¥ dropout å±‚ï¼Œè®°å¾—ä½¿ç”¨ `training` å‚æ•°è®¾ç½®æ¨¡å‹çš„è®­ç»ƒæ¨¡å¼ã€‚\n",
    "5. ä½¿ç”¨ for å¾ªç¯å°† dropout å±‚çš„è¾“å‡ºä¼ å…¥ç¼–ç å™¨å †æ ˆä¸­çš„å¤šå±‚ç¼–ç å™¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7j2Tjr0K0t0I"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Encoder\n",
    "# ----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, max_pos_enc):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_pos_enc, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model,num_heads,dff) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:,:x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Encoder output:\n",
      " tensor([[[-1.2887,  1.1822,  0.1434,  1.1508, -1.2586,  1.1050, -0.1589,\n",
      "          -0.8753],\n",
      "         [-1.6305, -0.4302,  0.3122,  0.4399, -1.4670,  1.0302,  0.6190,\n",
      "           1.1265],\n",
      "         [ 1.5993, -0.0456,  0.2878, -1.2994, -1.0969,  1.4314, -0.5660,\n",
      "          -0.3106],\n",
      "         [-0.5984,  0.7124, -1.9254, -0.3253,  1.0719,  1.4347, -0.3355,\n",
      "          -0.0342],\n",
      "         [-0.9853,  0.8708,  1.1447,  1.3021, -1.7555, -0.3262, -0.2760,\n",
      "           0.0254],\n",
      "         [-1.1708, -0.4520,  1.6188,  1.5009, -0.7907,  0.1383,  0.1024,\n",
      "          -0.9470],\n",
      "         [-1.2659,  2.1031, -0.0899,  0.3234, -0.0807, -0.3000, -1.2189,\n",
      "           0.5288],\n",
      "         [-1.0928,  1.4618, -1.8016,  0.0960,  0.8019,  0.7824, -0.3770,\n",
      "           0.1292],\n",
      "         [-0.8542,  0.4746,  0.0064,  1.5150, -0.9984,  1.2456,  0.0918,\n",
      "          -1.4809],\n",
      "         [-0.2980,  1.2445, -0.5708,  1.5310, -0.2904,  0.6579, -0.5943,\n",
      "          -1.6799]],\n",
      "\n",
      "        [[-0.4503, -0.2007,  0.7637,  0.7922, -2.3831,  0.6355,  0.1887,\n",
      "           0.6540],\n",
      "         [ 0.3956,  0.5571,  1.3192,  1.0855, -0.7927, -1.3241,  0.2350,\n",
      "          -1.4757],\n",
      "         [-0.1400,  1.8894,  0.9057,  0.3194, -1.0930, -0.4067, -1.4587,\n",
      "          -0.0162],\n",
      "         [ 1.7187, -0.1119, -0.3900,  0.3503, -0.4529,  1.2143, -0.7354,\n",
      "          -1.5931],\n",
      "         [-1.2784,  0.3460, -1.3442,  1.1020, -1.0457,  0.7597,  0.2367,\n",
      "           1.2239],\n",
      "         [-1.0760,  1.5706,  0.9995, -0.4401, -0.9342, -0.5400, -0.7722,\n",
      "           1.1924],\n",
      "         [ 0.1224, -0.5694, -0.3346, -1.7437, -0.6839,  1.5136,  0.4527,\n",
      "           1.2429],\n",
      "         [-0.9939,  1.0276,  1.7131,  0.2435, -1.3620,  0.2815, -1.0091,\n",
      "           0.0993],\n",
      "         [ 1.7805,  0.4739, -0.2814, -0.7870, -1.3101,  1.1747, -0.1657,\n",
      "          -0.8849],\n",
      "         [ 1.0446,  0.5810,  0.8287, -1.5682,  0.8038, -0.6254, -1.4865,\n",
      "           0.4221]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "Encoder passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• Encoder\n",
    "# ----------------------------\n",
    "encoder = Encoder(2,8,2,16,50,100)\n",
    "x_inp = torch.randint(0,50,(2,10))  # batch_size=2, seq_len=10\n",
    "out = encoder(x_inp)\n",
    "print(\"Final Encoder output:\\n\", out)\n",
    "assert out.shape == (2,10,8)\n",
    "print(\"Encoder passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - è§£ç å™¨ (Decoder)\n",
    "\n",
    "è§£ç å™¨å±‚æ¥æ”¶ç¼–ç å™¨ç”Ÿæˆçš„ K å’Œ V çŸ©é˜µï¼Œå¹¶ä½¿ç”¨è¾“å‡ºçš„ Q çŸ©é˜µè®¡ç®—ç¬¬äºŒä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ (multi-head attention)ï¼ˆå›¾ 3aï¼‰ã€‚\n",
    "\n",
    "<img src=\"decoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 3a: Transformer è§£ç å™¨å±‚</b></font></center></caption>\n",
    "\n",
    "<a name='5-1'></a>    \n",
    "### 5.1 - è§£ç å™¨å±‚ (Decoder Layer)\n",
    "\n",
    "åŒæ ·ï¼Œä½ å°†å¤šå¤´æ³¨æ„åŠ›ä¸å‰é¦ˆç¥ç»ç½‘ç»œç»“åˆï¼Œä½†è¿™æ¬¡éœ€è¦å®ç°ä¸¤å±‚å¤šå¤´æ³¨æ„åŠ›å±‚ã€‚ä½ è¿˜ä¼šä½¿ç”¨æ®‹å·®è¿æ¥ (residual connections) å’Œå±‚å½’ä¸€åŒ– (layer normalization) æ¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆå›¾ 3aï¼‰ã€‚\n",
    "\n",
    "<a name='ex-6'></a>    \n",
    "### ç»ƒä¹  6 - DecoderLayer\n",
    "\n",
    "ä½¿ç”¨ `call()` æ–¹æ³•å®ç° `DecoderLayer()`ï¼š\n",
    "\n",
    "1. **Block 1** æ˜¯å¸¦æœ‰æ®‹å·®è¿æ¥å’Œå‰ç»æ©ç  (look-ahead mask) çš„å¤šå¤´æ³¨æ„åŠ›å±‚ã€‚ä¸ `EncoderLayer` ç±»ä¼¼ï¼ŒDropout åœ¨å¤šå¤´æ³¨æ„åŠ›å±‚ä¸­å®šä¹‰ã€‚\n",
    "2. **Block 2** ä¼šè€ƒè™‘ç¼–ç å™¨çš„è¾“å‡ºï¼Œå› æ­¤å¤šå¤´æ³¨æ„åŠ›å±‚å°†æ¥æ”¶æ¥è‡ªç¼–ç å™¨çš„ K å’Œ Vï¼Œä»¥åŠæ¥è‡ª Block 1 çš„ Qã€‚ç„¶ååƒåœ¨ `EncoderLayer` ä¸­ä¸€æ ·ï¼Œåº”ç”¨å½’ä¸€åŒ–å±‚å’Œæ®‹å·®è¿æ¥ã€‚\n",
    "3. **Block 3** æ˜¯ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼Œå¸¦æœ‰ Dropoutã€å½’ä¸€åŒ–å±‚å’Œæ®‹å·®è¿æ¥ã€‚\n",
    "    \n",
    "**é¢å¤–æç¤ºï¼š**\n",
    "* å‰ä¸¤ä¸ªå—ä¸ `EncoderLayer` ç±»ä¼¼ï¼Œä¸è¿‡åœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›æ—¶ï¼Œéœ€è¦è¿”å› `attention_scores`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wEouNFvCzMeT"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Decoder Layer\n",
    "# ----------------------------\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff):\n",
    "        super().__init__()\n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "        self.ffn = FullyConnected(d_model,dff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        attn1,_ = self.mha1(x,x,x,key_padding_mask=look_ahead_mask)\n",
    "        x = self.norm1(x + attn1)\n",
    "        attn2,_ = self.mha2(x, enc_output, enc_output, key_padding_mask=padding_mask)\n",
    "        x = self.norm2(x + attn2)\n",
    "        ffn_out = self.dropout(self.ffn(x))\n",
    "        x = self.norm3(x + ffn_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DecoderLayer output:\n",
      " tensor([[[ 0.2048, -1.4163,  1.7223,  1.0603, -0.3060,  0.1882, -0.1452,\n",
      "          -1.3082],\n",
      "         [-0.7742, -1.2328,  1.2440,  0.9438,  0.0511, -0.8935,  1.4314,\n",
      "          -0.7698],\n",
      "         [ 0.0073, -0.2614,  1.8183,  1.2853, -0.6405, -0.3329, -0.3497,\n",
      "          -1.5264],\n",
      "         [ 0.8275,  0.4722,  2.0929, -0.4023, -1.1409, -0.4757, -0.4880,\n",
      "          -0.8855],\n",
      "         [-1.4754, -0.2938,  1.5280,  1.0081, -1.2989,  0.3728,  0.6024,\n",
      "          -0.4432]],\n",
      "\n",
      "        [[-1.2651,  0.4915,  1.2030,  1.3381,  0.0392,  0.2664, -0.4455,\n",
      "          -1.6277],\n",
      "         [-0.9654, -1.8488,  1.3095,  0.7773, -0.1100,  0.8894,  0.4869,\n",
      "          -0.5388],\n",
      "         [ 0.6265, -0.8522,  0.8081, -0.0264, -1.2837,  1.9259, -0.8751,\n",
      "          -0.3232],\n",
      "         [ 0.0217,  0.3204,  1.1604,  0.8521, -1.2287,  0.2628,  0.5864,\n",
      "          -1.9753],\n",
      "         [-0.6750,  0.5070,  2.2912,  0.2357, -1.1515, -0.1208, -0.7032,\n",
      "          -0.3834]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "DecoderLayer passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• DecoderLayer\n",
    "# ----------------------------\n",
    "dec_layer = DecoderLayer(8,2,16)\n",
    "enc_out = torch.rand(2,10,8)   # Encoderè¾“å‡º(batch_size=2, seq_len=10, d_model=8)\n",
    "x_tar = torch.rand(2,5,8)      # Decoderè¾“å…¥(batch_size=2, target_seq_len=5, d_model=8)\n",
    "out = dec_layer(x_tar, enc_out)\n",
    "print(\"Final DecoderLayer output:\\n\", out)\n",
    "assert out.shape == x_tar.shape\n",
    "print(\"DecoderLayer passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-2'></a> \n",
    "### 5.2 - å®Œæ•´è§£ç å™¨ (Full Decoder)\n",
    "\n",
    "ä½ å·²ç»æ¥è¿‘å®Œæˆäº†ï¼ç°åœ¨ä½¿ç”¨è§£ç å™¨å±‚ (DecoderLayer) æ¥æ„å»ºå®Œæ•´çš„ Transformer è§£ç å™¨ï¼ˆå›¾ 3bï¼‰ã€‚ä½ éœ€è¦å¯¹è¾“å‡ºè¿›è¡ŒåµŒå…¥ (embedding) å¹¶æ·»åŠ ä½ç½®ç¼–ç ï¼Œç„¶åå°†ç¼–ç åçš„åµŒå…¥è¾“å…¥åˆ°å¤šå±‚è§£ç å™¨å †æ ˆä¸­ã€‚\n",
    "\n",
    "<img src=\"decoder.png\" alt=\"Encoder\" width=\"300\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 3b: Transformer è§£ç å™¨</b></font></center></caption>\n",
    "\n",
    "<a name='ex-7'></a>     \n",
    "### ç»ƒä¹  7 - Decoder\n",
    "\n",
    "ä½¿ç”¨ `call()` æ–¹æ³•å®ç° `Decoder()`ï¼Œå¯¹è¾“å‡ºè¿›è¡ŒåµŒå…¥ã€æ·»åŠ ä½ç½®ç¼–ç ï¼Œå¹¶å®ç°å¤šå±‚è§£ç å™¨ã€‚\n",
    "\n",
    "åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†åˆå§‹åŒ–è§£ç å™¨ï¼ŒåŒ…å«åµŒå…¥å±‚ã€ä½ç½®ç¼–ç å’Œå¤šä¸ª DecoderLayersã€‚ä½ çš„ `call()` æ–¹æ³•å°†æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "1. å°†ç”Ÿæˆçš„è¾“å‡ºé€šè¿‡åµŒå…¥å±‚ (Embedding layer)ã€‚\n",
    "2. å°†åµŒå…¥å‘é‡æŒ‰åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹è¿›è¡Œç¼©æ”¾ã€‚è®°å¾—åœ¨è®¡ç®—å¹³æ–¹æ ¹å‰ï¼Œå°†åµŒå…¥ç»´åº¦è½¬æ¢ä¸º `tf.float32` ç±»å‹ã€‚\n",
    "3. æ·»åŠ ä½ç½®ç¼–ç ï¼š`self.pos_encoding[:, :seq_len, :]` åˆ°åµŒå…¥å‘é‡ä¸­ã€‚\n",
    "4. å°†ç¼–ç åçš„åµŒå…¥é€šè¿‡ Dropout å±‚ï¼Œä½¿ç”¨ `training` å‚æ•°è®¾ç½®æ¨¡å‹çš„è®­ç»ƒæ¨¡å¼ã€‚\n",
    "5. ä½¿ç”¨ for å¾ªç¯å°† Dropout å±‚è¾“å‡ºä¼ å…¥è§£ç å™¨å±‚å †æ ˆ (DecoderLayers)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "McS3by6k4pnP"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Decoder\n",
    "# ----------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_pos_enc):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_pos_enc, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model,num_heads,dff) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x, enc_output):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = x + self.pos_encoding[:,:x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Decoder output:\n",
      " tensor([[[ 0.1341,  0.6275,  0.7025,  0.5792, -1.8678, -1.5213,  0.8322,\n",
      "           0.5136],\n",
      "         [ 0.7185,  0.0637, -0.2469,  1.0986, -1.2498, -1.2965,  1.5859,\n",
      "          -0.6736],\n",
      "         [ 0.3267,  0.2996,  0.8264,  0.6075, -1.9368, -1.4355,  0.8541,\n",
      "           0.4580],\n",
      "         [-0.2054, -1.8874, -1.1054, -0.1506,  0.7862,  0.4343,  1.2630,\n",
      "           0.8653],\n",
      "         [-0.5874, -2.0008, -0.2940, -0.4756,  0.9730,  0.2215,  1.1216,\n",
      "           1.0418]],\n",
      "\n",
      "        [[ 0.1709,  0.6166, -0.1811, -0.3149, -1.4836, -1.3867,  1.2195,\n",
      "           1.3592],\n",
      "         [-1.2746, -0.7594, -1.5833,  1.1744,  0.3318,  1.1219,  0.3296,\n",
      "           0.6597],\n",
      "         [-0.0075,  0.0063, -1.7500,  0.3403, -0.8690, -0.3657,  0.8574,\n",
      "           1.7882],\n",
      "         [-1.4265, -0.6063,  0.9315,  1.2528,  0.5386, -0.1303, -1.4410,\n",
      "           0.8812],\n",
      "         [-0.3214,  0.0809, -1.6887,  0.5750, -0.3769, -0.7176,  0.4989,\n",
      "           1.9499]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "Decoder passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• Decoder (æ•°å­—è¾“å‡º)\n",
    "# ----------------------------\n",
    "decoder = Decoder(2, 8, 2, 16, 50, 100)\n",
    "enc_out = torch.rand(2,10,8)  # å‡è®¾ Encoder è¾“å‡º\n",
    "x_tar = torch.randint(0,50,(2,5))\n",
    "out = decoder(x_tar, enc_out)\n",
    "print(\"\\nFinal Decoder output:\\n\", out)\n",
    "assert out.shape == (2,5,8)\n",
    "print(\"Decoder passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a> \n",
    "## 6 - Transformer\n",
    "\n",
    "å‘¼ï¼è¿™æ¬¡ä½œä¸šçœŸçš„å¾ˆé•¿ï¼Œç°åœ¨ä½ å·²ç»æ¥åˆ°äº†æ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹çš„æœ€åä¸€ä¸ªç»ƒä¹ ã€‚æ­å–œï¼ä½ å·²ç»å®Œæˆäº†æ‰€æœ‰çš„æ ¸å¿ƒå·¥ä½œï¼Œç°åœ¨æ˜¯æ—¶å€™æŠŠæ‰€æœ‰æ¨¡å—æ•´åˆèµ·æ¥äº†ã€‚  \n",
    "\n",
    "<img src=\"transformer.png\" alt=\"Transformer\" width=\"550\"/>\n",
    "<caption><center><font color='purple'><b>å›¾ 4: Transformer</b></font></center></caption>\n",
    "    \n",
    "Transformer æ¶æ„çš„æ•°æ®æµå¦‚ä¸‹ï¼š\n",
    "* é¦–å…ˆï¼Œè¾“å…¥é€šè¿‡ç¼–ç å™¨ (Encoder)ï¼Œä¹Ÿå°±æ˜¯ä½ å®ç°çš„å¤šä¸ª Encoder å±‚çš„å †å ï¼š\n",
    "    - è¾“å…¥çš„åµŒå…¥å’Œä½ç½®ç¼–ç \n",
    "    - å¯¹è¾“å…¥çš„å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)\n",
    "    - å‰é¦ˆç¥ç»ç½‘ç»œ (Feed Forward Neural Network) ç”¨äºç‰¹å¾æå–\n",
    "* ç„¶åï¼Œé¢„æµ‹è¾“å‡ºé€šè¿‡è§£ç å™¨ (Decoder)ï¼Œç”±ä½ å®ç°çš„ Decoder å±‚ç»„æˆï¼š\n",
    "    - è¾“å‡ºçš„åµŒå…¥å’Œä½ç½®ç¼–ç \n",
    "    - å¯¹ç”Ÿæˆè¾“å‡ºçš„å¤šå¤´æ³¨æ„åŠ›\n",
    "    - ä½¿ç”¨æ¥è‡ªç¬¬ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚çš„ Qï¼Œä»¥åŠç¼–ç å™¨è¾“å‡ºçš„ K å’Œ V è¿›è¡Œå¤šå¤´æ³¨æ„åŠ›\n",
    "    - å‰é¦ˆç¥ç»ç½‘ç»œå¸®åŠ©æå–ç‰¹å¾\n",
    "* æœ€åï¼Œåœ¨ç¬¬ N ä¸ªè§£ç å™¨å±‚ä¹‹åï¼Œåº”ç”¨ä¸¤ä¸ªå…¨è¿æ¥å±‚å’Œ softmax æ¥ç”Ÿæˆåºåˆ—ä¸­ä¸‹ä¸€ä¸ªè¾“å‡ºçš„é¢„æµ‹ã€‚\n",
    "\n",
    "<a name='ex-8'></a> \n",
    "### ç»ƒä¹  8 - Transformer\n",
    "\n",
    "ä½¿ç”¨ `call()` æ–¹æ³•å®ç° `Transformer()`ï¼š\n",
    "1. å°†è¾“å…¥é€šè¿‡ç¼–ç å™¨ï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„æ©ç  (mask)ã€‚\n",
    "2. å°†ç¼–ç å™¨è¾“å‡ºå’Œç›®æ ‡åºåˆ—é€šè¿‡è§£ç å™¨ï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„æ©ç ã€‚\n",
    "3. ä½¿ç”¨çº¿æ€§å˜æ¢ (linear transformation) å’Œ softmax ç”Ÿæˆé¢„æµ‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "QHymPmaj-2ba"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Transformer\n",
    "# ----------------------------\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, max_pos_enc_input, max_pos_enc_target):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers,d_model,num_heads,dff,input_vocab_size,max_pos_enc_input)\n",
    "        self.decoder = Decoder(num_layers,d_model,num_heads,dff,target_vocab_size,max_pos_enc_target)\n",
    "        self.final_layer = nn.Linear(d_model,target_vocab_size)\n",
    "    def forward(self, inp, tar):\n",
    "        enc_out = self.encoder(inp)\n",
    "        dec_out = self.decoder(tar, enc_out)\n",
    "        return F.softmax(self.final_layer(dec_out), dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence indices (x_inp): tensor([[37, 38, 30, 31,  0, 49, 49, 48, 40, 35],\n",
      "        [ 7, 18, 12, 31, 21, 42, 33, 27, 36, 33]])\n",
      "Target sequence indices (x_tar): tensor([[ 9, 17, 19, 21, 24],\n",
      "        [33, 47, 42, 21, 19]])\n",
      "\n",
      "Transformer output shape: torch.Size([2, 5, 50])\n",
      "Transformer output sample values (first batch, first 3 tokens):\n",
      "tensor([[0.0089, 0.0054, 0.0122, 0.0095, 0.0430],\n",
      "        [0.0427, 0.0088, 0.0198, 0.0269, 0.0178],\n",
      "        [0.0453, 0.0128, 0.0333, 0.0264, 0.0131]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Output statistics:\n",
      "Max value: 0.07378123700618744\n",
      "Min value: 0.00341250142082572\n",
      "Mean value: 0.019999999552965164\n",
      "Sum along vocab dimension (should be 1 for softmax): 1.0\n",
      "\n",
      "Transformer passed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# æµ‹è¯• Transformerï¼ˆå¢åŠ æ•°å­—è¾“å‡ºï¼‰\n",
    "# ----------------------------\n",
    "model = Transformer(2,8,2,16,50,50,100,100)\n",
    "x_inp = torch.randint(0,50,(2,10))\n",
    "x_tar = torch.randint(0,50,(2,5))\n",
    "print(\"Input sequence indices (x_inp):\", x_inp)\n",
    "print(\"Target sequence indices (x_tar):\", x_tar)\n",
    "\n",
    "out = model(x_inp,x_tar)\n",
    "print(\"\\nTransformer output shape:\", out.shape)\n",
    "print(\"Transformer output sample values (first batch, first 3 tokens):\")\n",
    "print(out[0,:3,:5])  # æ˜¾ç¤ºç¬¬ä¸€æ¡åºåˆ—å‰ä¸‰ä¸ªä½ç½®ï¼Œå‰äº”ä¸ªè¯çš„æ¦‚ç‡\n",
    "\n",
    "# æ‰“å°æ•°å€¼ç»Ÿè®¡ä¿¡æ¯\n",
    "print(\"\\nOutput statistics:\")\n",
    "print(\"Max value:\", out.max().item())\n",
    "print(\"Min value:\", out.min().item())\n",
    "print(\"Mean value:\", out.mean().item())\n",
    "print(\"Sum along vocab dimension (should be 1 for softmax):\", out[0,0,:].sum().item())\n",
    "\n",
    "print(\"\\nTransformer passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»“è®º\n",
    "\n",
    "ä½ å·²ç»å®Œæˆäº†æœ¬æ¬¡ä½œä¸šçš„è¯„åˆ†éƒ¨åˆ†ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»æŒæ¡äº†ä»¥ä¸‹å†…å®¹ï¼š\n",
    "\n",
    "* åˆ›å»ºä½ç½®ç¼–ç  (Positional Encodings) æ¥æ•æ‰æ•°æ®ä¸­çš„åºåˆ—å…³ç³»\n",
    "* ä½¿ç”¨è¯åµŒå…¥ (Word Embeddings) è®¡ç®—ç¼©æ”¾ç‚¹ç§¯è‡ªæ³¨æ„åŠ› (Scaled Dot-Product Self-Attention)\n",
    "* å®ç°å¸¦æ©ç çš„å¤šå¤´æ³¨æ„åŠ› (Masked Multi-Head Attention)\n",
    "* æ„å»ºå¹¶è®­ç»ƒä¸€ä¸ª Transformer æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    <b>ä½ éœ€è¦è®°ä½çš„å†…å®¹</b>ï¼š\n",
    "\n",
    "- è‡ªæ³¨æ„åŠ› (Self-Attention) ä¸å·ç§¯ç½‘ç»œå±‚ (Convolutional Network Layers) çš„ç»“åˆèƒ½å¤Ÿå®ç°è®­ç»ƒçš„å¹¶è¡ŒåŒ–ï¼Œä»è€ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚\n",
    "- è‡ªæ³¨æ„åŠ›æ˜¯é€šè¿‡ç”Ÿæˆçš„æŸ¥è¯¢çŸ©é˜µ Qã€é”®çŸ©é˜µ K å’Œå€¼çŸ©é˜µ V æ¥è®¡ç®—çš„ã€‚\n",
    "- å°†ä½ç½®ç¼–ç  (Positional Encoding) æ·»åŠ åˆ°è¯åµŒå…¥ (Word Embeddings) ä¸­ï¼Œæ˜¯åœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­å¼•å…¥åºåˆ—ä¿¡æ¯çš„æœ‰æ•ˆæ–¹æ³•ã€‚\n",
    "- å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention) æœ‰åŠ©äºæ£€æµ‹å¥å­ä¸­çš„å¤šä¸ªç‰¹å¾ã€‚\n",
    "- æ©ç  (Masking) é˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­â€œå·çœ‹â€æœªæ¥ä¿¡æ¯ï¼Œæˆ–è€…åœ¨å¤„ç†æˆªæ–­å¥å­æ—¶è¿‡åº¦åŠ æƒé›¶å€¼ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ä½ å·²ç»å®Œæˆäº† Transformer ä½œä¸šï¼Œåˆ«å¿˜äº†æŸ¥çœ‹éè¯„åˆ†å®éªŒ (ungraded labs)ï¼Œå°† Transformer æ¨¡å‹åº”ç”¨åˆ°å®é™…æ¡ˆä¾‹ä¸­ï¼Œæ¯”å¦‚å‘½åå®ä½“è¯†åˆ« (NER) å’Œé—®ç­” (QA) ã€‚\n",
    "\n",
    "# æ­å–œä½ å®Œæˆæ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹ï¼ï¼ï¼ï¼ ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
    "\n",
    "è¿™æ˜¯è¯¥ä¸“é¡¹è¯¾ç¨‹çš„æœ€åä¸€ä¸ªè¯„åˆ†ä½œä¸šã€‚ç°åœ¨æ˜¯æ—¶å€™åº†ç¥ä½ æ‰€æœ‰çš„åŠªåŠ›å’Œä»˜å‡ºäº†ï¼\n",
    "\n",
    "<a name='7'></a> \n",
    "## 7 - å‚è€ƒæ–‡çŒ®\n",
    "\n",
    "Transformer ç®—æ³•ç”± Vaswani ç­‰äººæå‡º (2017)ã€‚\n",
    "\n",
    "- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
