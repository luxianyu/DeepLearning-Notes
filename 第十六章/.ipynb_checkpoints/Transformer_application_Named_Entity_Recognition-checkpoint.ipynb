{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer ç½‘ç»œåº”ç”¨ï¼šå‘½åå®ä½“è¯†åˆ«ï¼ˆNamed-Entity Recognitionï¼‰\n",
    "\n",
    "åœ¨æœ¬ç¬”è®°æœ¬ä¸­ï¼Œä½ å°†æ¢ç´¢ä¸€ç§åŸºäºä¸Šä¸€ä¸ªä½œä¸šä¸­æ‰€æ„å»ºçš„ Transformer æ¶æ„çš„åº”ç”¨ã€‚\n",
    "\n",
    "**å®Œæˆæœ¬æ¬¡å®éªŒåï¼Œä½ å°†èƒ½å¤Ÿï¼š**\n",
    "\n",
    "* ä½¿ç”¨ HuggingFace åº“ä¸­çš„åˆ†è¯å™¨ï¼ˆtokenizersï¼‰å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼ˆpre-trained modelsï¼‰ã€‚\n",
    "* å¯¹é¢„è®­ç»ƒçš„ Transformer æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆfine-tuningï¼‰ï¼Œä»¥å®ç°å‘½åå®ä½“è¯†åˆ«ï¼ˆNamed-Entity Recognition, NERï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Abhishek Jha Application Development Associate...   \n",
      "1  Afreen Jamadar Active member of IIIT Committee...   \n",
      "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
      "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
      "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
      "\n",
      "                                          annotation  \n",
      "0  [{'label': ['Skills'], 'points': [{'start': 12...  \n",
      "1  [{'label': ['Email Address'], 'points': [{'sta...  \n",
      "2  [{'label': ['Skills'], 'points': [{'start': 37...  \n",
      "3  [{'label': ['Skills'], 'points': [{'start': 80...  \n",
      "4  [{'label': ['Degree'], 'points': [{'start': 20...  \n",
      "                                             content  \\\n",
      "0  Abhishek Jha Application Development Associate...   \n",
      "1  Afreen Jamadar Active member of IIIT Committee...   \n",
      "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
      "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
      "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
      "\n",
      "                                          annotation  \\\n",
      "0  [{'label': ['Skills'], 'points': [{'start': 12...   \n",
      "1  [{'label': ['Email Address'], 'points': [{'sta...   \n",
      "2  [{'label': ['Skills'], 'points': [{'start': 37...   \n",
      "3  [{'label': ['Skills'], 'points': [{'start': 80...   \n",
      "4  [{'label': ['Degree'], 'points': [{'start': 20...   \n",
      "\n",
      "                                            entities  \n",
      "0  [(0, 12, Name), (13, 46, Designation), (49, 58...  \n",
      "1  [(0, 14, Name), (62, 68, Location), (104, 148,...  \n",
      "2  [(0, 21, Name), (22, 31, Location), (65, 117, ...  \n",
      "3  [(0, 12, Name), (13, 51, Designation), (54, 60...  \n",
      "4  [(0, 13, Name), (14, 22, Designation), (24, 41...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:01<00:00, 185.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    setences_cleaned\n",
      "0  [Name, Name, Designation, Designation, Designa...\n",
      "1  [Name, Name, Empty, Empty, Empty, Empty, Empty...\n",
      "2  [Name, Name, Name, Empty, Empty, Empty, Empty,...\n",
      "3  [Name, Name, Designation, Designation, Designa...\n",
      "4  [Name, Name, Designation, Empty, Companies wor...\n",
      "{'Name', 'College Name', 'Skills', 'Companies worked at', 'Designation', 'Location', 'Years of Experience', 'Graduation Year', 'UNKNOWN', 'Empty', 'Email Address', 'Degree'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60255350ded4b4abaa9f9292e8f5426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luxia\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\luxia\\.cache\\huggingface\\hub\\models--distilbert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f2b41eb95242b7b3745b260625bbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1648e0160af4459abd6cbad545ced8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb002d1d0e4a4c479c11ed63e554722f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27192899422d4736aed4869d33912f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [07:21<00:00, 31.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¹³å‡æŸå¤±: 1.4096\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [07:16<00:00, 31.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¹³å‡æŸå¤±: 0.5650\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [07:20<00:00, 31.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¹³å‡æŸå¤±: 0.5159\n",
      "âœ… è®­ç»ƒå®Œæˆï¼\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ° 'distilbert_ner_model' æ–‡ä»¶å¤¹ã€‚\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# å¯¼å…¥ä¾èµ–åº“\n",
    "# -------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. è¯»å–å¹¶é¢„å¤„ç†åŸå§‹ JSON æ•°æ®\n",
    "# -------------------------------------------------------------\n",
    "try:\n",
    "    df_data = pd.read_json(\"ner.json\", lines=True, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df_data = pd.read_json(\"ner.json\", lines=True, encoding=\"utf-8-sig\")\n",
    "\n",
    "df_data = df_data.drop(['extras'], axis=1, errors='ignore')\n",
    "df_data['content'] = df_data['content'].str.replace(\"\\n\", \" \")\n",
    "print(df_data.head())\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. å®šä¹‰åˆå¹¶åŒºé—´å‡½æ•°\n",
    "# -------------------------------------------------------------\n",
    "def mergeIntervals(intervals):\n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            if higher[0] <= lower[1]:\n",
    "                if lower[2] is higher[2]:\n",
    "                    upper_bound = max(lower[1], higher[1])\n",
    "                    merged[-1] = (lower[0], upper_bound, lower[2])\n",
    "                else:\n",
    "                    if lower[1] > higher[1]:\n",
    "                        merged[-1] = lower\n",
    "                    else:\n",
    "                        merged[-1] = (lower[0], higher[1], higher[2])\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "    return merged\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. æå–å®ä½“ä¿¡æ¯\n",
    "# -------------------------------------------------------------\n",
    "def get_entities(df):\n",
    "    entities = []\n",
    "    for i in range(len(df)):\n",
    "        entity = []\n",
    "        for annot in df['annotation'][i]:\n",
    "            try:\n",
    "                ent = annot['label'][0]\n",
    "                start = annot['points'][0]['start']\n",
    "                end = annot['points'][0]['end'] + 1\n",
    "                entity.append((start, end, ent))\n",
    "            except:\n",
    "                pass\n",
    "        entity = mergeIntervals(entity)\n",
    "        entities.append(entity)\n",
    "    return entities\n",
    "\n",
    "df_data['entities'] = get_entities(df_data)\n",
    "print(df_data.head())\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. å°† dataturks JSON è½¬ä¸º spaCy æ ¼å¼\n",
    "# -------------------------------------------------------------\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        with open(dataturks_JSON_FilePath, 'r', encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content'].replace(\"\\n\", \" \")\n",
    "            entities = []\n",
    "            data_annotations = data.get('annotation', [])\n",
    "            if data_annotations:\n",
    "                for annotation in data_annotations:\n",
    "                    point = annotation['points'][0]\n",
    "                    labels = annotation['label']\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        point_start = point['start']\n",
    "                        point_end = point['end']\n",
    "                        point_text = point['text']\n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start += lstrip_diff\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end -= rstrip_diff\n",
    "                        entities.append((point_start, point_end + 1, label))\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return []\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5. æ¸…ç†å®ä½“åŒºé—´ç©ºæ ¼\n",
    "# -------------------------------------------------------------\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data  \n",
    "\n",
    "data = trim_entity_spans(convert_dataturks_to_spacy(\"ner.json\"))\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 6. æ¸…æ´—æ•°æ®é›†\n",
    "# -------------------------------------------------------------\n",
    "def clean_dataset(data):\n",
    "    cleaned_rows = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        start = 0\n",
    "        emptyList = [\"Empty\"] * len(data[i][0].split())\n",
    "        numberOfWords = 0\n",
    "        lenOfString = len(data[i][0])\n",
    "        strData = data[i][0]\n",
    "        strDictData = data[i][1]\n",
    "        lastIndexOfSpace = strData.rfind(' ')\n",
    "        for j in range(lenOfString):\n",
    "            if strData[j] == \" \" and j + 1 < lenOfString and strData[j + 1] != \" \":\n",
    "                for k, v in strDictData.items():\n",
    "                    for m in range(len(v)):\n",
    "                        entList = v[len(v) - m - 1]\n",
    "                        if start >= int(entList[0]) and j <= int(entList[1]):\n",
    "                            emptyList[numberOfWords] = entList[2]\n",
    "                            break\n",
    "                start = j + 1\n",
    "                numberOfWords += 1\n",
    "            if j == lastIndexOfSpace:\n",
    "                for m in range(len(v)):\n",
    "                    entList = v[len(v) - m - 1]\n",
    "                    if lastIndexOfSpace >= int(entList[0]) and lenOfString <= int(entList[1]):\n",
    "                        emptyList[numberOfWords] = entList[2]\n",
    "                        numberOfWords += 1\n",
    "        cleaned_rows.append([emptyList])\n",
    "    cleanedDF = pd.DataFrame(cleaned_rows, columns=[\"setences_cleaned\"])\n",
    "    return cleanedDF\n",
    "\n",
    "cleanedDF = clean_dataset(data)\n",
    "print(cleanedDF.head())\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 7. æ ‡ç­¾ç¼–ç \n",
    "# -------------------------------------------------------------\n",
    "unique_tags = set(cleanedDF['setences_cleaned'].explode().unique())\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "print(unique_tags)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 8. æ ‡ç­¾å¡«å……\n",
    "# -------------------------------------------------------------\n",
    "MAX_LEN = 512\n",
    "labels = cleanedDF['setences_cleaned'].values.tolist()\n",
    "tags = [torch.tensor([tag2id.get(l) for l in lab]) for lab in labels]\n",
    "tags = pad_sequence(tags, batch_first=True, padding_value=tag2id[\"Empty\"])\n",
    "tags = torch.nn.functional.pad(tags, (0, MAX_LEN - tags.size(1)), value=tag2id[\"Empty\"])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 9. Tokenizationï¼ˆç›´æ¥ä½¿ç”¨å®˜æ–¹Tokenizerï¼‰\n",
    "# -------------------------------------------------------------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, examples, tags):\n",
    "    tokenized_inputs = tokenizer(examples, truncation=True, padding='max_length', max_length=512, is_split_into_words=False)\n",
    "    labels_aligned = []\n",
    "    for i, label in enumerate(tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels_aligned.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels_aligned\n",
    "    return tokenized_inputs\n",
    "\n",
    "test = tokenize_and_align_labels(tokenizer, df_data['content'].values.tolist(), tags)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 10. PyTorch Dataset å®šä¹‰\n",
    "# -------------------------------------------------------------\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "train_dataset = NERDataset(test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 11. æ¨¡å‹åŠ è½½ï¼ˆè‡ªåŠ¨ä¸‹è½½ PyTorch æƒé‡ï¼‰\n",
    "# -------------------------------------------------------------\n",
    "model = DistilBertForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-cased',\n",
    "    num_labels=len(unique_tags)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 12. ä¼˜åŒ–å™¨è®¾ç½®\n",
    "# -------------------------------------------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 13. è®­ç»ƒå¾ªç¯\n",
    "# -------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch + 1}/3\")\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"å¹³å‡æŸå¤±: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"âœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 14. æ¨¡å‹ä¿å­˜\n",
    "# -------------------------------------------------------------\n",
    "model.save_pretrained(\"distilbert_ner_model\")\n",
    "tokenizer.save_pretrained(\"distilbert_ner_model\")\n",
    "print(\"æ¨¡å‹å·²ä¿å­˜åˆ° 'distilbert_ner_model' æ–‡ä»¶å¤¹ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç›®å½•\n",
    "\n",
    "- [Packages](#0)\n",
    "- [1 - ç”¨å‘½åå®ä½“è¯†åˆ«å¤„ç†ç®€å†](#1)\n",
    "    - [1.1 - æ•°æ®æ¸…æ´—](#1-1)\n",
    "    - [1.2 - å¡«å……ä¸æ ‡ç­¾ç”Ÿæˆ](#1-2)\n",
    "    - [1.3 - ä½¿ç”¨ ğŸ¤— åº“è¿›è¡Œåˆ†è¯å’Œæ ‡ç­¾å¯¹é½](#1-3)\n",
    "        - [ç»ƒä¹  1 - tokenize_and_align_labels](#ex-1)\n",
    "    - [1.4 - ä¼˜åŒ–](#1-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## åŒ…\n",
    "\n",
    "è¿è¡Œä¸‹é¢çš„ä»£ç å•å…ƒä»¥åŠ è½½æ‰€éœ€çš„åŒ…ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# å¯¼å…¥å®éªŒæ‰€éœ€ä¾èµ–åº“\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# pandas æ˜¯ Python ä¸­æœ€å¸¸ç”¨çš„æ•°æ®åˆ†æåº“ä¹‹ä¸€ï¼Œç”¨äºè¯»å–ã€å¤„ç†å’Œå±•ç¤ºç»“æ„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚ CSVã€JSONã€Excelï¼‰\n",
    "import pandas as pd\n",
    "\n",
    "# torch æ˜¯ PyTorch çš„æ ¸å¿ƒåŒ…ï¼Œæä¾›å¼ é‡ï¼ˆTensorï¼‰è®¡ç®—ã€è‡ªåŠ¨æ±‚å¯¼ã€æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ„å»ºä¸è®­ç»ƒç­‰åŠŸèƒ½\n",
    "import torch\n",
    "\n",
    "# Dataset å’Œ DataLoader æ˜¯ PyTorch ä¸­çš„ä¸¤ä¸ªé‡è¦æ•°æ®ç»„ä»¶ï¼š\n",
    "# - Datasetï¼šå®šä¹‰æ•°æ®é›†çš„ç»“æ„ï¼ˆå¦‚å¦‚ä½•è¯»å–æ ·æœ¬å’Œæ ‡ç­¾ï¼‰\n",
    "# - DataLoaderï¼šè´Ÿè´£æ‰¹é‡åŠ è½½æ•°æ®ï¼Œå¹¶åœ¨è®­ç»ƒæ—¶è‡ªåŠ¨æ‰“ä¹±é¡ºåºï¼ˆshuffleï¼‰å’Œæ‰¹å¤„ç†ï¼ˆbatchingï¼‰\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# AdamW æ˜¯ä¸€ç§æ”¹è¿›çš„ Adam ä¼˜åŒ–å™¨ï¼ˆå¸¦æƒé‡è¡°å‡é¡¹ï¼ŒW ä»£è¡¨ weight decayï¼‰ï¼Œ\n",
    "# å®ƒæ˜¯ Transformer ç³»åˆ—æ¨¡å‹ï¼ˆå¦‚ BERTã€DistilBERTï¼‰å®˜æ–¹æ¨èçš„ä¼˜åŒ–ç®—æ³•\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ä» Hugging Face Transformers åº“å¯¼å…¥ä»¥ä¸‹ä¸¤ä¸ªç»„ä»¶ï¼š\n",
    "# - DistilBertTokenizerFastï¼šDistilBERT æ¨¡å‹çš„å¿«é€Ÿåˆ†è¯å™¨ï¼ˆåŸºäº Rust å®ç°ï¼Œé€Ÿåº¦æ›´å¿«ï¼‰\n",
    "# - DistilBertForTokenClassificationï¼šç”¨äºâ€œæ ‡æ³¨ä»»åŠ¡ï¼ˆToken Classificationï¼‰â€çš„ DistilBERT æ¨¡å‹ï¼Œ\n",
    "#   æ¯”å¦‚å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€è¯æ€§æ ‡æ³¨ï¼ˆPOS taggingï¼‰ç­‰\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification\n",
    "\n",
    "# json ç”¨äºè¯»å–æˆ–è§£æ JSON æ ¼å¼çš„æ•°æ®æ–‡ä»¶ï¼ˆå¸¸ç”¨äºæ ‡æ³¨æ•°æ®é›†ï¼‰\n",
    "import json\n",
    "\n",
    "# random æ˜¯ Python å†…ç½®çš„éšæœºæ•°æ¨¡å—ï¼Œå¯ç”¨äºéšæœºæ‰“ä¹±æ•°æ®ã€ç”Ÿæˆéšæœºç´¢å¼•ç­‰\n",
    "import random\n",
    "\n",
    "# logging æ¨¡å—ç”¨äºè®°å½•è®­ç»ƒæ—¥å¿—ã€è°ƒè¯•ä¿¡æ¯ã€è¿è¡ŒçŠ¶æ€ç­‰\n",
    "import logging\n",
    "\n",
    "# re æ¨¡å—æ˜¯ Python çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆRegular Expressionï¼‰å·¥å…·ï¼Œ\n",
    "# å¸¸ç”¨äºæ–‡æœ¬æ•°æ®çš„é¢„å¤„ç†ï¼Œå¦‚æ¸…æ´—æ ‡ç‚¹ç¬¦å·ã€æå–æ ‡ç­¾ç­‰\n",
    "import re\n",
    "\n",
    "# tqdm æ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„è¿›åº¦æ¡å·¥å…·ï¼Œ\n",
    "# å¯ä»¥åœ¨è®­ç»ƒã€æ•°æ®åŠ è½½ç­‰å¾ªç¯æ“ä½œä¸­ç›´è§‚æ˜¾ç¤ºè¿›åº¦å’Œå‰©ä½™æ—¶é—´\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pad_sequence æ˜¯ PyTorch çš„ä¸€ä¸ªå®ç”¨å‡½æ•°ï¼Œ\n",
    "# ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—ï¼ˆä¾‹å¦‚å¥å­ token åºåˆ—ï¼‰å¡«å……ï¼ˆpaddingï¼‰ä¸ºç›¸åŒé•¿åº¦ï¼Œ\n",
    "# ä»¥ä¾¿æ‰¹é‡è¾“å…¥æ¨¡å‹è¿›è¡Œè®­ç»ƒæˆ–æ¨ç†\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - ç”¨å‘½åå®ä½“è¯†åˆ«å¤„ç†ç®€å†\n",
    "\n",
    "é¢å¯¹å¤§é‡éç»“æ„åŒ–æ–‡æœ¬æ•°æ®æ—¶ï¼Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å¯ä»¥å¸®åŠ©ä½ æ£€æµ‹å¹¶åˆ†ç±»æ•°æ®é›†ä¸­é‡è¦çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¤ºä¾‹å¥å­ â€œJane visits Africa in Septemberâ€ ä¸­ï¼ŒNER å¯ä»¥å¸®åŠ©ä½ è¯†åˆ« â€œJaneâ€ã€â€œAfricaâ€ å’Œ â€œSeptemberâ€ ä¸ºå‘½åå®ä½“ï¼Œå¹¶å°†å®ƒä»¬åˆ†åˆ«åˆ†ç±»ä¸ºäººåï¼ˆpersonï¼‰ã€åœ°ç‚¹ï¼ˆlocationï¼‰å’Œæ—¶é—´ï¼ˆtimeï¼‰ã€‚\n",
    "\n",
    "* ä½ å°†ä½¿ç”¨ä¸Šä¸€æ¬¡ä½œä¸šä¸­æ„å»ºçš„ Transformer æ¨¡å‹çš„ä¸€ä¸ªå˜ä½“æ¥å¤„ç†å¤§é‡ç®€å†æ•°æ®é›†ã€‚\n",
    "* ä½ å°†æŸ¥æ‰¾å¹¶åˆ†ç±»ç›¸å…³ä¿¡æ¯ï¼Œä¾‹å¦‚ç”³è¯·è€…æ›¾å·¥ä½œçš„å…¬å¸ã€æŠ€èƒ½ã€å­¦ä½ç±»å‹ç­‰ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - æ•°æ®é›†æ¸…æ´—\n",
    "\n",
    "åœ¨æœ¬æ¬¡ä½œä¸šä¸­ï¼Œä½ å°†å¯¹ä¸€ä¸ªç®€å†æ•°æ®é›†ä¸Šçš„ Transformer æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚å…ˆçœ‹çœ‹ä½ å°†è¦å¤„ç†çš„æ•°æ®æ˜¯å¦‚ä½•ç»„ç»‡çš„ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# è¯»å–å¹¶é¢„å¤„ç†åŸå§‹ JSON æ•°æ®\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    # å°è¯•ä½¿ç”¨ UTF-8 ç¼–ç æ–¹å¼è¯»å– JSON æ–‡ä»¶ã€‚\n",
    "    # å‚æ•°è¯´æ˜ï¼š\n",
    "    # - \"ner.json\"ï¼šè¦è¯»å–çš„æ–‡ä»¶å\n",
    "    # - lines=Trueï¼šè¡¨ç¤º JSON æ–‡ä»¶æ˜¯æŒ‰è¡Œå­˜å‚¨çš„ï¼ˆæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼‰\n",
    "    # - encoding=\"utf-8\"ï¼šæŒ‡å®šæ–‡ä»¶çš„ç¼–ç æ ¼å¼ä¸º UTF-8\n",
    "    df_data = pd.read_json(\"ner.json\", lines=True, encoding=\"utf-8\")\n",
    "\n",
    "except UnicodeDecodeError:\n",
    "    # å¦‚æœæ–‡ä»¶ç¼–ç ä¸ UTF-8 ä¸åŒ¹é…ï¼ˆä¾‹å¦‚å¸¦æœ‰ BOM å¤´ï¼‰ï¼Œåˆ™å°è¯•ä½¿ç”¨ \"utf-8-sig\" é‡æ–°è¯»å–ã€‚\n",
    "    # è¿™ç§æƒ…å†µåœ¨ Windows ç³»ç»Ÿæˆ–éƒ¨åˆ†ä¸­æ–‡æ–‡ä»¶ä¸­è¾ƒå¸¸è§ã€‚\n",
    "    df_data = pd.read_json(\"ner.json\", lines=True, encoding=\"utf-8-sig\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# åˆ é™¤æ— å…³åˆ—\n",
    "# -------------------------------------------------------------\n",
    "# æœ‰äº› JSON æ–‡ä»¶ä¸­å¯èƒ½åŒ…å«å¤šä½™çš„ä¿¡æ¯ï¼ˆå¦‚ â€œextrasâ€ å­—æ®µï¼‰ï¼Œ\n",
    "# è¿™äº›æ•°æ®å¯¹äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡æ— ç”¨ï¼Œå› æ­¤æˆ‘ä»¬ç›´æ¥åˆ é™¤è¯¥åˆ—ã€‚\n",
    "# å‚æ•°è¯´æ˜ï¼š\n",
    "# - ['extras']ï¼šè¦åˆ é™¤çš„åˆ—å\n",
    "# - axis=1ï¼šè¡¨ç¤ºåˆ é™¤åˆ—ï¼ˆaxis=0 è¡¨ç¤ºåˆ é™¤è¡Œï¼‰\n",
    "# - errors='ignore'ï¼šå¦‚æœè¯¥åˆ—ä¸å­˜åœ¨ï¼Œåˆ™å¿½ç•¥é”™è¯¯è€Œä¸æ˜¯æŠ¥é”™\n",
    "df_data = df_data.drop(['extras'], axis=1, errors='ignore')\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# æ¸…æ´—æ–‡æœ¬å†…å®¹\n",
    "# -------------------------------------------------------------\n",
    "# éƒ¨åˆ†æ ·æœ¬çš„ content å­—æ®µå¯èƒ½åŒ…å«æ¢è¡Œç¬¦ \"\\n\"ï¼Œå½±å“åç»­åˆ†è¯å¤„ç†ã€‚\n",
    "# ä½¿ç”¨ str.replace() å°†å…¶æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä»¥ç¡®ä¿å¥å­ç»“æ„å®Œæ•´ã€‚\n",
    "df_data['content'] = df_data['content'].str.replace(\"\\n\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Abhishek Jha Application Development Associate...   \n",
      "1  Afreen Jamadar Active member of IIIT Committee...   \n",
      "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
      "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
      "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
      "\n",
      "                                          annotation  \n",
      "0  [{'label': ['Skills'], 'points': [{'start': 12...  \n",
      "1  [{'label': ['Email Address'], 'points': [{'sta...  \n",
      "2  [{'label': ['Skills'], 'points': [{'start': 37...  \n",
      "3  [{'label': ['Skills'], 'points': [{'start': 80...  \n",
      "4  [{'label': ['Degree'], 'points': [{'start': 20...  \n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# æ‰“å°æ•°æ®é¢„è§ˆ\n",
    "# -------------------------------------------------------------\n",
    "# ä½¿ç”¨ head() æ‰“å°å‰5æ¡è®°å½•ï¼Œä¾¿äºæ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®è¯»å–ä¸æ¸…æ´—ã€‚\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# å®šä¹‰åˆå¹¶åŒºé—´ï¼ˆmerge intervalsï¼‰å‡½æ•°\n",
    "# -------------------------------------------------------------\n",
    "# åŠŸèƒ½è¯´æ˜ï¼š\n",
    "# è¯¥å‡½æ•°ç”¨äºåˆå¹¶å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ ‡æ³¨ä¸­é‡å æˆ–ç›¸é‚»çš„å®ä½“åŒºé—´ã€‚\n",
    "# å®ä½“åŒºé—´æ ¼å¼ä¸º (start_index, end_index, label)ï¼Œ\n",
    "# ä¾‹å¦‚ (10, 15, \"ORG\") è¡¨ç¤ºåœ¨æ–‡æœ¬ç¬¬ 10~15 ä¸ªå­—ç¬¦ä¹‹é—´å­˜åœ¨ä¸€ä¸ªâ€œç»„ç»‡åâ€å®ä½“ã€‚\n",
    "# \n",
    "# é€šè¿‡æœ¬å‡½æ•°ï¼Œå¯ä»¥ï¼š\n",
    "# - æŒ‰èµ·å§‹ä½ç½®å¯¹åŒºé—´æ’åºï¼›\n",
    "# - åˆå¹¶é‡å æˆ–ç›¸é‚»çš„åŒºé—´ï¼›\n",
    "# - ç¡®ä¿åŒä¸€å®ä½“æ ‡ç­¾(label)çš„è¿ç»­åŒºé—´åˆå¹¶ä¸ºä¸€ä¸ªæ›´å¤§çš„æ•´ä½“ã€‚\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def mergeIntervals(intervals):\n",
    "    \"\"\"\n",
    "    å‚æ•°ï¼š\n",
    "    intervals : list\n",
    "        å«å¤šä¸ªåŒºé—´çš„åˆ—è¡¨ï¼Œæ¯ä¸ªåŒºé—´å½¢å¦‚ (start, end, label)\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "    merged : list\n",
    "        åˆå¹¶åçš„åŒºé—´åˆ—è¡¨ï¼Œå»é™¤é‡å ã€ç›¸é‚»æˆ–é‡å¤éƒ¨åˆ†ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # â‘  å…ˆæŒ‰æ¯ä¸ªåŒºé—´çš„èµ·å§‹ä¸‹æ ‡è¿›è¡Œæ’åº\n",
    "    # -------------------------------------------------------------\n",
    "    # key=lambda tup: tup[0] è¡¨ç¤ºæŒ‰åŒºé—´çš„èµ·ç‚¹ï¼ˆç¬¬1ä¸ªå…ƒç´ ï¼‰å‡åºæ’åˆ—\n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "\n",
    "    # åˆå§‹åŒ–åˆå¹¶ç»“æœåˆ—è¡¨\n",
    "    merged = []\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # â‘¡ éå†æ’åºåçš„æ‰€æœ‰åŒºé—´\n",
    "    # -------------------------------------------------------------\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        # å¦‚æœ merged ä¸ºç©ºï¼Œåˆ™ç›´æ¥åŠ å…¥ç¬¬ä¸€ä¸ªåŒºé—´\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            # å–å½“å‰å·²åˆå¹¶åˆ—è¡¨çš„æœ€åä¸€ä¸ªåŒºé—´\n",
    "            lower = merged[-1]\n",
    "\n",
    "            # -------------------------------------------------------------\n",
    "            # â‘¢ åˆ¤æ–­ä¸¤ä¸ªåŒºé—´æ˜¯å¦é‡å ï¼ˆæˆ–ç›¸é‚»ï¼‰\n",
    "            # -------------------------------------------------------------\n",
    "            # å¦‚æœå½“å‰åŒºé—´çš„èµ·ç‚¹ <= ä¸Šä¸€åŒºé—´çš„ç»ˆç‚¹ï¼Œè¯´æ˜æœ‰é‡å \n",
    "            if higher[0] <= lower[1]:\n",
    "                \n",
    "                # -------------------------------------------------------------\n",
    "                # â‘£ è‹¥ä¸¤ä¸ªåŒºé—´çš„æ ‡ç­¾(label)ç›¸åŒï¼Œåˆ™ç›´æ¥åˆå¹¶\n",
    "                # -------------------------------------------------------------\n",
    "                if lower[2] is higher[2]:\n",
    "                    # upper_bound å–ä¸¤è€…ä¸­è¾ƒå¤§çš„ç»ˆç‚¹\n",
    "                    upper_bound = max(lower[1], higher[1])\n",
    "                    # ç”¨æ–°çš„åˆå¹¶åŒºé—´æ›¿æ¢æ—§çš„æœ€åä¸€ä¸ª\n",
    "                    merged[-1] = (lower[0], upper_bound, lower[2])\n",
    "\n",
    "                # -------------------------------------------------------------\n",
    "                # â‘¤ è‹¥æ ‡ç­¾ä¸åŒï¼Œéœ€åˆ¤æ–­å“ªä¸ªåŒºé—´æ›´â€œé•¿â€æˆ–ä¼˜å…ˆä¿ç•™å“ªä¸ª\n",
    "                # -------------------------------------------------------------\n",
    "                else:\n",
    "                    # å¦‚æœå‰ä¸€ä¸ªåŒºé—´å®Œå…¨è¦†ç›–å½“å‰åŒºé—´ï¼Œåˆ™ä¿æŒä¸å˜\n",
    "                    if lower[1] > higher[1]:\n",
    "                        merged[-1] = lower\n",
    "                    # å¦åˆ™ï¼Œä½¿ç”¨åä¸€ä¸ªåŒºé—´çš„ç»ˆç‚¹å’Œæ ‡ç­¾æ›´æ–°\n",
    "                    else:\n",
    "                        merged[-1] = (lower[0], higher[1], higher[2])\n",
    "            \n",
    "            # -------------------------------------------------------------\n",
    "            # â‘¥ è‹¥ä¸¤ä¸ªåŒºé—´æ— é‡å ï¼Œåˆ™ç›´æ¥åŠ å…¥æ–°çš„åŒºé—´\n",
    "            # -------------------------------------------------------------\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # â‘¦ è¿”å›æœ€ç»ˆåˆå¹¶åçš„åŒºé—´åˆ—è¡¨\n",
    "    # -------------------------------------------------------------\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# ä»æ•°æ®é›†ä¸­æå–å®ä½“ä¿¡æ¯ï¼ˆå‘½åå®ä½“è¯†åˆ« NERï¼‰\n",
    "# -------------------------------------------------------------\n",
    "# åŠŸèƒ½è¯´æ˜ï¼š\n",
    "# è¯¥å‡½æ•°ä» DataFrameï¼ˆJSON è½¬æ¢ç»“æœï¼‰ä¸­æå–å‡ºæ¯æ¡æ ·æœ¬çš„å®ä½“æ ‡ç­¾åŠå…¶ä½ç½®åŒºé—´ã€‚\n",
    "# JSON æ ¼å¼é€šå¸¸åŒ…å« annotationï¼ˆæ ‡æ³¨ï¼‰å­—æ®µï¼Œå…¶ä¸­åŒ…æ‹¬æ¯ä¸ªå®ä½“çš„èµ·æ­¢ä½ç½®ä¸ç±»åˆ«ã€‚\n",
    "# \n",
    "# æå–å‡ºçš„å®ä½“å½¢å¦‚ï¼š\n",
    "#   [(10, 15, 'ORG'), (20, 25, 'LOC'), ...]\n",
    "# è¡¨ç¤ºåœ¨æ–‡æœ¬ä¸­ï¼š\n",
    "#   - 10~15å­—ç¬¦æ˜¯â€œç»„ç»‡å(ORG)â€\n",
    "#   - 20~25å­—ç¬¦æ˜¯â€œåœ°å(LOC)â€\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def get_entities(df):\n",
    "    \"\"\"\n",
    "    å‚æ•°ï¼š\n",
    "    df : pandas.DataFrame\n",
    "        ä» ner.json æ–‡ä»¶ä¸­è¯»å–çš„åŸå§‹æ•°æ®ï¼Œæ¯è¡Œå¯¹åº”ä¸€æ¡æ–‡æœ¬æ ·æœ¬ã€‚\n",
    "        å¿…é¡»åŒ…å«åä¸º 'annotation' çš„åˆ—ï¼Œå­˜æ”¾æ¯ä¸ªæ ·æœ¬çš„å®ä½“æ ‡æ³¨ã€‚\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    entities : list\n",
    "        æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„å®ä½“åˆ—è¡¨ã€‚æ¯ä¸ªå…ƒç´ å½¢å¦‚ï¼š\n",
    "        [(start_1, end_1, label_1), (start_2, end_2, label_2), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # åˆå§‹åŒ–ä¸€ä¸ªæ€»åˆ—è¡¨ï¼Œç”¨äºä¿å­˜æ‰€æœ‰æ ·æœ¬çš„å®ä½“ä¿¡æ¯\n",
    "    # -------------------------------------------------------------\n",
    "    entities = []\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # éå†æ•´ä¸ª DataFrame çš„æ¯ä¸€è¡Œï¼ˆå³æ¯æ¡æ–‡æœ¬æ ·æœ¬ï¼‰\n",
    "    # -------------------------------------------------------------\n",
    "    for i in range(len(df)):\n",
    "        # å½“å‰æ ·æœ¬çš„å®ä½“åˆ—è¡¨\n",
    "        entity = []\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # éå†è¯¥æ ·æœ¬çš„æ‰€æœ‰æ ‡æ³¨ annotation\n",
    "        # æ¯ä¸ª annot æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæ ¼å¼å¤§è‡´å¦‚ä¸‹ï¼š\n",
    "        # {'label': ['ORG'], 'points': [{'start': 10, 'end': 15, 'text': 'Google'}]}\n",
    "        # -------------------------------------------------------------\n",
    "        for annot in df['annotation'][i]:\n",
    "            try:\n",
    "                # å–å®ä½“æ ‡ç­¾ï¼ˆå¦‚ 'ORG', 'PER', 'LOC'ï¼‰\n",
    "                ent = annot['label'][0]  # label é€šå¸¸æ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ \n",
    "\n",
    "                # å®ä½“èµ·å§‹ä½ç½®ï¼ˆå­—ç¬¦ä¸‹æ ‡ï¼‰\n",
    "                start = annot['points'][0]['start']\n",
    "\n",
    "                # å®ä½“ç»“æŸä½ç½®ï¼ˆå­—ç¬¦ä¸‹æ ‡ï¼‰+1ï¼ˆå› ä¸º Python å­—ç¬¦ä¸²åˆ‡ç‰‡åŒºé—´æ˜¯å·¦é—­å³å¼€ï¼‰\n",
    "                end = annot['points'][0]['end'] + 1\n",
    "\n",
    "                # å°†è¯¥å®ä½“ä¿¡æ¯ä»¥ (start, end, label) å½¢å¼åŠ å…¥ä¸´æ—¶åˆ—è¡¨\n",
    "                entity.append((start, end, ent))\n",
    "\n",
    "            # è‹¥æ ‡æ³¨ä¸­ç¼ºå°‘å­—æ®µæˆ–æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡è¯¥é¡¹\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # ä½¿ç”¨å‰é¢å®šä¹‰çš„ mergeIntervals() åˆå¹¶é‡å æˆ–ç›¸é‚»å®ä½“åŒºé—´\n",
    "        # -------------------------------------------------------------\n",
    "        entity = mergeIntervals(entity)\n",
    "\n",
    "        # å°†åˆå¹¶åçš„å®ä½“ç»“æœæ·»åŠ è¿›æ€»åˆ—è¡¨\n",
    "        entities.append(entity)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # è¿”å›æ‰€æœ‰æ ·æœ¬çš„å®ä½“ä¿¡æ¯åˆ—è¡¨\n",
    "    # -------------------------------------------------------------\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Abhishek Jha Application Development Associate...   \n",
      "1  Afreen Jamadar Active member of IIIT Committee...   \n",
      "2  Akhil Yadav Polemaina Hyderabad, Telangana - E...   \n",
      "3  Alok Khandai Operational Analyst (SQL DBA) Eng...   \n",
      "4  Ananya Chavan lecturer - oracle tutorials  Mum...   \n",
      "\n",
      "                                          annotation  \\\n",
      "0  [{'label': ['Skills'], 'points': [{'start': 12...   \n",
      "1  [{'label': ['Email Address'], 'points': [{'sta...   \n",
      "2  [{'label': ['Skills'], 'points': [{'start': 37...   \n",
      "3  [{'label': ['Skills'], 'points': [{'start': 80...   \n",
      "4  [{'label': ['Degree'], 'points': [{'start': 20...   \n",
      "\n",
      "                                            entities  \n",
      "0  [(0, 12, Name), (13, 46, Designation), (49, 58...  \n",
      "1  [(0, 14, Name), (62, 68, Location), (104, 148,...  \n",
      "2  [(0, 21, Name), (22, 31, Location), (65, 117, ...  \n",
      "3  [(0, 12, Name), (13, 51, Designation), (54, 60...  \n",
      "4  [(0, 13, Name), (14, 22, Designation), (24, 41...  \n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# è°ƒç”¨å®ä½“æå–å‡½æ•°ï¼Œç”Ÿæˆæ¯æ¡æ ·æœ¬çš„å®ä½“æ ‡ç­¾åˆ—è¡¨\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# ä¸º DataFrame æ–°å¢ä¸€åˆ— 'entities'ï¼Œ\n",
    "# è¯¥åˆ—å†…å®¹ç”± get_entities(df_data) å‡½æ•°è¿”å›ï¼Œ\n",
    "# å³æ¯æ¡æ–‡æœ¬å¯¹åº”çš„å®ä½“ä¿¡æ¯åˆ—è¡¨ [(start, end, label), ...]\n",
    "df_data['entities'] = get_entities(df_data)\n",
    "\n",
    "# æ‰“å°å‰5è¡Œï¼ŒéªŒè¯å®ä½“æå–ç»“æœ\n",
    "print(df_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# å°† Dataturks æ ‡æ³¨ JSON è½¬æ¢ä¸º spaCy å¯è¯†åˆ«æ ¼å¼\n",
    "# -------------------------------------------------------------\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    \"\"\"\n",
    "    å°† Dataturks å¯¼å‡ºçš„ JSON æ–‡ä»¶è½¬æ¢ä¸º spaCy æ‰€éœ€çš„è®­ç»ƒæ•°æ®æ ¼å¼ã€‚\n",
    "    è¾“å‡ºç»“æ„ç±»ä¼¼ï¼š\n",
    "    [\n",
    "        (\"æ–‡æœ¬å†…å®¹\", {\"entities\": [(start, end, label), ...]}),\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜æ”¾æœ€ç»ˆçš„è®­ç»ƒæ•°æ®\n",
    "        training_data = []\n",
    "\n",
    "        # æ‰“å¼€æŒ‡å®šè·¯å¾„çš„ JSON æ–‡ä»¶ï¼Œé€è¡Œè¯»å–\n",
    "        with open(dataturks_JSON_FilePath, 'r', encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # éå†æ¯ä¸€è¡Œ JSON æ•°æ®ï¼ˆDataturks é€šå¸¸æ˜¯è¡Œçº§ JSON æ ¼å¼ï¼‰\n",
    "        for line in lines:\n",
    "            # è§£æ JSON å­—ç¬¦ä¸²ä¸º Python å­—å…¸\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # å–å‡ºæ–‡æœ¬å†…å®¹ï¼Œå¹¶å»æ‰æ¢è¡Œç¬¦\n",
    "            text = data['content'].replace(\"\\n\", \" \")\n",
    "\n",
    "            # åˆå§‹åŒ–å½“å‰æ–‡æœ¬çš„å®ä½“åˆ—è¡¨\n",
    "            entities = []\n",
    "\n",
    "            # è·å–å½“å‰æ ·æœ¬çš„æ ‡æ³¨éƒ¨åˆ†ï¼ˆannotation å­—æ®µï¼‰\n",
    "            data_annotations = data.get('annotation', [])\n",
    "\n",
    "            # å¦‚æœè¯¥æ ·æœ¬å­˜åœ¨æ ‡æ³¨ä¿¡æ¯ï¼Œè¿›å…¥å¤„ç†é€»è¾‘\n",
    "            if data_annotations:\n",
    "                for annotation in data_annotations:\n",
    "                    # å–å‡ºå®ä½“çš„èµ·æ­¢ä½ç½®ä¸æ–‡æœ¬\n",
    "                    point = annotation['points'][0]\n",
    "\n",
    "                    # è·å–æ ‡ç­¾ï¼ˆå¯èƒ½æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰\n",
    "                    labels = annotation['label']\n",
    "\n",
    "                    # è‹¥æ ‡ç­¾ä¸æ˜¯åˆ—è¡¨ï¼Œåˆ™å¼ºåˆ¶è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä¾¿äºç»Ÿä¸€éå†\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    # éå†æ‰€æœ‰æ ‡ç­¾ï¼ˆæœ‰æ—¶ä¸€ä¸ªåŒºé—´å¯¹åº”å¤šä¸ªæ ‡ç­¾ï¼‰\n",
    "                    for label in labels:\n",
    "                        # è·å–å®ä½“çš„èµ·å§‹ä¸ç»“æŸä½ç½®\n",
    "                        point_start = point['start']\n",
    "                        point_end = point['end']\n",
    "\n",
    "                        # å®ä½“å¯¹åº”çš„åŸå§‹æ–‡æœ¬\n",
    "                        point_text = point['text']\n",
    "\n",
    "                        # è®¡ç®—å»é™¤å‰åç©ºæ ¼åçš„å·®å€¼\n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "\n",
    "                        # è‹¥å®ä½“èµ·å§‹å¤„æœ‰ç©ºæ ¼ï¼Œåˆ™å‘å³è°ƒæ•´ start\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start += lstrip_diff\n",
    "\n",
    "                        # è‹¥å®ä½“ç»“å°¾å¤„æœ‰ç©ºæ ¼ï¼Œåˆ™å‘å·¦è°ƒæ•´ end\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end -= rstrip_diff\n",
    "\n",
    "                        # å°†æ¸…ç†åçš„å®ä½“åæ ‡ä¸æ ‡ç­¾åŠ å…¥å®ä½“åˆ—è¡¨\n",
    "                        # æ³¨æ„ï¼šend + 1 æ˜¯ä¸ºäº†åŒ…å«æœ€åä¸€ä¸ªå­—ç¬¦ï¼ˆPython åˆ‡ç‰‡å³å¼€ï¼‰\n",
    "                        entities.append((point_start, point_end + 1, label))\n",
    "\n",
    "            # å°†ä¸€æ¡æ ·æœ¬ï¼ˆæ–‡æœ¬ + å®ä½“ä¿¡æ¯ï¼‰åŠ å…¥æœ€ç»ˆè®­ç»ƒé›†\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "        # è¿”å› spaCy æ‰€éœ€çš„è®­ç»ƒæ•°æ®æ ¼å¼\n",
    "        return training_data\n",
    "\n",
    "    # è‹¥æ–‡ä»¶è¯»å–æˆ–è§£æä¸­å‡ºç°å¼‚å¸¸ï¼Œæ‰“å°æ—¥å¿—å¹¶è¿”å›ç©ºåˆ—è¡¨\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# æ¸…ç†å®ä½“åŒºé—´ï¼Œå»é™¤å¤šä½™ç©ºæ ¼\n",
    "# -------------------------------------------------------------\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"\n",
    "    æ¸…ç†å®ä½“åŒºé—´ä¸­çš„å¤šä½™ç©ºæ ¼ã€‚\n",
    "    ç¡®ä¿æ¯ä¸ªå®ä½“çš„ startã€end å¯¹åº”çš„æ–‡æœ¬éƒ¨åˆ†ä¸åŒ…å«ç©ºæ ¼ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "    - data: å½¢å¦‚ [(\"æ–‡æœ¬\", {\"entities\": [(start, end, label), ...]}), ...] çš„åˆ—è¡¨\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "    - cleaned_data: æ¸…ç†åçš„åŒç»“æ„æ•°æ®\n",
    "    \"\"\"\n",
    "\n",
    "    # ç¼–è¯‘ä¸€ä¸ªç”¨äºåŒ¹é…ç©ºç™½å­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆåŒ…æ‹¬ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ç­‰ï¼‰\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    # åˆå§‹åŒ–æ¸…ç†åçš„æ•°æ®åˆ—è¡¨\n",
    "    cleaned_data = []\n",
    "\n",
    "    # éå†æ¯ä¸€æ¡æ ·æœ¬\n",
    "    for text, annotations in data:\n",
    "        # å–å‡ºå®ä½“åˆ—è¡¨\n",
    "        entities = annotations['entities']\n",
    "\n",
    "        # ç”¨äºå­˜æ”¾æœ¬å¥æœ‰æ•ˆå®ä½“çš„åˆ—è¡¨\n",
    "        valid_entities = []\n",
    "\n",
    "        # éå†æ¯ä¸ªå®ä½“çš„èµ·æ­¢ä½ç½®å’Œæ ‡ç­¾\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "\n",
    "            # ä»å·¦å‘å³ç§»åŠ¨ startï¼Œç›´åˆ°ä¸å†æ˜¯ç©ºæ ¼\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(text[valid_start]):\n",
    "                valid_start += 1\n",
    "\n",
    "            # ä»å³å‘å·¦ç§»åŠ¨ endï¼Œç›´åˆ°ä¸å†æ˜¯ç©ºæ ¼\n",
    "            while valid_end > 1 and invalid_span_tokens.match(text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "\n",
    "            # å°†æ¸…ç†åçš„åæ ‡ä¸æ ‡ç­¾åŠ å…¥æœ‰æ•ˆå®ä½“åˆ—è¡¨\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "\n",
    "        # å°†è¯¥æ ·æœ¬çš„æ–‡æœ¬ä¸æ¸…ç†åçš„å®ä½“ä¿¡æ¯åŠ å…¥æœ€ç»ˆåˆ—è¡¨\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    # è¿”å›å»é™¤ç©ºæ ¼åçš„å®Œæ•´æ•°æ®é›†\n",
    "    return cleaned_data  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# æ•´åˆæ•°æ®æ¸…æ´—æµç¨‹ â€”â€” ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæ•°æ®\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# é€šè¿‡å‡½æ•°åµŒå¥—è°ƒç”¨çš„æ–¹å¼ï¼Œå°† \"ner.json\" æ•°æ®è½¬æ¢å¹¶æ¸…ç†ï¼š\n",
    "# ç¬¬ä¸€æ­¥ï¼šconvert_dataturks_to_spacy(\"ner.json\")\n",
    "#          ä» Dataturks JSON æ–‡ä»¶ä¸­æå–æ–‡æœ¬å’Œå®ä½“ä¿¡æ¯ï¼Œè½¬æ¢ä¸º spaCy æ ¼å¼ã€‚\n",
    "# ç¬¬äºŒæ­¥ï¼štrim_entity_spans(...)\n",
    "#          æ¸…ç†ä¸Šä¸€æ­¥å¾—åˆ°çš„å®ä½“åæ ‡ä¸­çš„ç©ºæ ¼åç§»é—®é¢˜ï¼Œä½¿æ ‡æ³¨åŒºé—´æ›´ç²¾ç¡®ã€‚\n",
    "\n",
    "data = trim_entity_spans(convert_dataturks_to_spacy(\"ner.json\"))\n",
    "\n",
    "# æ‰§è¡Œå®Œæœ¬è¡Œä»£ç åï¼Œå˜é‡ `data` çš„æ ¼å¼ä¸ºï¼š\n",
    "# [\n",
    "#     [\n",
    "#         \"åŸå§‹æ–‡æœ¬å†…å®¹\",\n",
    "#         {\n",
    "#             \"entities\": [\n",
    "#                 [start_index, end_index, \"å®ä½“æ ‡ç­¾\"],\n",
    "#                 [start_index, end_index, \"å®ä½“æ ‡ç­¾\"],\n",
    "#                 ...\n",
    "#             ]\n",
    "#         }\n",
    "#     ],\n",
    "#     ...\n",
    "# ]\n",
    "#\n",
    "# ä¸¾ä¾‹è¯´æ˜ï¼š\n",
    "# data[0] å¯èƒ½ç±»ä¼¼äºï¼š\n",
    "# [\n",
    "#   \"Apple Inc. is looking at buying U.K. startup for $1 billion.\",\n",
    "#   {\"entities\": [[0, 10, \"ORG\"], [27, 31, \"LOC\"], [44, 54, \"MONEY\"]]}\n",
    "# ]\n",
    "#\n",
    "# è¿™æ ·å¾—åˆ°çš„ data å¯ç›´æ¥ç”¨äºåç»­ DataLoaderã€BERT Tokenizerã€\n",
    "#    æˆ– spaCy / HuggingFace çš„ token-classification ä»»åŠ¡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# æ¸…æ´—æ•°æ®é›†ï¼Œå°†å®ä½“æ ‡æ³¨æ˜ å°„åˆ°æ¯ä¸ªå•è¯\n",
    "# -------------------------------------------------------------\n",
    "def clean_dataset(data):\n",
    "    \"\"\"\n",
    "    å‡½æ•°åŠŸèƒ½ï¼š\n",
    "    å°†æ¯æ¡æ ·æœ¬ä¸­çš„æ–‡æœ¬ä¸å…¶å¯¹åº”çš„å®ä½“æ ‡ç­¾è¿›è¡Œå¯¹é½ï¼Œ\n",
    "    ä½¿å¾—æ¯ä¸ªå•è¯éƒ½æœ‰å¯¹åº”çš„æ ‡ç­¾ï¼ˆå¦‚ PERSONã€ORGã€LOC æˆ– Emptyï¼‰ï¼Œ\n",
    "    ä¸ºåç»­å‘½åå®ä½“è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒæ‰“ä¸‹åŸºç¡€ã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "    - data : list\n",
    "        ç»è¿‡ trim_entity_spans() æ¸…æ´—åçš„æ•°æ®ï¼Œ\n",
    "        æ ¼å¼ä¸º [\n",
    "            [text, {\"entities\": [[start, end, label], ...]}],\n",
    "            ...\n",
    "        ]\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    - cleanedDF : pandas.DataFrame\n",
    "        æ¸…æ´—åçš„ DataFrameï¼Œæ¯è¡ŒåŒ…å«ä¸€ä¸ªåˆ—è¡¨ï¼š\n",
    "        åˆ—è¡¨ä¸­æ˜¯å¯¹åº”å¥å­æ¯ä¸ªå•è¯çš„æ ‡ç­¾ã€‚\n",
    "        ä¾‹å¦‚ [\"Empty\", \"ORG\", \"ORG\", \"Empty\", \"LOC\"]\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_rows = []  # å­˜å‚¨æ‰€æœ‰å¥å­çš„æ¸…æ´—ç»“æœ\n",
    "\n",
    "    # tqdm æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œéå†æ¯ä¸€æ¡æ ·æœ¬\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        start = 0  # å½“å‰å•è¯åœ¨å­—ç¬¦ä¸²ä¸­çš„èµ·å§‹å­—ç¬¦ä½ç½®\n",
    "        emptyList = [\"Empty\"] * len(data[i][0].split())  # åˆå§‹åŒ–æ¯ä¸ªå•è¯æ ‡ç­¾ä¸º \"Empty\"\n",
    "        numberOfWords = 0  # å½“å‰å•è¯åºå·è®¡æ•°å™¨\n",
    "        lenOfString = len(data[i][0])  # å½“å‰å¥å­çš„å­—ç¬¦æ€»é•¿åº¦\n",
    "        strData = data[i][0]  # å½“å‰æ ·æœ¬çš„æ–‡æœ¬å†…å®¹\n",
    "        strDictData = data[i][1]  # å½“å‰æ ·æœ¬çš„å®ä½“ä¿¡æ¯å­—å…¸ï¼Œå¦‚ {\"entities\": [[start, end, label]]}\n",
    "        lastIndexOfSpace = strData.rfind(' ')  # æ‰¾å‡ºå¥å­ä¸­æœ€åä¸€ä¸ªç©ºæ ¼çš„ä½ç½®ï¼Œç”¨äºå¤„ç†æœ€åä¸€ä¸ªå•è¯\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # éå†å¥å­çš„æ¯ä¸€ä¸ªå­—ç¬¦ï¼Œæ‰¾åˆ°æ¯ä¸ªå•è¯çš„è¾¹ç•Œ\n",
    "        # ---------------------------------------------------------\n",
    "        for j in range(lenOfString):\n",
    "\n",
    "            # âœ… å½“é‡åˆ°ä¸€ä¸ªâ€œç©ºæ ¼â€ä¸”ä¸‹ä¸€ä¸ªå­—ç¬¦ä¸æ˜¯ç©ºæ ¼æ—¶ï¼Œè¯´æ˜æ˜¯ä¸€ä¸ªå•è¯çš„ç»“æŸä½ç½®\n",
    "            if strData[j] == \" \" and j + 1 < lenOfString and strData[j + 1] != \" \":\n",
    "\n",
    "                # éå†å®ä½“å­—å…¸ä¸­çš„å®ä½“æ ‡æ³¨\n",
    "                for k, v in strDictData.items():\n",
    "                    for m in range(len(v)):\n",
    "                        entList = v[len(v) - m - 1]  # å€’åºå–å‡ºæ¯ä¸ªå®ä½“ [start, end, label]\n",
    "\n",
    "                        # è‹¥å½“å‰å•è¯çš„èµ·å§‹å’Œç»“æŸå­—ç¬¦è½åœ¨å®ä½“åŒºé—´å†…ï¼Œåˆ™ç»™è¯¥å•è¯æ ‡è®°æ ‡ç­¾\n",
    "                        if start >= int(entList[0]) and j <= int(entList[1]):\n",
    "                            emptyList[numberOfWords] = entList[2]\n",
    "                            break\n",
    "\n",
    "                # å•è¯ç»“æŸåï¼Œæ›´æ–°ä¸‹ä¸€ä¸ªå•è¯çš„èµ·å§‹ä½ç½®ä¸è®¡æ•°\n",
    "                start = j + 1\n",
    "                numberOfWords += 1\n",
    "\n",
    "            # -----------------------------------------------------\n",
    "            # ç‰¹æ®Šå¤„ç†ï¼šå¥å­çš„æœ€åä¸€ä¸ªå•è¯ï¼ˆæ²¡æœ‰ç©ºæ ¼ç»“å°¾ï¼‰\n",
    "            # -----------------------------------------------------\n",
    "            if j == lastIndexOfSpace:\n",
    "                for m in range(len(v)):\n",
    "                    entList = v[len(v) - m - 1]\n",
    "                    if lastIndexOfSpace >= int(entList[0]) and lenOfString <= int(entList[1]):\n",
    "                        emptyList[numberOfWords] = entList[2]\n",
    "                        numberOfWords += 1\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # å°†è¯¥å¥å­çš„å•è¯æ ‡ç­¾åˆ—è¡¨æ·»åŠ åˆ°ç»“æœé›†ä¸­\n",
    "        # ---------------------------------------------------------\n",
    "        cleaned_rows.append([emptyList])\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # å°†æ¸…æ´—ç»“æœè½¬æ¢ä¸º DataFrame ç»“æ„ï¼Œåˆ—åä¸º \"setences_cleaned\"\n",
    "    # -------------------------------------------------------------\n",
    "    cleanedDF = pd.DataFrame(cleaned_rows, columns=[\"setences_cleaned\"])\n",
    "\n",
    "    return cleanedDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:00<00:00, 459.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# æ‰§è¡Œæ•°æ®æ¸…æ´—ï¼Œå°†å®ä½“æ ‡ç­¾æ˜ å°„åˆ°æ¯ä¸ªå•è¯\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# è°ƒç”¨ clean_dataset() å‡½æ•°ï¼Œå°†æ¯æ¡æ–‡æœ¬çš„å•è¯ä¸å®ä½“æ ‡ç­¾å¯¹é½\n",
    "# å‚æ•°ï¼š\n",
    "# - data : ç»è¿‡ trim_entity_spans() å’Œ convert_dataturks_to_spacy() æ¸…æ´—åçš„æ•°æ®\n",
    "# è¿”å›ï¼š\n",
    "# - cleanedDF : DataFrameï¼Œæ¯è¡ŒåŒ…å«ä¸€ä¸ªåˆ—è¡¨ï¼Œå¯¹åº”å¥å­ä¸­æ¯ä¸ªå•è¯çš„æ ‡ç­¾\n",
    "cleanedDF = clean_dataset(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŸ¥çœ‹ä½ çš„æ¸…æ´—åæ•°æ®é›†ï¼Œä»¥åŠå‘½åå®ä½“æ‰€å¯¹åº”çš„ç±»åˆ«æˆ–â€œæ ‡ç­¾â€ï¼ˆtagsï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    setences_cleaned\n",
      "0  [Name, Name, Designation, Designation, Designa...\n",
      "1  [Name, Name, Empty, Empty, Empty, Empty, Empty...\n",
      "2  [Name, Name, Name, Empty, Empty, Empty, Empty,...\n",
      "3  [Name, Name, Designation, Designation, Designa...\n",
      "4  [Name, Name, Designation, Empty, Companies wor...\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹æ¸…æ´—åçš„å‰å‡ æ¡æ•°æ®\n",
    "print(cleanedDF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - å¡«å……ä¸æ ‡ç­¾ç”Ÿæˆ\n",
    "\n",
    "ç°åœ¨ï¼Œæ˜¯æ—¶å€™ç”Ÿæˆä¸€ä¸ªå”¯ä¸€æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨äºä¸å‘½åå®ä½“è¿›è¡ŒåŒ¹é…ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# æ ‡ç­¾ç¼–ç ï¼ˆå°†æ–‡æœ¬æ ‡ç­¾æ˜ å°„ä¸ºæ•°å­— IDï¼‰\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# 1ï¸ æå–æ‰€æœ‰å”¯ä¸€æ ‡ç­¾\n",
    "# cleanedDF['setences_cleaned'] æ˜¯æ¯è¡Œä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨å†…æ˜¯æ¯ä¸ªå•è¯çš„å®ä½“æ ‡ç­¾\n",
    "# explode() ä¼šå°†åˆ—è¡¨å±•å¼€ä¸ºå¤šè¡Œï¼Œunique() æå–æ‰€æœ‰ä¸åŒæ ‡ç­¾ï¼Œset() è½¬ä¸ºé›†åˆ\n",
    "unique_tags = set(cleanedDF['setences_cleaned'].explode().unique())\n",
    "\n",
    "# 2ï¸ æ„å»ºæ ‡ç­¾åˆ°æ•°å­— ID çš„æ˜ å°„å­—å…¸\n",
    "# key: æ ‡ç­¾åç§°ï¼Œvalue: æ ‡ç­¾å¯¹åº”çš„æ•´æ•° ID\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "\n",
    "# 3ï¸ æ„å»ºæ•°å­— ID åˆ°æ ‡ç­¾çš„åå‘æ˜ å°„å­—å…¸\n",
    "# key: æ•°å­— IDï¼Œvalue: æ ‡ç­¾åç§°\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ‡ç­¾åˆ° ID æ˜ å°„ç¤ºä¾‹ï¼š {'UNKNOWN': 0, 'Email Address': 1, 'Name': 2, 'Skills': 3, 'Companies worked at': 4}\n",
      "ID åˆ°æ ‡ç­¾æ˜ å°„ç¤ºä¾‹ï¼š {0: 'UNKNOWN', 1: 'Email Address', 2: 'Name', 3: 'Skills', 4: 'Companies worked at'}\n"
     ]
    }
   ],
   "source": [
    "# æ‰“å°æ˜ å°„å…³ç³»ç¤ºä¾‹\n",
    "print(\"æ ‡ç­¾åˆ° ID æ˜ å°„ç¤ºä¾‹ï¼š\", dict(list(tag2id.items())[:5]))\n",
    "print(\"ID åˆ°æ ‡ç­¾æ˜ å°„ç¤ºä¾‹ï¼š\", dict(list(id2tag.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNKNOWN', 'Email Address', 'Name', 'Skills', 'Companies worked at', 'Graduation Year', 'Location', 'Degree', 'College Name', 'Designation', 'Empty', 'Years of Experience'}\n"
     ]
    }
   ],
   "source": [
    "# æ‰“å°æ‰€æœ‰å”¯ä¸€æ ‡ç­¾\n",
    "# unique_tags æ˜¯å‰é¢ä» cleanedDF['setences_cleaned'] æå–çš„æ‰€æœ‰ä¸åŒå®ä½“æ ‡ç­¾é›†åˆ\n",
    "# æ‰“å°å‡ºæ¥å¯ä»¥ç›´è§‚æŸ¥çœ‹æœ‰å“ªäº›æ ‡ç­¾\n",
    "print(unique_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œä½ å°†ä»æ¸…æ´—åçš„æ•°æ®é›†ä¸­åˆ›å»ºä¸€ä¸ªæ ‡ç­¾æ•°ç»„ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œä½ çš„è¾“å…¥åºåˆ—å¯èƒ½ä¼šè¶…è¿‡ç½‘ç»œå¯ä»¥å¤„ç†çš„æœ€å¤§é•¿åº¦ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåºåˆ—ä¼šè¢«æˆªæ–­ï¼Œä½ éœ€è¦ä½¿ç”¨ [Keras padding API](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) åœ¨æˆªæ–­åºåˆ—çš„æœ«å°¾è¡¥é›¶ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# æ ‡ç­¾å¡«å……ï¼ˆPaddingï¼‰å¤„ç†\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# å®šä¹‰åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆTransformer æ¨¡å‹é€šå¸¸å›ºå®šè¾“å…¥é•¿åº¦ï¼‰\n",
    "MAX_LEN = 512  \n",
    "\n",
    "# è·å–æ‰€æœ‰æ¸…æ´—åçš„æ ‡ç­¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå¥å­å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨\n",
    "# cleanedDF['setences_cleaned'].values.tolist() è¿”å›ä¸€ä¸ªäºŒç»´åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨æ˜¯å¥å­çš„æ ‡ç­¾åºåˆ—\n",
    "labels = cleanedDF['setences_cleaned'].values.tolist()  \n",
    "\n",
    "# å°†æ¯ä¸ªæ ‡ç­¾åˆ—è¡¨è½¬æ¢ä¸ºå¯¹åº”çš„æ•°å­— IDï¼ˆtensorï¼‰\n",
    "# torch.tensor([tag2id.get(l) for l in lab])ï¼š\n",
    "#   - lab: å½“å‰å¥å­çš„æ ‡ç­¾åˆ—è¡¨\n",
    "#   - tag2id.get(l): æ ¹æ®æ ‡ç­¾åè·å–å¯¹åº”çš„æ•°å­— ID\n",
    "tags = [torch.tensor([tag2id.get(l) for l in lab]) for lab in labels]  \n",
    "\n",
    "# å¯¹ä¸åŒå¥å­çš„æ ‡ç­¾åºåˆ—è¿›è¡Œé•¿åº¦ç»Ÿä¸€ï¼Œä½¿ç”¨ pad_sequence\n",
    "# pad_sequence(tags, batch_first=True, padding_value=tag2id[\"Empty\"])ï¼š\n",
    "#   - batch_first=True: è¾“å‡ºå½¢çŠ¶ä¸º (batch_size, seq_len)\n",
    "#   - padding_value=tag2id[\"Empty\"]: ç”¨ \"Empty\" å¯¹åº”çš„ ID å¡«å……çŸ­åºåˆ—\n",
    "tags = pad_sequence(tags, batch_first=True, padding_value=tag2id[\"Empty\"])  \n",
    "\n",
    "# å°†æ‰€æœ‰åºåˆ—æ‰©å±•åˆ° MAX_LENï¼Œä¿è¯æ¯ä¸ªåºåˆ—é•¿åº¦ä¸€è‡´\n",
    "# torch.nn.functional.pad(tags, (0, MAX_LEN - tags.size(1)), value=tag2id[\"Empty\"])ï¼š\n",
    "#   - (0, MAX_LEN - tags.size(1)) è¡¨ç¤ºåœ¨åºåˆ—æœ«å°¾å¡«å…… (MAX_LEN - å½“å‰é•¿åº¦) ä¸ªä½ç½®\n",
    "#   - value=tag2id[\"Empty\"]: å¡«å……å€¼ä»ä¸º \"Empty\" æ ‡ç­¾çš„ ID\n",
    "tags = torch.nn.functional.pad(tags, (0, MAX_LEN - tags.size(1)), value=tag2id[\"Empty\"])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2,  9,  ..., 10, 10, 10],\n",
       "        [ 2,  2, 10,  ..., 10, 10, 10],\n",
       "        [ 2,  2,  2,  ..., 10,  3, 10],\n",
       "        ...,\n",
       "        [ 2,  2,  9,  ..., 10, 10, 10],\n",
       "        [ 2,  2,  9,  ..., 10, 10, 10],\n",
       "        [ 2,  2,  9,  ..., 10, 10, 10]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - ä½¿ç”¨ ğŸ¤— åº“è¿›è¡Œåˆ†è¯å¹¶å¯¹é½æ ‡ç­¾\n",
    "\n",
    "åœ¨å°†æ–‡æœ¬è¾“å…¥ Transformer æ¨¡å‹ä¹‹å‰ï¼Œä½ éœ€è¦ä½¿ç”¨ [ğŸ¤— Transformer åˆ†è¯å™¨](https://huggingface.co/transformers/main_classes/tokenizer.html) å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯ã€‚éå¸¸é‡è¦çš„ä¸€ç‚¹æ˜¯ï¼Œä½ ä½¿ç”¨çš„åˆ†è¯å™¨å¿…é¡»ä¸æ‰€ç”¨çš„ Transformer æ¨¡å‹ç±»å‹åŒ¹é…ï¼åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†ä½¿ç”¨ ğŸ¤— [DistilBERT å¿«é€Ÿåˆ†è¯å™¨ï¼ˆfast tokenizerï¼‰](https://huggingface.co/transformers/model_doc/distilbert.html)ï¼Œå®ƒä¼šå°†åºåˆ—é•¿åº¦æ ‡å‡†åŒ–ä¸º 512ï¼Œå¹¶ä½¿ç”¨é›¶è¿›è¡Œå¡«å……ã€‚æ³¨æ„ï¼Œè¿™ä¸åœ¨åˆ›å»ºæ ‡ç­¾æ—¶ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ä¸€è‡´ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Tokenizationï¼ˆä½¿ç”¨ HuggingFace å®˜æ–¹æä¾›çš„ DistilBertTokenizerFastï¼‰\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†è¯å™¨\n",
    "# pretrained_model_name_or_path='distilbert-base-cased'ï¼š\n",
    "#    ä½¿ç”¨é¢„è®­ç»ƒçš„ DistilBERT æ¨¡å‹ï¼ˆåŒºåˆ†å¤§å°å†™çš„ç‰ˆæœ¬ï¼‰\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# tokenizer åŠŸèƒ½ï¼š\n",
    "# 1. å°†æ–‡æœ¬å¥å­æ‹†åˆ†æˆ tokenï¼ˆå•è¯æˆ–å­è¯ï¼‰\n",
    "# 2. è½¬æ¢ä¸ºå¯¹åº”çš„ token_idï¼ˆæ•°å­—ç´¢å¼•ï¼‰\n",
    "# 3. å¯ä»¥ä¸€æ¬¡æ€§æ‰¹é‡å¤„ç†å¤šä¸ªå¥å­\n",
    "# 4. æ”¯æŒ padding å’Œ truncationï¼Œä»¥ä¿è¯æ‰€æœ‰è¾“å…¥åºåˆ—é•¿åº¦ç›¸åŒ\n",
    "# 5. è¾“å‡ºå¯ä»¥ç›´æ¥ç”¨äº PyTorch æ¨¡å‹ï¼ˆè¿”å› tensorsï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer æ¨¡å‹é€šå¸¸ä½¿ç”¨å¯ä»¥å°†å•è¯æ‹†åˆ†ä¸ºå­è¯ï¼ˆsubwordsï¼‰çš„åˆ†è¯å™¨è¿›è¡Œè®­ç»ƒã€‚ä¾‹å¦‚ï¼Œå•è¯ \"Africa\" å¯èƒ½è¢«æ‹†åˆ†ä¸ºå¤šä¸ªå­è¯ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´æ•°æ®é›†æ ‡ç­¾åˆ—è¡¨ä¸åˆ†è¯å™¨ç”Ÿæˆçš„æ ‡ç­¾åˆ—è¡¨ä¹‹é—´å‡ºç°é”™ä½ï¼Œå› ä¸ºåˆ†è¯å™¨å¯èƒ½ä¼šå°†ä¸€ä¸ªå•è¯æ‹†åˆ†æˆå¤šä¸ªï¼Œæˆ–æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚åœ¨å¤„ç†ä¹‹å‰ï¼Œé‡è¦çš„æ˜¯ä½¿ç”¨ `tokenize_and_align_labels()` å‡½æ•°å°†æ ‡ç­¾åˆ—è¡¨ä¸åˆ†è¯å™¨ç”Ÿæˆçš„æ ‡ç­¾å¯¹é½ã€‚\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### ç»ƒä¹  1 - tokenize_and_align_labels\n",
    "\n",
    "å®ç° `tokenize_and_align_labels()` å‡½æ•°ã€‚è¯¥å‡½æ•°åº”æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n",
    "* ä½¿ç”¨å‚æ•° `truncation=True` æˆªæ–­è¶…è¿‡æ¨¡å‹å…è®¸çš„æœ€å¤§é•¿åº¦çš„åºåˆ—ã€‚\n",
    "* ä½¿ç”¨åˆ†è¯å™¨çš„ `word_ids` æ–¹æ³•å¯¹æ ‡ç­¾åˆ—è¡¨ä¸å­è¯è¿›è¡Œå¯¹é½ï¼Œè¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå°†å­è¯æ˜ å°„åˆ°å¥å­ä¸­çš„åŸå§‹å•è¯ï¼Œç‰¹æ®Šæ ‡è®°æ˜ å°„ä¸º `None`ã€‚\n",
    "* å°†æ‰€æœ‰ç‰¹æ®Šæ ‡è®° (`None`) çš„æ ‡ç­¾è®¾ç½®ä¸º -100ï¼Œä»¥é˜²æ­¢å®ƒä»¬å½±å“æŸå¤±å‡½æ•°ã€‚\n",
    "* è®¾ç½®å•è¯ç¬¬ä¸€ä¸ªå­è¯çš„æ ‡ç­¾ï¼Œå…¶åçš„å­è¯æ ‡ç­¾è®¾ç½®ä¸º -100ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Tokenization + æ ‡ç­¾å¯¹é½å‡½æ•°\n",
    "# -------------------------------------------------------------\n",
    "label_all_tokens = True  # æ˜¯å¦å¯¹åˆ†è¯åçš„å­è¯éƒ½æ ‡æ³¨åŒæ ·æ ‡ç­¾ï¼ˆTrue: å¯¹æ‰€æœ‰å­è¯æ ‡æ³¨ï¼ŒFalse: åªæ ‡æ³¨ç¬¬ä¸€ä¸ªå­è¯ï¼‰\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, examples, tags):\n",
    "    \"\"\"\n",
    "    å°†æ–‡æœ¬åˆ†è¯å¹¶å¯¹é½å®ä½“æ ‡ç­¾ï¼Œä½¿å¾—æ¯ä¸ª token éƒ½æœ‰å¯¹åº”çš„æ ‡ç­¾ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "    - tokenizer: å·²åˆå§‹åŒ–çš„ DistilBertTokenizerFast\n",
    "    - examples: æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€æ¡å¥å­\n",
    "    - tags: æ ‡ç­¾ tensor åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå¥å­çš„æ ‡ç­¾ id åºåˆ—\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "    - tokenized_inputs: dictï¼ŒåŒ…å« token ids, attention mask å’Œå¯¹é½åçš„ labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯\n",
    "    # truncation=True: è¶…è¿‡ max_length æˆªæ–­\n",
    "    # padding='max_length': è¡¥é½åˆ° max_length\n",
    "    # max_length=512: å›ºå®šæœ€å¤§é•¿åº¦\n",
    "    # is_split_into_words=False: å¥å­æœªæ‹†åˆ†æˆå•è¯\n",
    "    tokenized_inputs = tokenizer(examples, \n",
    "                                 truncation=True, \n",
    "                                 padding='max_length', \n",
    "                                 max_length=512, \n",
    "                                 is_split_into_words=False)\n",
    "    \n",
    "    # Step 2: å¯¹é½æ ‡ç­¾\n",
    "    labels_aligned = []  # å­˜å‚¨æ¯æ¡å¥å­çš„ token label\n",
    "    for i, label in enumerate(tags):\n",
    "        # è·å–æ¯ä¸ª token å¯¹åº”çš„åŸå§‹å•è¯ç´¢å¼•\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []  # å½“å‰å¥å­çš„ token label\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # special token ([CLS], [SEP], padding) è®¾ç½®ä¸º -100ï¼Œé¿å…å‚ä¸ loss è®¡ç®—\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # å½“å‰ token å¯¹åº”æ–°çš„å•è¯ï¼Œä½¿ç”¨è¯¥å•è¯çš„æ ‡ç­¾\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # å½“å‰ token æ˜¯å‰ä¸€ä¸ªå•è¯çš„å­è¯\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels_aligned.append(label_ids)\n",
    "    \n",
    "    # Step 3: å°†å¯¹é½å¥½çš„ labels æ·»åŠ åˆ° tokenized_inputs ä¸­\n",
    "    tokenized_inputs[\"labels\"] = labels_aligned\n",
    "    \n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œä½ å·²ç»å®Œæˆäº†è¾“å…¥çš„åˆ†è¯ï¼Œå¯ä»¥åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†äº†ï¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# å°†åŸå§‹æ–‡æœ¬å’Œæ ‡ç­¾è¿›è¡Œåˆ†è¯å¹¶å¯¹é½\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# è°ƒç”¨ tokenize_and_align_labels å‡½æ•°\n",
    "# å‚æ•°è¯´æ˜ï¼š\n",
    "# - tokenizer: DistilBertTokenizerFast å¯¹è±¡\n",
    "# - df_data['content'].values.tolist(): æ‰€æœ‰æ–‡æœ¬å¥å­åˆ—è¡¨\n",
    "# - tags: å¯¹åº”çš„æ ‡ç­¾ tensorï¼Œå·²ç» pad å’Œç¼–ç \n",
    "test = tokenize_and_align_labels(tokenizer, df_data['content'].values.tolist(), tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input keys: ['input_ids', 'attention_mask', 'labels']\n",
      "ç¬¬1æ¡å¥å­çš„ input_ids å‰ 20 ä¸ª: [101, 138, 1830, 27516, 4638, 1377, 147, 2328, 22491, 3273, 9666, 118, 138, 19515, 3452, 3313, 7756, 12328, 117, 12247]\n",
      "ç¬¬1æ¡å¥å­çš„ labels å‰ 20 ä¸ª: [-100, tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(9), tensor(9), tensor(9), tensor(10), tensor(10), tensor(10), tensor(10), tensor(10), tensor(10), tensor(10), tensor(10), tensor(10)]\n"
     ]
    }
   ],
   "source": [
    "# è¾“å‡ºå­—å…¸ä¸­çš„ key\n",
    "print(\"Tokenized input keys:\", list(test.keys()))  # æ˜¾ç¤º keys åˆ—è¡¨ï¼Œè€Œä¸æ˜¯å†…å®¹\n",
    "\n",
    "# å¦‚æœè¦æŸ¥çœ‹æŸæ¡å¥å­çš„ input_ids å’Œ labels çš„å‰ 20 ä¸ª token\n",
    "print(\"ç¬¬1æ¡å¥å­çš„ input_ids å‰ 20 ä¸ª:\", test[\"input_ids\"][0][:20])\n",
    "print(\"ç¬¬1æ¡å¥å­çš„ labels å‰ 20 ä¸ª:\", test[\"labels\"][0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# PyTorch Dataset å®šä¹‰\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    è‡ªå®šä¹‰ PyTorch æ•°æ®é›†ç±»ï¼Œç”¨äº Named-Entity Recognition ä»»åŠ¡\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‡½æ•°\n",
    "        å‚æ•°ï¼š\n",
    "        - encodings: tokenizer è¾“å‡ºçš„å­—å…¸ï¼ŒåŒ…æ‹¬ input_ids, attention_mask, labels ç­‰\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        è¿”å›æ•°æ®é›†å¤§å°\n",
    "        \"\"\"\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        è·å–ç´¢å¼• idx å¯¹åº”çš„æ•°æ®é¡¹ï¼Œå¹¶è½¬æ¢ä¸º Tensor\n",
    "        \"\"\"\n",
    "        # å¯¹ encodings å­—å…¸ä¸­çš„æ¯ä¸ª keyï¼Œå°†å¯¹åº”çš„ç¬¬ idx ä¸ªå…ƒç´ è½¬æ¢ä¸º tensor\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒæ•°æ®é›†\n",
    "train_dataset = NERDataset(test)\n",
    "\n",
    "# åˆ›å»º DataLoaderï¼Œç”¨äºæ‰¹é‡è®­ç»ƒ\n",
    "# batch_size=16: æ¯ä¸ª batch 16 æ¡æ•°æ®\n",
    "# shuffle=True: æ¯è½®è®­ç»ƒæ‰“ä¹±é¡ºåº\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(1),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(10),\n",
       " tensor(4),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(10),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(8),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " tensor(10),\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æŸ¥çœ‹ç¬¬ 0 ä¸ªæ ·æœ¬çš„æ ‡ç­¾åºåˆ—ï¼ˆtoken å¯¹åº”çš„æ ‡ç­¾ IDï¼‰\n",
    "# æ³¨æ„ï¼šBERT/DistilBERT tokenization ä¼šå°†ä¸€ä¸ªå•è¯æ‹†åˆ†æˆå¤šä¸ª tokenï¼Œå› æ­¤åŒä¸€ä¸ªå•è¯çš„æ ‡ç­¾å¯èƒ½é‡å¤æˆ–æ ‡è®°ä¸º -100\n",
    "test['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-4'></a>\n",
    "### 1.4 - ä¼˜åŒ–\n",
    "\n",
    "å¤ªæ£’äº†ï¼ç°åœ¨ä½ ç»ˆäºå¯ä»¥å°†æ•°æ®è¾“å…¥åˆ°é¢„è®­ç»ƒçš„ ğŸ¤— æ¨¡å‹ä¸­ã€‚ä½ å°†ä¼˜åŒ–ä¸€ä¸ª DistilBERT æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ä¹‹å‰ç”¨äºé¢„å¤„ç†æ•°æ®çš„åˆ†è¯å™¨åŒ¹é…ã€‚å°è¯•è°ƒæ•´ä¸åŒçš„è¶…å‚æ•°ï¼ˆhyperparametersï¼‰ï¼Œä»¥æå‡ä½ çš„ç»“æœï¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForTokenClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# æ¨¡å‹åŠ è½½ï¼ˆè‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒ DistilBERT æƒé‡ï¼Œç”¨äº Token Classificationï¼‰\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "# å‚æ•°è¯´æ˜ï¼š\n",
    "# - 'distilbert-base-cased': ä½¿ç”¨é¢„è®­ç»ƒçš„ DistilBERTï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰\n",
    "# - num_labels: åˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ•°ï¼Œè¿™é‡Œç­‰äº NER æ ‡ç­¾æ€»æ•°\n",
    "# æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½å¯¹åº”çš„é¢„è®­ç»ƒæƒé‡ï¼Œå¹¶åœ¨æœ€åä¸€å±‚æ·»åŠ  token åˆ†ç±»å™¨\n",
    "model = DistilBertForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-cased',\n",
    "    num_labels=len(unique_tags)\n",
    ")\n",
    "\n",
    "# è¾“å‡ºæ¨¡å‹ç»“æ„ï¼Œæ–¹ä¾¿è§‚å¯Ÿ\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# ä¼˜åŒ–å™¨è®¾ç½®\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# åˆ›å»º AdamW ä¼˜åŒ–å™¨ï¼Œç”¨äºè®­ç»ƒ Transformer æ¨¡å‹\n",
    "# å‚æ•°è¯´æ˜ï¼š\n",
    "# - model.parameters(): æ¨¡å‹çš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°\n",
    "# - lr=1e-5: å­¦ä¹ ç‡ï¼ŒTransformer å¾®è°ƒæ—¶é€šå¸¸è®¾ç½®è¾ƒå°çš„ lrï¼ˆå¦‚ 1e-5 ~ 5e-5ï¼‰\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# é€å‚æ•°è¯´æ˜ï¼š\n",
    "# - model.parameters(): è¿”å›æ¨¡å‹æ‰€æœ‰å‚æ•°çš„è¿­ä»£å™¨ï¼Œä¼˜åŒ–å™¨ä¼šæ›´æ–°è¿™äº›å‚æ•°\n",
    "# - lr (learning rate): å­¦ä¹ ç‡ï¼Œæ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿\n",
    "# AdamW æ˜¯å¸¦æƒé‡è¡°å‡çš„ Adam ä¼˜åŒ–å™¨ï¼Œé€‚åˆ Transformer å¾®è°ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [02:45<00:00, 11.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¹³å‡æŸå¤±: 1.5071\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [02:34<00:00, 11.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¹³å‡æŸå¤±: 0.5644\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [02:32<00:00, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¹³å‡æŸå¤±: 0.5121\n",
      "âœ… è®­ç»ƒå®Œæˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# è®­ç»ƒå¾ªç¯\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# 1ï¸âƒ£ é€‰æ‹©è®¾å¤‡\n",
    "# å¦‚æœæœ‰ GPU å¯ç”¨ï¼Œåˆ™ä½¿ç”¨ GPUï¼Œå¦åˆ™ä½¿ç”¨ CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡\n",
    "model.to(device)\n",
    "\n",
    "# è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "# è®­ç»ƒæ¨¡å¼ä¼šå¯ç”¨ dropout ç­‰è®­ç»ƒç‰¹æœ‰æ“ä½œ\n",
    "model.train()\n",
    "\n",
    "# è®­ç»ƒ 3 ä¸ª epochï¼ˆè½®æ¬¡ï¼‰\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch + 1}/3\")  # æ‰“å°å½“å‰è½®æ¬¡\n",
    "    total_loss = 0  # åˆå§‹åŒ–ç´¯è®¡æŸå¤±\n",
    "\n",
    "    # ä½¿ç”¨ DataLoader éå†è®­ç»ƒæ•°æ®é›†\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        \n",
    "        # 1. æ¢¯åº¦æ¸…é›¶\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2. è·å–è¾“å…¥æ•°æ®å¹¶ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "        input_ids = batch[\"input_ids\"].to(device)           # è¯æ±‡ç´¢å¼•\n",
    "        attention_mask = batch[\"attention_mask\"].to(device) # attention maskï¼ŒæŒ‡ç¤ºå“ªäº›ä½ç½®æ˜¯å¡«å……\n",
    "        labels = batch[\"labels\"].to(device)               # æ ‡ç­¾ç´¢å¼•\n",
    "\n",
    "        # 3. å‰å‘ä¼ æ’­\n",
    "        # è¿”å›çš„ outputs æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å« loss å’Œ logits ç­‰\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # è·å–å½“å‰ batch çš„æŸå¤±\n",
    "        total_loss += loss.item()  # ç´¯åŠ æŸå¤±å€¼\n",
    "\n",
    "        # 4. åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. å‚æ•°æ›´æ–°\n",
    "        optimizer.step()\n",
    "\n",
    "    # è®¡ç®—å¹³å‡æŸå¤±å¹¶æ‰“å°\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"å¹³å‡æŸå¤±: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"âœ… è®­ç»ƒå®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹å·²ä¿å­˜åˆ° 'distilbert_ner_model' æ–‡ä»¶å¤¹ã€‚\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# æ¨¡å‹ä¿å­˜\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# å°†è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹\n",
    "# DistilBertForTokenClassification æ¨¡å‹æä¾› save_pretrained æ–¹æ³•\n",
    "model.save_pretrained(\"distilbert_ner_model\")\n",
    "\n",
    "# ä¿å­˜å¯¹åº”çš„ tokenizerï¼ˆåˆ†è¯å™¨ï¼‰\n",
    "# ç”¨äºåç»­åŠ è½½æ¨¡å‹æ—¶ä¿æŒä¸€è‡´çš„åˆ†è¯è§„åˆ™\n",
    "tokenizer.save_pretrained(\"distilbert_ner_model\")\n",
    "\n",
    "# æç¤ºç”¨æˆ·æ¨¡å‹å·²ä¿å­˜\n",
    "print(\"æ¨¡å‹å·²ä¿å­˜åˆ° 'distilbert_ner_model' æ–‡ä»¶å¤¹ã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ­å–œï¼\n",
    "\n",
    "#### ä½ åº”è¯¥è®°ä½çš„è¦ç‚¹\n",
    "\n",
    "- å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å¯ä»¥æ£€æµ‹å¹¶åˆ†ç±»å‘½åå®ä½“ï¼Œèƒ½å¤Ÿå¸®åŠ©å¤„ç†ç®€å†ã€å®¢æˆ·è¯„è®ºã€æµè§ˆå†å²ç­‰æ•°æ®ã€‚\n",
    "- åœ¨å°†æ–‡æœ¬è¾“å…¥ Transformer æ¨¡å‹ä¹‹å‰ï¼Œå¿…é¡»ä½¿ç”¨ä¸é¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„åˆ†è¯å™¨å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
