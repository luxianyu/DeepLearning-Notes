{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¡¨æƒ…ç¬¦å·ç”Ÿæˆå™¨ï¼ˆEmojify!ï¼‰\n",
    "\n",
    "æ¬¢è¿æ¥åˆ°ç¬¬äºŒå‘¨çš„ç¬¬äºŒä¸ªä½œä¸šã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•åˆ©ç”¨**è¯å‘é‡è¡¨ç¤ºï¼ˆword vector representationsï¼‰**æ¥æ„å»ºä¸€ä¸ªâ€œè¡¨æƒ…ç¬¦å·ç”Ÿæˆå™¨â€ï¼ˆEmojifierï¼‰ã€‚\n",
    "\n",
    "\n",
    "### é¡¹ç›®ç›®æ ‡\n",
    "\n",
    "ä½ æ˜¯å¦æ›¾ç»å¸Œæœ›è‡ªå·±çš„çŸ­ä¿¡æ›´ç”ŸåŠ¨ã€æ›´æœ‰è¡¨ç°åŠ›ï¼Ÿè¿™ä¸ªè¡¨æƒ…ç¬¦å·ç”Ÿæˆå™¨åº”ç”¨ç¨‹åºï¼ˆemojifier appï¼‰å°†å¸®åŠ©ä½ å®ç°è¿™ä¸€ç‚¹ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œå°†ä¸‹é¢è¿™å¥è¯ï¼š\n",
    "\n",
    "> â€œCongratulations on the promotion! Lets get coffee and talk. Love you!â€\n",
    "\n",
    "è‡ªåŠ¨å˜æˆï¼š\n",
    "\n",
    "> â€œCongratulations on the promotion! ğŸ‘ Lets get coffee and talk. â˜•ï¸ Love you! â¤ï¸â€\n",
    "\n",
    "### ä»»åŠ¡è¯´æ˜\n",
    "\n",
    "ä½ å°†å®ç°ä¸€ä¸ªæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿè¾“å…¥ä¸€å¥è¯ï¼ˆä¾‹å¦‚ â€œLet's go see the baseball game tonight!â€ï¼‰ï¼Œå¹¶è¾“å‡ºæœ€åˆé€‚çš„è¡¨æƒ…ç¬¦å·ï¼ˆâš¾ï¸ï¼‰ã€‚\n",
    "\n",
    "åœ¨è®¸å¤šè¡¨æƒ…ç•Œé¢ä¸­ï¼Œä½ å¿…é¡»è®°ä½ â¤ï¸ æ˜¯â€œheartâ€ï¼ˆå¿ƒå½¢ï¼‰ç¬¦å·ï¼Œè€Œä¸æ˜¯â€œloveâ€ï¼ˆçˆ±ï¼‰ç¬¦å·ã€‚ä½†é€šè¿‡ä½¿ç”¨**è¯å‘é‡**ï¼Œä½ ä¼šçœ‹åˆ°å³ä½¿è®­ç»ƒé›†ä¸­ä»…æ˜ç¡®åœ°å°†å°‘æ•°å•è¯ä¸æŸä¸ªè¡¨æƒ…ç¬¦å·å…³è”èµ·æ¥ï¼Œä½ çš„ç®—æ³•ä»ç„¶èƒ½å¤Ÿ**æ³›åŒ–ï¼ˆgeneralizeï¼‰**åˆ°æµ‹è¯•é›†ä¸­çš„å…¶ä»–å•è¯ä¸Šâ€”â€”å³ä¾¿è¿™äº›å•è¯åœ¨è®­ç»ƒé›†ä¸­ä»æœªå‡ºç°è¿‡ã€‚\n",
    "\n",
    "è¿™ç§ç‰¹æ€§è®©ä½ èƒ½å¤Ÿåœ¨**å°è§„æ¨¡è®­ç»ƒé›†**çš„åŸºç¡€ä¸Šï¼Œä¾ç„¶æ„å»ºä¸€ä¸ªå‡†ç¡®çš„â€œå¥å­åˆ°è¡¨æƒ…ç¬¦å·â€çš„åˆ†ç±»å™¨ï¼ˆclassifierï¼‰ã€‚\n",
    "\n",
    "\n",
    "### æ¨¡å‹ç‰ˆæœ¬\n",
    "\n",
    "åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†ä¾æ¬¡æ„å»ºä¸¤ä¸ªæ¨¡å‹ï¼š\n",
    "\n",
    "1. **Emojifier-V1**ï¼šä¸€ä¸ªä½¿ç”¨è¯åµŒå…¥ï¼ˆword embeddingsï¼‰çš„åŸºç¡€æ¨¡å‹ã€‚\n",
    "2. **Emojifier-V2**ï¼šä¸€ä¸ªè¿›ä¸€æ­¥ç»“åˆ **LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰** çš„é«˜çº§æ¨¡å‹ã€‚\n",
    "\n",
    "\n",
    "### å‡†å¤‡å¼€å§‹\n",
    "\n",
    "ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒä»¥åŠ è½½æ‰€éœ€çš„åŒ…ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - åŸºçº¿æ¨¡å‹ï¼šEmojifier-V1\n",
    "\n",
    "### 1.1 - æ•°æ®é›† EMOJISET\n",
    "\n",
    "è®©æˆ‘ä»¬ä»æ„å»ºä¸€ä¸ªç®€å•çš„åŸºçº¿åˆ†ç±»å™¨å¼€å§‹ã€‚\n",
    "\n",
    "åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œä½ å°†ä½¿ç”¨ä¸€ä¸ªéå¸¸å°çš„æ•°æ®é›† **(X, Y)**ï¼Œå…¶ä¸­ï¼š\n",
    "\n",
    "- **X** åŒ…å« **127 æ¡å¥å­ï¼ˆå­—ç¬¦ä¸²ï¼‰**  \n",
    "- **Y** ä¸ºæ¯æ¡å¥å­å¯¹åº”çš„ **æ•´æ•°æ ‡ç­¾ï¼ˆ0 åˆ° 4 ä¹‹é—´ï¼‰**ï¼Œæ¯ä¸ªæ ‡ç­¾ä»£è¡¨ä¸€ç§è¡¨æƒ…ç¬¦å·\n",
    "\n",
    "\n",
    "<img src=\"images/data_set.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **å›¾ 1**ï¼šEMOJISET â€”â€” ä¸€ä¸ªåŒ…å« 5 ä¸ªç±»åˆ«çš„åˆ†ç±»é—®é¢˜ã€‚ä¸Šå›¾å±•ç¤ºäº†éƒ¨åˆ†å¥å­ç¤ºä¾‹ã€‚</center></caption>\n",
    "\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½è¯¥æ•°æ®é›†ã€‚  \n",
    "æˆ‘ä»¬å°†æ•°æ®é›†åˆ’åˆ†ä¸ºï¼š\n",
    "- **è®­ç»ƒé›†ï¼ˆ127 ä¸ªæ ·æœ¬ï¼‰**\n",
    "- **æµ‹è¯•é›†ï¼ˆ56 ä¸ªæ ·æœ¬ï¼‰**\n",
    "\n",
    "ï¼ˆä»¥ä¸‹ä»£ç å°†åœ¨åç»­å•å…ƒä¸­æ‰§è¡Œï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒï¼Œä»¥æ‰“å°å‡ºæ¥è‡ª **X_train** çš„å¥å­åŠå…¶åœ¨ **Y_train** ä¸­å¯¹åº”çš„æ ‡ç­¾ã€‚  \n",
    "ä½ å¯ä»¥ä¿®æ”¹å˜é‡ `index` çš„å€¼ï¼Œä»¥æŸ¥çœ‹ä¸åŒçš„ç¤ºä¾‹ã€‚\n",
    "\n",
    ">  æ³¨æ„ï¼šç”±äº iPython Notebook ä½¿ç”¨çš„å­—ä½“åŸå› ï¼Œå¿ƒå½¢è¡¨æƒ…ç¬¦å· â¤ï¸ å¯èƒ½ä¼šæ˜¾ç¤ºä¸ºé»‘è‰²ï¼ˆğŸ–¤ï¼‰ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡ã€‚\n",
    "\n",
    "```python\n",
    "# ç¤ºä¾‹ä»£ç \n",
    "index = 10  # å¯ä¿®æ”¹ä»¥æŸ¥çœ‹ä¸åŒæ ·æœ¬\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 45\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Emojifier-V1 æ¨¡å‹æ¦‚è§ˆ\n",
    "\n",
    "åœ¨æœ¬éƒ¨åˆ†ä¸­ï¼Œä½ å°†å®ç°ä¸€ä¸ªåŸºçº¿æ¨¡å‹ï¼Œåä¸º **â€œEmojifier-V1â€**ã€‚\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"images/image_1.png\" style=\"width:900px;height:300px;\">\n",
    "<caption><center>**å›¾ 2**ï¼šåŸºçº¿æ¨¡å‹ï¼ˆEmojifier-V1ï¼‰ã€‚</center></caption>\n",
    "</center>\n",
    "\n",
    "\n",
    "è¯¥æ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ªä»£è¡¨å¥å­çš„å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼šâ€œI love youâ€ï¼‰ã€‚  \n",
    "æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º **(1, 5)** çš„**æ¦‚ç‡å‘é‡ï¼ˆprobability vectorï¼‰**ï¼Œè¡¨ç¤ºè¯¥å¥å­å±äºäº”ä¸ªè¡¨æƒ…ç±»åˆ«ä¸­æ¯ä¸€ç±»çš„æ¦‚ç‡ã€‚\n",
    "\n",
    "æœ€ç»ˆï¼Œä½ å°†æŠŠè¿™ä¸ªæ¦‚ç‡å‘é‡ä¼ å…¥ä¸€ä¸ª **argmax å±‚**ï¼Œä»¥æå–å‡º**æœ€å¯èƒ½çš„è¡¨æƒ…ç¬¦å·å¯¹åº”çš„ç´¢å¼•**ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹é¢„æµ‹çš„ç»“æœã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†è®©æ ‡ç­¾ **Y** é€‚ç”¨äº **Softmax åˆ†ç±»å™¨ï¼ˆsoftmax classifierï¼‰** çš„è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒä»å½“å‰çš„å½¢çŠ¶  \n",
    "$(m, 1)$ è½¬æ¢ä¸º **â€œç‹¬çƒ­è¡¨ç¤ºâ€ï¼ˆone-hot representationï¼‰**ï¼Œå½¢çŠ¶ä¸º $(m, 5)$ã€‚\n",
    "\n",
    "åœ¨è¿™ç§è¡¨ç¤ºæ–¹å¼ä¸­ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ª **one-hot å‘é‡**ï¼Œç”¨äºæŒ‡ç¤ºæŸä¸ªæ ·æœ¬æ‰€å±çš„ç±»åˆ«ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼š\n",
    "- è‹¥æ ‡ç­¾ä¸º 2ï¼Œåˆ™å¯¹åº”çš„ one-hot å‘é‡ä¸º `[0, 0, 1, 0, 0]`ã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç ç‰‡æ®µå¯ä»¥å®Œæˆè¿™ä¸€è½¬æ¢æ“ä½œã€‚  \n",
    "å…¶ä¸­ï¼Œå˜é‡å `Y_oh_train` å’Œ `Y_oh_test` ä¸­çš„ `Y_oh` è¡¨ç¤º **â€œY çš„ one-hot å½¢å¼ï¼ˆY-one-hotï¼‰â€**ã€‚\n",
    "\n",
    "```python\n",
    "# å°†æ ‡ç­¾è½¬æ¢ä¸º one-hot å‘é‡è¡¨ç¤º\n",
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ `convert_to_one_hot()` å‡½æ•°çš„è¿è¡Œç»“æœã€‚  \n",
    "ä½ å¯ä»¥ä¿®æ”¹å˜é‡ `index` çš„å€¼ï¼Œä»¥æ‰“å°å¹¶æŸ¥çœ‹ä¸åŒæ ·æœ¬å¯¹åº”çš„ **one-hot å‘é‡è¡¨ç¤º**ã€‚\n",
    "\n",
    "```python\n",
    "# æŸ¥çœ‹æŸä¸ªæ ·æœ¬çš„ one-hot è½¬æ¢ç»“æœ\n",
    "index = 1  # å¯æ›´æ”¹ç´¢å¼•ä»¥æŸ¥çœ‹ä¸åŒæ ·æœ¬\n",
    "print(f\"æ ‡ç­¾å€¼: {Y_train[index]}\")\n",
    "print(f\"One-hot è¡¨ç¤º: {Y_oh_train[index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 25\n",
    "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæ‰€æœ‰æ•°æ®éƒ½å·²ç»å‡†å¤‡å®Œæ¯•ï¼Œå¯ä»¥è¾“å…¥åˆ° **Emojifier-V1 æ¨¡å‹** ä¸­è¿›è¡Œè®­ç»ƒäº†ã€‚  \n",
    "æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å¼€å§‹å®ç°è¿™ä¸ªæ¨¡å‹å§ï¼ ğŸš€\n",
    "\n",
    "```python\n",
    "# å®ç° Emojifier-V1 æ¨¡å‹çš„å‡½æ•°æ¡†æ¶\n",
    "def model(X, Y, word_to_vec_map, learning_rate=0.01, num_iterations=400):\n",
    "    \"\"\"\n",
    "    å®ç° Emojifier-V1 æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "    X -- å¥å­åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ ä¸ºå­—ç¬¦ä¸²ï¼‰\n",
    "    Y -- å¥å­å¯¹åº”çš„æ ‡ç­¾ï¼ˆ0~4ï¼‰\n",
    "    word_to_vec_map -- è¯å‘é‡å­—å…¸ï¼šæ¯ä¸ªå•è¯æ˜ å°„åˆ°å…¶å¯¹åº”çš„ GloVe å‘é‡\n",
    "    learning_rate -- å­¦ä¹ ç‡ï¼ˆé»˜è®¤ 0.01ï¼‰\n",
    "    num_iterations -- è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ 400ï¼‰\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "    pred -- æ¨¡å‹å¯¹æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœ\n",
    "    \"\"\"\n",
    "    # æ¨¡å‹å®ç°ç»†èŠ‚å°†åœ¨åç»­å°èŠ‚ä¸­å®Œæˆ\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - å®ç° Emojifier-V1\n",
    "\n",
    "å¦‚å›¾ï¼ˆ2ï¼‰æ‰€ç¤ºï¼Œç¬¬ä¸€æ­¥æ˜¯å°†è¾“å…¥çš„å¥å­è½¬æ¢ä¸ºè¯å‘é‡è¡¨ç¤ºï¼Œç„¶åå¯¹è¿™äº›è¯å‘é‡å–å¹³å‡å€¼ã€‚  \n",
    "ä¸ä¹‹å‰çš„ç»ƒä¹ ç±»ä¼¼ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é¢„è®­ç»ƒçš„ **50 ç»´ GloVe è¯å‘é‡**ã€‚  \n",
    "è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒä»¥åŠ è½½åŒ…å«æ‰€æœ‰è¯å‘é‡è¡¨ç¤ºçš„ `word_to_vec_map`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å·²ç»åŠ è½½äº†ä»¥ä¸‹å†…å®¹ï¼š\n",
    "\n",
    "- `word_to_index`ï¼šå°†å•è¯æ˜ å°„åˆ°è¯æ±‡è¡¨ç´¢å¼•çš„å­—å…¸ï¼ˆå…± 400,001 ä¸ªå•è¯ï¼Œæœ‰æ•ˆç´¢å¼•èŒƒå›´ 0 åˆ° 400,000ï¼‰  \n",
    "- `index_to_word`ï¼šå°†ç´¢å¼•æ˜ å°„å›è¯æ±‡è¡¨å¯¹åº”å•è¯çš„å­—å…¸  \n",
    "- `word_to_vec_map`ï¼šå°†å•è¯æ˜ å°„åˆ°å…¶ GloVe å‘é‡è¡¨ç¤ºçš„å­—å…¸  \n",
    "\n",
    "è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒä»¥æ£€æŸ¥åŠ è½½æ˜¯å¦æ­£å¸¸ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç»ƒä¹ **ï¼šå®ç° `sentence_to_avg()` å‡½æ•°ã€‚ä½ éœ€è¦å®Œæˆä»¥ä¸‹ä¸¤æ­¥ï¼š\n",
    "\n",
    "1. å°†æ¯ä¸ªå¥å­è½¬æ¢ä¸ºå°å†™å­—æ¯ï¼Œç„¶åå°†å¥å­æ‹†åˆ†ä¸ºå•è¯åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨ `X.lower()` å’Œ `X.split()`ã€‚  \n",
    "2. å¯¹å¥å­ä¸­çš„æ¯ä¸ªå•è¯ï¼Œè·å–å…¶å¯¹åº”çš„ GloVe å‘é‡è¡¨ç¤ºã€‚ç„¶åï¼Œå¯¹æ‰€æœ‰è¯å‘é‡å–å¹³å‡å€¼ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sentence_to_avg\n",
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Split sentence into list of lower case words (â‰ˆ 1 line)\n",
    "\n",
    "    \n",
    "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "\n",
    "    \n",
    "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **avg= **\n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
    " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
    "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
    "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
    "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
    "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
    " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
    " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
    "  0.1445417   0.09808667]\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### æ¨¡å‹\n",
    "\n",
    "ç°åœ¨ï¼Œä½ å·²ç»å…·å¤‡å®Œæˆ `model()` å‡½æ•°å®ç°çš„æ‰€æœ‰ç»„ä»¶ã€‚  \n",
    "åœ¨ä½¿ç”¨ `sentence_to_avg()` åï¼Œä½ éœ€è¦å°†å¹³å‡å‘é‡è¾“å…¥åˆ°å‰å‘ä¼ æ’­ä¸­ï¼Œè®¡ç®—æŸå¤±ï¼Œç„¶åè¿›è¡Œåå‘ä¼ æ’­ä»¥æ›´æ–° softmax å‚æ•°ã€‚\n",
    "\n",
    "**ç»ƒä¹ **ï¼šå®ç°å›¾ï¼ˆ2ï¼‰ä¸­æè¿°çš„ `model()` å‡½æ•°ã€‚  \n",
    "å‡è®¾è¿™é‡Œçš„ $Yoh$ï¼ˆâ€œY one hotâ€ï¼‰æ˜¯è¾“å‡ºæ ‡ç­¾çš„ one-hot ç¼–ç ï¼Œå‰å‘ä¼ æ’­å’Œäº¤å‰ç†µæŸå¤±è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "$$ z^{(i)} = W . avg^{(i)} + b$$  \n",
    "$$ a^{(i)} = softmax(z^{(i)})$$  \n",
    "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$\n",
    "\n",
    "å½“ç„¶å¯ä»¥å®ç°ä¸€ä¸ªæ›´é«˜æ•ˆçš„å‘é‡åŒ–å®ç°ã€‚ä½†ç”±äºæˆ‘ä»¬è¿™æ¬¡ä»ç„¶ä½¿ç”¨ for å¾ªç¯å°†å¥å­é€ä¸€è½¬æ¢ä¸º $avg^{(i)}$ è¡¨ç¤ºï¼Œè¿™é‡Œå°±ä¸å†ä¼˜åŒ–äº†ã€‚\n",
    "\n",
    "æˆ‘ä»¬å·²ç»ä¸ºä½ æä¾›äº†ä¸€ä¸ª `softmax()` å‡½æ•°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Define number of training examples\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 5                                 # number of classes  \n",
    "    n_h = 50                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
    "        for i in range(m):                                # Loop over the training examples\n",
    "            \n",
    "            ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "\n",
    "            \n",
    "            # Forward propagate the avg through the softmax layer\n",
    "\n",
    "            \n",
    "\n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(X_train[0])\n",
    "print(type(X_train))\n",
    "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
    "print(Y.shape)\n",
    "\n",
    "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
    " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
    " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
    " 'You totally deserve this prize', 'Let us go play football',\n",
    " 'Are you down for football this afternoon', 'Work hard play harder',\n",
    " 'It is suprising how people can be dumb sometimes',\n",
    " 'I am very disappointed','It is the best day in my life',\n",
    " 'I think I will end up alone','My life is so boring','Good job',\n",
    " 'Great so awesome'])\n",
    "\n",
    "print(X.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(type(X_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿è¡Œä¸‹ä¸€ä¸ªä»£ç å•å…ƒä»¥è®­ç»ƒä½ çš„æ¨¡å‹ï¼Œå¹¶å­¦ä¹  softmax å‚æ•° **(W, b)**ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (on a subset of iterations):\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Epoch: 0**\n",
    "        </td>\n",
    "        <td>\n",
    "           cost = 1.95204988128\n",
    "        </td>\n",
    "        <td>\n",
    "           Accuracy: 0.348484848485\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "        <td>\n",
    "            **Epoch: 100**\n",
    "        </td>\n",
    "        <td>\n",
    "           cost = 0.0797181872601\n",
    "        </td>\n",
    "        <td>\n",
    "           Accuracy: 0.931818181818\n",
    "        </td>\n",
    "    </tr>\n",
    "    \n",
    "<tr>\n",
    "        <td>\n",
    "            **Epoch: 200**\n",
    "        </td>\n",
    "        <td>\n",
    "           cost = 0.0445636924368\n",
    "        </td>\n",
    "        <td>\n",
    "           Accuracy: 0.954545454545\n",
    "        </td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td>\n",
    "            **Epoch: 300**\n",
    "        </td>\n",
    "        <td>\n",
    "           cost = 0.0343226737879\n",
    "        </td>\n",
    "        <td>\n",
    "           Accuracy: 0.969696969697\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤ªæ£’äº†ï¼ä½ çš„æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„å‡†ç¡®ç‡ç›¸å½“é«˜ã€‚  \n",
    "ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å®ƒåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¦‚ä½•ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 1.4 - æµ‹è¯•é›†æ€§èƒ½è¯„ä¼°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Train set accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "           97.7\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Test set accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "           85.7\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æœ‰ 5 ä¸ªç±»åˆ«çš„æƒ…å†µä¸‹ï¼ŒéšæœºçŒœæµ‹çš„å‡†ç¡®ç‡åªæœ‰ 20%ã€‚  \n",
    "è€Œåœ¨ä»…ç”¨ 127 ä¸ªæ ·æœ¬è®­ç»ƒåï¼Œä½ çš„æ¨¡å‹å°±èƒ½è¾¾åˆ°è¿™æ ·çš„æ€§èƒ½ï¼Œå·²ç»ç›¸å½“ä¸é”™äº†ã€‚\n",
    "\n",
    "åœ¨è®­ç»ƒé›†ä¸­ï¼Œç®—æ³•è§è¿‡å¥å­ \"*I love you*\"ï¼Œå¯¹åº”æ ‡ç­¾ä¸º â¤ï¸ã€‚  \n",
    "ç„¶è€Œï¼Œä½ å¯ä»¥æ³¨æ„åˆ°å•è¯ \"adore\" å¹¶æœªå‡ºç°åœ¨è®­ç»ƒé›†ä¸­ã€‚  \n",
    "å°½ç®¡å¦‚æ­¤ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¾“å…¥ \"*I adore you*\" æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤ªæ£’äº†ï¼ç”±äº *adore* ä¸ *love* çš„è¯å‘é‡ç›¸ä¼¼ï¼Œç®—æ³•èƒ½å¤Ÿæ­£ç¡®æ³›åŒ–ï¼Œå³ä½¿æ˜¯ä¸€ä¸ªä»æœªè§è¿‡çš„å•è¯ä¹Ÿèƒ½å¤„ç†æ­£ç¡®ã€‚  \n",
    "åƒ *heart*ã€*dear*ã€*beloved* æˆ– *adore* è¿™æ ·çš„è¯ï¼Œå…¶è¯å‘é‡éƒ½ä¸ *love* ç±»ä¼¼ï¼Œå› æ­¤ä¹Ÿå¯èƒ½åŒæ ·æœ‰æ•ˆâ€”â€”ä½ å¯ä»¥è‡ªç”±ä¿®æ”¹ä¸Šè¿°è¾“å…¥ï¼Œå°è¯•å„ç§å¥å­ï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚\n",
    "\n",
    "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç®—æ³•æ— æ³•æ­£ç¡®å¤„ç† \"*not feeling happy*\" è¿™ç§æƒ…å†µã€‚  \n",
    "è¿™æ˜¯å› ä¸ºè¯¥ç®—æ³•å¿½ç•¥äº†**å•è¯é¡ºåº**ï¼Œå› æ­¤ä¸æ“…é•¿ç†è§£è¯¸å¦‚ \"*not happy*\" è¿™æ ·çš„çŸ­è¯­ã€‚\n",
    "\n",
    "æ‰“å° **æ··æ·†çŸ©é˜µï¼ˆconfusion matrixï¼‰** ä¹Ÿèƒ½å¸®åŠ©ç†è§£å“ªäº›ç±»åˆ«å¯¹ä½ çš„æ¨¡å‹æ¥è¯´æ›´éš¾ã€‚  \n",
    "æ··æ·†çŸ©é˜µæ˜¾ç¤ºäº†æŸä¸ªå®é™…æ ‡ç­¾ä¸ºæŸç±»åˆ«ï¼ˆ\"actual\" classï¼‰çš„æ ·æœ¬ï¼Œè¢«ç®—æ³•è¯¯æ ‡ä¸ºå…¶ä»–ç±»åˆ«ï¼ˆ\"predicted\" class\"ï¼‰çš„é¢‘ç‡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test.shape)\n",
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**æœ¬éƒ¨åˆ†éœ€è¦è®°ä½çš„å†…å®¹**ï¼š\n",
    "- å³ä½¿åªæœ‰ 127 ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä½ ä¹Ÿå¯ä»¥å¾—åˆ°ä¸€ä¸ªç›¸å½“ä¸é”™çš„è¡¨æƒ…ç¬¦å·ç”Ÿæˆæ¨¡å‹ã€‚è¿™è¦å½’åŠŸäºè¯å‘é‡æä¾›çš„æ³›åŒ–èƒ½åŠ›ã€‚  \n",
    "- Emojify-V1 åœ¨å¤„ç†åƒ *\"This movie is not good and not enjoyable\"* è¿™æ ·çš„å¥å­æ—¶è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒæ— æ³•ç†è§£å•è¯ç»„åˆâ€”â€”å®ƒåªæ˜¯å°†æ‰€æœ‰å•è¯çš„è¯å‘é‡å¹³å‡ï¼Œè€Œä¸å…³æ³¨å•è¯çš„é¡ºåºã€‚ä½ å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†æ„å»ºä¸€ä¸ªæ›´å¥½çš„ç®—æ³•ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Emojifier-V2ï¼šåœ¨ Keras ä¸­ä½¿ç”¨ LSTM\n",
    "\n",
    "è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª **LSTM æ¨¡å‹**ï¼Œè¾“å…¥ä¸ºå•è¯åºåˆ—ã€‚  \n",
    "è¯¥æ¨¡å‹èƒ½å¤Ÿè€ƒè™‘å•è¯çš„é¡ºåºã€‚Emojifier-V2 å°†ç»§ç»­ä½¿ç”¨**é¢„è®­ç»ƒè¯å‘é‡**æ¥è¡¨ç¤ºå•è¯ï¼Œä½†ä¼šå°†è¿™äº›è¯å‘é‡è¾“å…¥åˆ° LSTM ä¸­ï¼Œç”± LSTM é¢„æµ‹æœ€åˆé€‚çš„è¡¨æƒ…ç¬¦å·ã€‚\n",
    "\n",
    "è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒä»¥åŠ è½½ Keras æ‰€éœ€çš„åŒ…ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - æ¨¡å‹æ¦‚è§ˆ\n",
    "\n",
    "ä¸‹é¢æ˜¯ä½ å°†è¦å®ç°çš„ **Emojifier-V2**ï¼š\n",
    "\n",
    "<img src=\"images/emojifier-v2.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **å›¾ 3**ï¼šEmojifier-V2ã€‚ä¸€ä¸ªä¸¤å±‚ LSTM åºåˆ—åˆ†ç±»å™¨ã€‚</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Keras ä¸å°æ‰¹é‡è®­ç»ƒï¼ˆmini-batchingï¼‰\n",
    "\n",
    "åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ **mini-batch** æ¥è®­ç»ƒ Keras æ¨¡å‹ã€‚  \n",
    "ç„¶è€Œï¼Œå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶è¦æ±‚åŒä¸€ mini-batch ä¸­çš„æ‰€æœ‰åºåˆ—é•¿åº¦ç›¸åŒã€‚  \n",
    "è¿™æ˜¯å‘é‡åŒ–ï¼ˆvectorizationï¼‰èƒ½å¤Ÿå·¥ä½œçš„å‰æï¼š  \n",
    "å¦‚æœä¸€ä¸ªå¥å­æœ‰ 3 ä¸ªå•è¯ï¼Œå¦ä¸€ä¸ªå¥å­æœ‰ 4 ä¸ªå•è¯ï¼Œé‚£ä¹ˆå®ƒä»¬æ‰€éœ€çš„è®¡ç®—æ­¥éª¤ä¸åŒï¼ˆä¸€ä¸ª LSTM éœ€è¦ 3 æ­¥ï¼Œå¦ä¸€ä¸ªéœ€è¦ 4 æ­¥ï¼‰ï¼Œå› æ­¤æ— æ³•åŒæ—¶å¤„ç†ã€‚\n",
    "\n",
    "å¸¸è§çš„è§£å†³æ–¹æ¡ˆæ˜¯ **å¡«å……ï¼ˆpaddingï¼‰**ã€‚  \n",
    "å…·ä½“æ–¹æ³•æ˜¯è®¾ç½®ä¸€ä¸ªæœ€å¤§åºåˆ—é•¿åº¦ï¼Œå¹¶å°†æ‰€æœ‰åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ€å¤§åºåˆ—é•¿åº¦ä¸º 20ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨â€œ0â€å¡«å……æ¯ä¸ªå¥å­ï¼Œä½¿æ¯ä¸ªè¾“å…¥å¥å­é•¿åº¦ä¸º 20ã€‚  \n",
    "å› æ­¤ï¼Œå¥å­ \"i love you\" å°†è¡¨ç¤ºä¸ºï¼š\n",
    "\n",
    "$$(e_{i}, e_{love}, e_{you}, \\vec{0}, \\vec{0}, \\ldots, \\vec{0})$$\n",
    "\n",
    "åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œä»»ä½•è¶…è¿‡ 20 ä¸ªå•è¯çš„å¥å­éƒ½å¿…é¡»æˆªæ–­ã€‚  \n",
    "ä¸€ç§ç®€å•é€‰æ‹©æœ€å¤§åºåˆ—é•¿åº¦çš„æ–¹æ³•æ˜¯ç›´æ¥å–è®­ç»ƒé›†ä¸­æœ€é•¿å¥å­çš„é•¿åº¦ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - åµŒå…¥å±‚ï¼ˆEmbedding layerï¼‰\n",
    "\n",
    "åœ¨ Keras ä¸­ï¼ŒåµŒå…¥çŸ©é˜µè¢«è¡¨ç¤ºä¸ºä¸€ä¸ª **â€œå±‚ï¼ˆlayerï¼‰â€**ï¼Œå®ƒå°†æ­£æ•´æ•°ï¼ˆå¯¹åº”å•è¯çš„ç´¢å¼•ï¼‰æ˜ å°„åˆ°å›ºå®šå¤§å°çš„ç¨ å¯†å‘é‡ï¼ˆembedding å‘é‡ï¼‰ã€‚  \n",
    "åµŒå…¥å±‚å¯ä»¥è®­ç»ƒï¼Œä¹Ÿå¯ä»¥ç”¨é¢„è®­ç»ƒå‘é‡åˆå§‹åŒ–ã€‚åœ¨æœ¬éƒ¨åˆ†ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•åœ¨ Keras ä¸­åˆ›å»ºä¸€ä¸ª [Embedding()](https://keras.io/layers/embeddings/) å±‚ï¼Œå¹¶ç”¨ä¹‹å‰åŠ è½½çš„ **50 ç»´ GloVe å‘é‡** åˆå§‹åŒ–ã€‚  \n",
    "ç”±äºæˆ‘ä»¬çš„è®­ç»ƒé›†è¾ƒå°ï¼Œæˆ‘ä»¬ä¸ä¼šæ›´æ–°è¯å‘é‡ï¼Œè€Œæ˜¯ä¿æŒå…¶å›ºå®šå€¼ã€‚ä½†ä¸‹é¢çš„ä»£ç ä¹Ÿä¼šå±•ç¤º Keras å¦‚ä½•å…è®¸ä½ é€‰æ‹©è®­ç»ƒæˆ–ä¿æŒå›ºå®šè¯¥å±‚ã€‚\n",
    "\n",
    "\n",
    "\n",
    "`Embedding()` å±‚çš„è¾“å…¥æ˜¯ä¸€ä¸ªæ•´æ•°çŸ©é˜µï¼Œå½¢çŠ¶ä¸º **(batch size, max input length)**ã€‚  \n",
    "è¿™å¯¹åº”äºå°†å¥å­è½¬æ¢ä¸ºç´¢å¼•åˆ—è¡¨ï¼ˆæ•´æ•°ï¼‰ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "\n",
    "<img src=\"images/embedding1.png\" style=\"width:700px;height:250px;\">\n",
    "<caption><center> **å›¾ 4**ï¼šåµŒå…¥å±‚ç¤ºæ„å›¾ã€‚è¯¥ç¤ºä¾‹å±•ç¤ºäº†ä¸¤ä¸ªæ ·æœ¬é€šè¿‡åµŒå…¥å±‚çš„ä¼ æ’­è¿‡ç¨‹ã€‚  \n",
    "ä¸¤ä¸ªæ ·æœ¬éƒ½è¢«ç”¨ 0 å¡«å……è‡³é•¿åº¦ `max_len=5`ã€‚æœ€ç»ˆè¡¨ç¤ºçš„ç»´åº¦ä¸º `(2, max_len, 50)`ï¼Œå› ä¸ºä½¿ç”¨çš„è¯å‘é‡ä¸º 50 ç»´ã€‚</center></caption>\n",
    "\n",
    "è¾“å…¥ä¸­çš„æœ€å¤§æ•´æ•°ï¼ˆå³å•è¯ç´¢å¼•ï¼‰ä¸åº”å¤§äºè¯æ±‡è¡¨å¤§å°ã€‚  \n",
    "è¯¥å±‚è¾“å‡ºå½¢çŠ¶ä¸º **(batch size, max input length, è¯å‘é‡ç»´åº¦)**ã€‚\n",
    "\n",
    "\n",
    "\n",
    "ç¬¬ä¸€æ­¥æ˜¯å°†æ‰€æœ‰è®­ç»ƒå¥å­è½¬æ¢ä¸ºç´¢å¼•åˆ—è¡¨ï¼Œç„¶åå¯¹è¿™äº›åˆ—è¡¨è¿›è¡Œ **é›¶å¡«å……ï¼ˆzero-paddingï¼‰**ï¼Œä½¿å…¶é•¿åº¦ç­‰äºæœ€é•¿å¥å­çš„é•¿åº¦ã€‚\n",
    "\n",
    "**ç»ƒä¹ **ï¼šå®ç°ä¸‹åˆ—å‡½æ•°ï¼Œå°† **X**ï¼ˆå¥å­æ•°ç»„ï¼Œå­—ç¬¦ä¸²å½¢å¼ï¼‰è½¬æ¢ä¸ºå¯¹åº”å•è¯ç´¢å¼•çš„æ•°ç»„ã€‚  \n",
    "è¾“å‡ºå½¢çŠ¶åº”é€‚ç”¨äº `Embedding()` å±‚ï¼ˆå¦‚å›¾ 4 æ‰€ç¤ºï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sentences_to_indices\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n",
    "\n",
    "    \n",
    "    # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "\n",
    "        \n",
    "        # Initialize j to 0\n",
    "\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "\n",
    "            # Increment j to j + 1\n",
    "\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒï¼Œä»¥æŸ¥çœ‹ `sentences_to_indices()` çš„ä½œç”¨ï¼Œå¹¶æ£€æŸ¥ä½ çš„ç»“æœã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **X1 =**\n",
    "        </td>\n",
    "        <td>\n",
    "           ['funny lol' 'lets play football' 'food is ready for you']\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **X1_indices =**\n",
    "        </td>\n",
    "        <td>\n",
    "           [[ 155345.  225122.       0.       0.       0.] <br>\n",
    "            [ 220930.  286375.  151266.       0.       0.] <br>\n",
    "            [ 151204.  192973.  302254.  151349.  394475.]]\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®©æˆ‘ä»¬åœ¨ Keras ä¸­æ„å»º **Embedding() å±‚**ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡ã€‚  \n",
    "åœ¨æ„å»ºå¥½è¯¥å±‚åï¼Œä½ å°†æŠŠ `sentences_to_indices()` çš„è¾“å‡ºä½œä¸ºè¾“å…¥ä¼ å…¥ï¼ŒEmbedding() å±‚å°†è¿”å›å¥å­çš„è¯å‘é‡è¡¨ç¤ºã€‚\n",
    "\n",
    "**ç»ƒä¹ **ï¼šå®ç° `pretrained_embedding_layer()`ã€‚ä½ éœ€è¦å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. å°†åµŒå…¥çŸ©é˜µåˆå§‹åŒ–ä¸ºå½¢çŠ¶æ­£ç¡®çš„å…¨é›¶ numpy æ•°ç»„ã€‚  \n",
    "2. ç”¨ `word_to_vec_map` ä¸­çš„æ‰€æœ‰è¯å‘é‡å¡«å……åµŒå…¥çŸ©é˜µã€‚  \n",
    "3. å®šä¹‰ Keras åµŒå…¥å±‚ï¼Œä½¿ç”¨ [Embedding()](https://keras.io/layers/embeddings/)ã€‚  \n",
    "   - ç¡®ä¿å°†è¯¥å±‚è®¾ç½®ä¸º **ä¸å¯è®­ç»ƒï¼ˆtrainable = Falseï¼‰**ã€‚  \n",
    "   - è‹¥è®¾ç½®ä¸º `trainable = True`ï¼Œä¼˜åŒ–ç®—æ³•å°†å…è®¸ä¿®æ”¹è¯å‘é‡çš„å€¼ã€‚  \n",
    "4. å°†åµŒå…¥æƒé‡è®¾ç½®ä¸ºä½ æ„å»ºçš„åµŒå…¥çŸ©é˜µã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pretrained_embedding_layer\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "\n",
    "    \n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **weights[0][1][3] =**\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.3403\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 æ„å»º Emojifier-V2\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬æ„å»º **Emojifier-V2** æ¨¡å‹ã€‚  \n",
    "ä½ å°†ä½¿ç”¨ä¹‹å‰æ„å»ºçš„åµŒå…¥å±‚ï¼Œå¹¶å°†å…¶è¾“å‡ºä¼ å…¥ LSTM ç½‘ç»œã€‚\n",
    "\n",
    "<img src=\"images/emojifier-v2.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **å›¾ 3**ï¼šEmojifier-V2ã€‚ä¸€ä¸ªä¸¤å±‚ LSTM åºåˆ—åˆ†ç±»å™¨ã€‚</center></caption>\n",
    "\n",
    "---\n",
    "\n",
    "**ç»ƒä¹ **ï¼šå®ç° `Emojify_V2()`ï¼Œæ„å»ºå›¾ 3 ä¸­æ‰€ç¤ºçš„ Keras æ¨¡å‹ã€‚  \n",
    "- æ¨¡å‹è¾“å…¥ä¸ºå½¢çŠ¶ä¸º (`m`, `max_len`) çš„å¥å­æ•°ç»„ï¼Œç”± `input_shape` å®šä¹‰ã€‚  \n",
    "- æ¨¡å‹è¾“å‡ºä¸ºå½¢çŠ¶ä¸º (`m`, `C=5`) çš„ softmax æ¦‚ç‡å‘é‡ã€‚  \n",
    "- ä½ å¯èƒ½éœ€è¦ä½¿ç”¨ï¼š\n",
    "  - `Input(shape = ..., dtype = '...')`\n",
    "  - [LSTM()](https://keras.io/layers/recurrent/#lstm)\n",
    "  - [Dropout()](https://keras.io/layers/core/#dropout)\n",
    "  - [Dense()](https://keras.io/layers/core/#dense)\n",
    "  - [Activation()](https://keras.io/activations/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Emojify_V2\n",
    "\n",
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "\n",
    "    # Add dropout with a probability of 0.5\n",
    "\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "\n",
    "    # Add dropout with a probability of 0.5\n",
    "\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "\n",
    "    # Add a softmax activation\n",
    "\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒä»¥åˆ›å»ºæ¨¡å‹å¹¶æŸ¥çœ‹å…¶æ‘˜è¦ã€‚  \n",
    "ç”±äºæ•°æ®é›†ä¸­æ‰€æœ‰å¥å­é•¿åº¦éƒ½å°äº 10 ä¸ªå•è¯ï¼Œæˆ‘ä»¬é€‰æ‹© `max_len = 10`ã€‚  \n",
    "\n",
    "ä½ åº”è¯¥å¯ä»¥çœ‹åˆ°æ¨¡å‹æ¶æ„ï¼Œå®ƒä½¿ç”¨äº† **20,223,927** ä¸ªå‚æ•°ï¼Œå…¶ä¸­ **20,000,050**ï¼ˆè¯å‘é‡éƒ¨åˆ†ï¼‰ä¸ºä¸å¯è®­ç»ƒï¼Œå…¶ä½™ **223,877** ä¸ªä¸ºå¯è®­ç»ƒå‚æ•°ã€‚  \n",
    "ç”±äºæˆ‘ä»¬çš„è¯æ±‡è¡¨å¤§å°ä¸º 400,001 ä¸ªå•è¯ï¼ˆæœ‰æ•ˆç´¢å¼•ä» 0 åˆ° 400,000ï¼‰ï¼Œå› æ­¤ä¸å¯è®­ç»ƒå‚æ•°æ•°é‡ä¸º 400,001 Ã— 50 = 20,000,050ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å’Œå¾€å¸¸ä¸€æ ·ï¼Œåœ¨ Keras ä¸­åˆ›å»ºæ¨¡å‹åï¼Œä½ éœ€è¦ **ç¼–è¯‘æ¨¡å‹**ï¼Œå¹¶å®šä¹‰ä½¿ç”¨çš„ **æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè¯„ä¼°æŒ‡æ ‡**ã€‚  \n",
    "\n",
    "ä½¿ç”¨ä»¥ä¸‹è®¾ç½®ç¼–è¯‘ä½ çš„æ¨¡å‹ï¼š  \n",
    "- æŸå¤±å‡½æ•°ï¼š`categorical_crossentropy`  \n",
    "- ä¼˜åŒ–å™¨ï¼š`adam`  \n",
    "- è¯„ä¼°æŒ‡æ ‡ï¼š`['accuracy']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æ˜¯è®­ç»ƒæ¨¡å‹çš„æ—¶å€™äº†ã€‚  \n",
    "\n",
    "ä½ çš„ Emojifier-V2 **æ¨¡å‹**è¾“å…¥ä¸ºå½¢çŠ¶ (`m`, `max_len`) çš„æ•°ç»„ï¼Œè¾“å‡ºä¸ºå½¢çŠ¶ (`m`, ç±»åˆ«æ•°) çš„æ¦‚ç‡å‘é‡ã€‚  \n",
    "å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†ï¼š\n",
    "- **X_train**ï¼ˆå¥å­æ•°ç»„ï¼Œå­—ç¬¦ä¸²å½¢å¼ï¼‰è½¬æ¢ä¸º **X_train_indices**ï¼ˆå¥å­æ•°ç»„ï¼Œåˆ—è¡¨å½¢å¼çš„å•è¯ç´¢å¼•ï¼‰  \n",
    "- **Y_train**ï¼ˆæ ‡ç­¾ç´¢å¼•ï¼‰è½¬æ¢ä¸º **Y_train_oh**ï¼ˆæ ‡ç­¾çš„ one-hot å‘é‡ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ **X_train_indices** å’Œ **Y_train_oh** ä¸Šè®­ç»ƒ Keras æ¨¡å‹ã€‚  \n",
    "è®­ç»ƒå‚æ•°è®¾ç½®ä¸ºï¼š\n",
    "- `epochs = 50`  \n",
    "- `batch_size = 32`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ çš„æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„å‡†ç¡®ç‡åº”è¯¥æ¥è¿‘ **100%**ã€‚  \n",
    "å…·ä½“çš„å‡†ç¡®ç‡å¯èƒ½ä¼šç•¥æœ‰å·®å¼‚ã€‚  \n",
    "\n",
    "è¿è¡Œä»¥ä¸‹ä»£ç å•å…ƒï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ä½ çš„æ¨¡å‹è¡¨ç°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡åº”åœ¨ **80% åˆ° 95%** ä¹‹é—´ã€‚  \n",
    "è¿è¡Œä¸‹æ–¹ä»£ç å•å…ƒï¼ŒæŸ¥çœ‹è¢«é”™è¯¯æ ‡æ³¨çš„ç¤ºä¾‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ä½ å¯ä»¥åœ¨è‡ªå·±çš„ç¤ºä¾‹ä¸Šå°è¯•ã€‚  \n",
    "åœ¨ä¸‹é¢å†™ä¸‹ä½ è‡ªå·±çš„å¥å­ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
    "x_test = np.array(['not feeling good'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¹‹å‰ï¼ŒEmojify-V1 æ¨¡å‹æ— æ³•æ­£ç¡®æ ‡æ³¨ \"*not feeling happy*\"ï¼Œä½†æˆ‘ä»¬å®ç°çš„ Emojifier-V2 èƒ½æ­£ç¡®è¯†åˆ«ã€‚ï¼ˆKeras çš„è¾“å‡ºæ¯æ¬¡ç•¥æœ‰éšæœºæ€§ï¼Œå› æ­¤ä½ å¯èƒ½æ²¡æœ‰å¾—åˆ°å®Œå…¨ç›¸åŒçš„ç»“æœã€‚ï¼‰  \n",
    "\n",
    "å½“å‰æ¨¡å‹åœ¨ç†è§£å¦å®šè¯ï¼ˆå¦‚ \"*not happy*\"ï¼‰æ–¹é¢ä»ä¸å¤Ÿç¨³å¥ï¼Œå› ä¸ºè®­ç»ƒé›†è¾ƒå°ï¼Œç¼ºå°‘å¦å®šå¥çš„ç¤ºä¾‹ã€‚  \n",
    "ä½†å¦‚æœè®­ç»ƒé›†æ›´å¤§ï¼ŒLSTM æ¨¡å‹åœ¨ç†è§£è¿™ç§å¤æ‚å¥å­æ–¹é¢å°†è¿œä¼˜äº Emojify-V1 æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ­å–œï¼\n",
    "\n",
    "ä½ å·²ç»å®Œæˆäº†æœ¬ç¬”è®°æœ¬çš„å…¨éƒ¨å†…å®¹ï¼â¤ï¸â¤ï¸â¤ï¸\n",
    "\n",
    "<font color='blue'>\n",
    "**æœ¬æ¬¡å†…å®¹éœ€è¦è®°ä½çš„è¦ç‚¹**ï¼š\n",
    "- å¦‚æœä½ çš„ NLP ä»»åŠ¡è®­ç»ƒé›†è¾ƒå°ï¼Œä½¿ç”¨è¯å‘é‡ï¼ˆword embeddingsï¼‰å¯ä»¥æ˜¾è‘—æå‡ç®—æ³•æ•ˆæœã€‚è¯å‘é‡ä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†æµ‹è¯•é›†ä¸­å¯èƒ½æœªå‡ºç°åœ¨è®­ç»ƒé›†ä¸­çš„å•è¯ã€‚  \n",
    "- åœ¨ Kerasï¼ˆä»¥åŠå¤§å¤šæ•°å…¶ä»–æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼‰ä¸­è®­ç»ƒåºåˆ—æ¨¡å‹ï¼Œéœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š\n",
    "    - ä½¿ç”¨ mini-batch æ—¶ï¼Œéœ€è¦å¯¹åºåˆ—è¿›è¡Œå¡«å……ï¼ˆpaddingï¼‰ï¼Œç¡®ä¿ mini-batch ä¸­çš„æ‰€æœ‰æ ·æœ¬é•¿åº¦ç›¸åŒã€‚  \n",
    "    - `Embedding()` å±‚å¯ä»¥ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡åˆå§‹åŒ–ã€‚è¿™äº›è¯å‘é‡å¯ä»¥ä¿æŒå›ºå®šï¼Œä¹Ÿå¯ä»¥åœ¨ä½ çš„æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒã€‚ä½†å¦‚æœæ ‡æ³¨æ•°æ®é›†å¾ˆå°ï¼Œé€šå¸¸ä¸å€¼å¾—è®­ç»ƒå¤§è§„æ¨¡é¢„è®­ç»ƒè¯å‘é‡ã€‚  \n",
    "    - `LSTM()` æœ‰ä¸€ä¸ªå‚æ•° `return_sequences`ï¼Œç”¨äºå†³å®šè¿”å›æ‰€æœ‰éšè—çŠ¶æ€è¿˜æ˜¯ä»…è¿”å›æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚  \n",
    "    - å¯ä»¥åœ¨ `LSTM()` åä½¿ç”¨ `Dropout()` æ¥å¯¹ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "æ­å–œä½ å®Œæˆäº†æœ¬æ¬¡ä½œä¸šå¹¶æˆåŠŸæ„å»ºäº†ä¸€ä¸ª Emojifierã€‚  \n",
    "å¸Œæœ›ä½ å¯¹è‡ªå·±åœ¨æœ¬ç¬”è®°æœ¬ä¸­çš„æˆæœæ„Ÿåˆ°æ»¡æ„ï¼\n",
    "\n",
    "# ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡´è°¢\n",
    "\n",
    "æ„Ÿè°¢ Alison Darcy å’Œ Woebot å›¢é˜Ÿåœ¨æœ¬ä½œä¸šåˆ›å»ºè¿‡ç¨‹ä¸­æä¾›çš„å»ºè®®ã€‚  \n",
    "Woebot æ˜¯ä¸€ä¸ªå¯ä»¥éšæ—¶ä¸ä½ äº¤æµçš„èŠå¤©æœºå™¨äººæœ‹å‹ï¼Œå…¨å¤© 24 å°æ—¶åœ¨çº¿ã€‚  \n",
    "ä½œä¸º Woebot æŠ€æœ¯çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒä½¿ç”¨è¯å‘é‡æ¥ç†è§£ä½ æ‰€è¯´å†…å®¹çš„æƒ…ç»ªã€‚  \n",
    "ä½ å¯ä»¥è®¿é—® [http://woebot.io](http://woebot.io) ä½“éªŒå®ƒã€‚\n",
    "\n",
    "<img src=\"images/woebot.png\" style=\"width:600px;height:300px;\">\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
