{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¿«ä¹å±‹çš„äººè„¸è¯†åˆ«\n",
    "\n",
    "æœ¬èŠ‚ï¼Œä½ å°†æ„å»ºä¸€ä¸ªäººè„¸è¯†åˆ«ç³»ç»Ÿã€‚è®¸å¤šè¿™é‡Œä»‹ç»çš„æ€æƒ³æ¥è‡ª [FaceNet](https://arxiv.org/pdf/1503.03832.pdf)ã€‚åœ¨è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº† [DeepFace](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)ã€‚\n",
    "\n",
    "äººè„¸è¯†åˆ«é—®é¢˜é€šå¸¸åˆ†ä¸ºä¸¤ç±»ï¼š\n",
    "\n",
    "- **äººè„¸éªŒè¯ (Face Verification)** - â€œè¿™ä¸ªäººæ˜¯å¦æ˜¯å£°ç§°çš„é‚£ä¸ªäººï¼Ÿâ€ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€äº›æœºåœºï¼Œä½ å¯ä»¥é€šè¿‡è®©ç³»ç»Ÿæ‰«ææŠ¤ç…§å¹¶éªŒè¯ä½ ï¼ˆæŒæœ‰æŠ¤ç…§çš„äººï¼‰æ˜¯å¦ä¸ºæ­£ç¡®èº«ä»½æ¥é€šè¿‡æµ·å…³ã€‚ç”¨äººè„¸è§£é”çš„æ‰‹æœºä¹Ÿåœ¨ä½¿ç”¨äººè„¸éªŒè¯ã€‚è¿™æ˜¯ä¸€ä¸ª **1:1 åŒ¹é…é—®é¢˜**ã€‚\n",
    "- **äººè„¸è¯†åˆ« (Face Recognition)** - â€œè¿™ä¸ªäººæ˜¯è°ï¼Ÿâ€ã€‚ä¾‹å¦‚ï¼Œè§†é¢‘è¯¾ç¨‹å±•ç¤ºäº†ä¸€ä¸ªäººè„¸è¯†åˆ«è§†é¢‘ (https://www.youtube.com/watch?v=wr4rx0Spihs)ï¼Œç™¾åº¦å‘˜å·¥å¯ä»¥ä¸ç”¨é¢å¤–èº«ä»½éªŒè¯å°±è¿›å…¥åŠå…¬å®¤ã€‚è¿™æ˜¯ä¸€ä¸ª **1:K åŒ¹é…é—®é¢˜**ã€‚\n",
    "\n",
    "FaceNet è®­ç»ƒå‡ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå¯ä»¥å°†ä¸€å¼ äººè„¸å›¾åƒç¼–ç ä¸ºä¸€ä¸ª 128 ç»´å‘é‡ã€‚é€šè¿‡æ¯”è¾ƒä¸¤ä¸ªè¿™æ ·çš„å‘é‡ï¼Œå°±èƒ½åˆ¤æ–­ä¸¤å¼ å›¾ç‰‡æ˜¯å¦æ˜¯åŒä¸€ä¸ªäººã€‚\n",
    "\n",
    "**æœ¬èŠ‚ä¸­ï¼Œä½ å°†ï¼š**\n",
    "- å®ç°ä¸‰å…ƒç»„æŸå¤±å‡½æ•° (triplet loss function)\n",
    "- ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å°†äººè„¸å›¾åƒæ˜ å°„åˆ° 128 ç»´ç¼–ç \n",
    "- ä½¿ç”¨è¿™äº›ç¼–ç æ¥è¿›è¡Œäººè„¸éªŒè¯å’Œäººè„¸è¯†åˆ«\n",
    "\n",
    "åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ **â€œchannels firstâ€** çš„å·ç§¯ç½‘ç»œæ¿€æ´»è¡¨ç¤ºæ–¹å¼ï¼Œè€Œä¸æ˜¯ä¹‹å‰ä¸­ä½¿ç”¨çš„ **â€œchannels lastâ€** æ–¹å¼ã€‚æ¢å¥è¯è¯´ï¼Œä¸€æ‰¹å›¾åƒçš„å½¢çŠ¶æ˜¯ $(m, n_C, n_H, n_W)$ è€Œä¸æ˜¯ $(m, n_H, n_W, n_C)$ã€‚åœ¨å¼€æºå®ç°ä¸­ï¼Œè¿™ä¸¤ç§æ–¹å¼éƒ½æœ‰ä¸€å®šåº”ç”¨ï¼›æ·±åº¦å­¦ä¹ ç¤¾åŒºä¸­è¿˜æ²¡æœ‰ç»Ÿä¸€æ ‡å‡†ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬åŠ è½½æ‰€éœ€çš„åŒ…å§ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ PyTorch ç‰ˆæœ¬è¯´æ˜\n",
    "\n",
    "### æ ¸å¿ƒç‰¹æ€§\n",
    "\n",
    "1. **åŒæ¨¡å‹æ”¯æŒ** ğŸ”„\n",
    "   - âœ… é¢„è®­ç»ƒæ¨¡å‹ï¼ˆfacenet-pytorchï¼‰- çœŸå®è¯†åˆ«èƒ½åŠ›ï¼Œå‡†ç¡®ç‡>95%\n",
    "   - âœ… éšæœºæƒé‡æ¨¡å‹ - å­¦ä¹ ç®—æ³•åŸç†ï¼ˆä¸‰å…ƒç»„æŸå¤±ç­‰ï¼‰\n",
    "\n",
    "2. **ä¸€é”®åˆ‡æ¢** âš™ï¸\n",
    "   ```python\n",
    "   USE_PRETRAINED = True   # é¢„è®­ç»ƒæ¨¡å‹\n",
    "   USE_PRETRAINED = False  # éšæœºæƒé‡  \n",
    "   ```\n",
    "\n",
    "3. **æ™ºèƒ½é€‚é…** ğŸ¤–\n",
    "   - è‡ªåŠ¨è°ƒæ•´é˜ˆå€¼ï¼ˆé¢„è®­ç»ƒ=0.8ï¼Œéšæœº=0.7ï¼‰\n",
    "   - è‡ªåŠ¨é…ç½®ç¼–ç ç»´åº¦ï¼ˆ512ç»´/128ç»´ï¼‰\n",
    "   - è‡ªåŠ¨é€‰æ‹©å›¾åƒé¢„å¤„ç†æ–¹å¼\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ ¸å¿ƒç»„ä»¶è§£æ\n",
    "\n",
    "#### 1. **Inception ç½‘ç»œæ¶æ„**\n",
    "```\n",
    "InceptionBlock åŒ…å«4ä¸ªå¹¶è¡Œåˆ†æ”¯:\n",
    "â”œâ”€â”€ 3x3å·ç§¯åˆ†æ”¯: æå–ä¸­ç­‰å°ºåº¦ç‰¹å¾\n",
    "â”‚   â””â”€â”€ Conv2d(in_channelsâ†’96, 1x1) + BatchNorm + ReLU\n",
    "â”‚   â””â”€â”€ Conv2d(96â†’128, 3x3, padding=1) + BatchNorm + ReLU\n",
    "â”‚\n",
    "â”œâ”€â”€ 5x5å·ç§¯åˆ†æ”¯: æå–å¤§å°ºåº¦ç‰¹å¾  \n",
    "â”‚   â””â”€â”€ Conv2d(in_channelsâ†’16, 1x1) + BatchNorm + ReLU\n",
    "â”‚   â””â”€â”€ Conv2d(16â†’32, 5x5, padding=2) + BatchNorm + ReLU\n",
    "â”‚\n",
    "â”œâ”€â”€ æ± åŒ–åˆ†æ”¯: ä¸‹é‡‡æ ·å’Œç‰¹å¾èšåˆ\n",
    "â”‚   â””â”€â”€ MaxPool2d(3x3, stride=2, padding=1)\n",
    "â”‚   â””â”€â”€ Conv2d(in_channelsâ†’32, 1x1) + BatchNorm + ReLU\n",
    "â”‚   â””â”€â”€ åŠ¨æ€paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯å°ºå¯¸\n",
    "â”‚\n",
    "â””â”€â”€ 1x1å·ç§¯åˆ†æ”¯: é™ç»´å’Œå¢åŠ éçº¿æ€§\n",
    "    â””â”€â”€ Conv2d(in_channelsâ†’64, 1x1) + BatchNorm + ReLU\n",
    "\n",
    "æœ€å: Concatenateæ‰€æœ‰åˆ†æ”¯ â†’ è¾“å‡º\n",
    "```\n",
    "\n",
    "#### 2. **å›¾åƒé¢„å¤„ç†æµç¨‹** (img_to_encoding)\n",
    "```\n",
    "è¾“å…¥: å›¾åƒè·¯å¾„ (ä¾‹å¦‚ \"images/younes.jpg\")\n",
    "    â†“\n",
    "Step 1: cv2.imread() - è¯»å–å›¾åƒ (BGRæ ¼å¼)\n",
    "    â†“\n",
    "Step 2: img[..., ::-1] - è½¬æ¢ä¸ºRGBæ ¼å¼\n",
    "    â†“\n",
    "Step 3: transpose(2,0,1) - è°ƒæ•´ç»´åº¦ (H,W,C) â†’ (C,H,W)\n",
    "    â†“\n",
    "Step 4: /255.0 - å½’ä¸€åŒ–åˆ° [0,1]\n",
    "    â†“\n",
    "Step 5: torch.from_numpy().unsqueeze(0) - è½¬ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦\n",
    "    â†“\n",
    "Step 6: model(x) - å‰å‘ä¼ æ’­è·å¾—ç¼–ç \n",
    "    â†“\n",
    "è¾“å‡º: 128ç»´ç¼–ç å‘é‡ (shape: 1Ã—128)\n",
    "```\n",
    "\n",
    "#### 3. **ä¸‰å…ƒç»„æŸå¤±å‡½æ•°** (Triplet Loss)\n",
    "```\n",
    "æ•°å­¦å…¬å¼:\n",
    "L = Î£ max(||f(A) - f(P)||Â² - ||f(A) - f(N)||Â² + Î±, 0)\n",
    "\n",
    "å‚æ•°è¯´æ˜:\n",
    "- f(A): Anchorå›¾åƒçš„ç¼–ç  (åŸºå‡†å›¾åƒ)\n",
    "- f(P): Positiveå›¾åƒçš„ç¼–ç  (ä¸Aæ˜¯åŒä¸€äºº)\n",
    "- f(N): Negativeå›¾åƒçš„ç¼–ç  (ä¸Aæ˜¯ä¸åŒäºº)  \n",
    "- Î±: marginå‚æ•° (é»˜è®¤0.2ï¼Œç¡®ä¿æ­£è´Ÿæ ·æœ¬æœ‰è¶³å¤Ÿåˆ†ç¦»åº¦)\n",
    "\n",
    "æŸå¤±å«ä¹‰:\n",
    "- loss = 0: æ¨¡å‹å·²å¾ˆå¥½åˆ†ç¦»æ­£è´Ÿæ ·æœ¬ âœ“\n",
    "- loss > 0: éœ€è¦ç»§ç»­è®­ç»ƒï¼Œå¢å¤§æ­£è´Ÿæ ·æœ¬è·ç¦»\n",
    "```\n",
    "\n",
    "#### 4. **äººè„¸éªŒè¯** (verifyå‡½æ•°)\n",
    "```\n",
    "ä»»åŠ¡: 1:1åŒ¹é…é—®é¢˜\n",
    "è¾“å…¥: å›¾åƒ + å£°ç§°çš„èº«ä»½\n",
    "è¾“å‡º: (è·ç¦», æ˜¯å¦é€šè¿‡)\n",
    "\n",
    "æµç¨‹:\n",
    "1. æå–è¾“å…¥å›¾åƒç¼–ç  â†’ encoding\n",
    "2. è·å–æ•°æ®åº“ä¸­è¯¥èº«ä»½çš„ç¼–ç  â†’ database[identity]\n",
    "3. è®¡ç®—L2è·ç¦»: dist = ||encoding - database[identity]||\n",
    "4. åˆ¤æ–­: dist < 0.7 â†’ é€šè¿‡ âœ“\n",
    "         dist â‰¥ 0.7 â†’ æ‹’ç» âœ—\n",
    "```\n",
    "\n",
    "#### 5. **äººè„¸è¯†åˆ«** (who_is_itå‡½æ•°)\n",
    "```\n",
    "ä»»åŠ¡: 1:KåŒ¹é…é—®é¢˜\n",
    "è¾“å…¥: ä»…å›¾åƒ (ä¸éœ€è¦æä¾›èº«ä»½)\n",
    "è¾“å‡º: (æœ€å°è·ç¦», è¯†åˆ«å‡ºçš„èº«ä»½)\n",
    "\n",
    "æµç¨‹:\n",
    "1. æå–è¾“å…¥å›¾åƒç¼–ç \n",
    "2. éå†æ•°æ®åº“æ‰€æœ‰äººå‘˜:\n",
    "   - è®¡ç®—ä¸æ¯ä¸ªäººçš„è·ç¦»\n",
    "   - è®°å½•æœ€å°è·ç¦»å’Œå¯¹åº”èº«ä»½\n",
    "3. åˆ¤æ–­: min_dist < 0.7 â†’ è¯†åˆ«æˆåŠŸï¼Œè¿”å›èº«ä»½\n",
    "         min_dist â‰¥ 0.7 â†’ ä¸åœ¨æ•°æ®åº“ä¸­\n",
    "```\n",
    "\n",
    "### å…³é”®å‚æ•°è¯´æ˜\n",
    "\n",
    "| å‚æ•° | å«ä¹‰ | å…¸å‹å€¼ |\n",
    "|------|------|--------|\n",
    "| `in_channels` | è¾“å…¥ç‰¹å¾å›¾é€šé“æ•° | 192, 256, 320ç­‰ |\n",
    "| `kernel_size` | å·ç§¯æ ¸å¤§å° | 1, 3, 5, 7 |\n",
    "| `stride` | æ­¥é•¿ | 1(ä¿æŒå°ºå¯¸), 2(å‡åŠ) |\n",
    "| `padding` | å¡«å…… | æ ¹æ®kernel_sizeè°ƒæ•´ |\n",
    "| `eps` | BatchNormçš„Îµ | 0.00001(æ•°å€¼ç¨³å®šæ€§) |\n",
    "| `alpha` | Triplet Lossçš„margin | 0.2 |\n",
    "| `threshold` | éªŒè¯é˜ˆå€¼ | 0.7 |\n",
    "\n",
    "### æ³¨æ„äº‹é¡¹\n",
    "\n",
    "æ³¨æ„äº‹é¡¹ï¼š\n",
    "- ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡è¿›è¡Œæ¼”ç¤ºï¼Œå®é™…è¾“å‡ºä¼šä¸é¢„è®­ç»ƒæ¨¡å‹ä¸åŒ\n",
    "- è¦è·å¾—å‡†ç¡®çš„äººè„¸è¯†åˆ«æ•ˆæœï¼Œéœ€è¦åŠ è½½é¢„è®­ç»ƒçš„ PyTorch æƒé‡\n",
    "- å¯ä»¥ä» [facenet-pytorch](https://github.com/timesler/facenet-pytorch) ç­‰é¡¹ç›®è·å–é¢„è®­ç»ƒæƒé‡\n",
    "\n",
    "âš ï¸ **éšæœºåˆå§‹åŒ–æƒé‡ç‰¹ç‚¹**\n",
    "- æ‰€æœ‰ç¼–ç å‘é‡è¢«L2å½’ä¸€åŒ–åˆ°å•ä½çƒé¢(èŒƒæ•°=1.0)\n",
    "- éšæœºæƒé‡å¯¼è‡´æ‰€æœ‰å›¾åƒç¼–ç ç›¸ä¼¼ï¼Œè·ç¦»å¾ˆå°(<0.2)\n",
    "- éœ€è¦åŠ è½½é¢„è®­ç»ƒæƒé‡æ‰èƒ½è·å¾—æœ‰æ„ä¹‰çš„äººè„¸è¯†åˆ«ç»“æœ\n",
    "\n",
    "âœ… **é¢„è®­ç»ƒæƒé‡çš„è¡¨ç°**\n",
    "- åŒä¸€äºº: è·ç¦» < 0.7\n",
    "- ä¸åŒäºº: è·ç¦» > 0.7 (é€šå¸¸0.8-1.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ========== å¯¼å…¥å¿…è¦çš„åº“ ==========\n",
    "# ===============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# PyTorch æ ¸å¿ƒåº“\n",
    "# -----------------------------\n",
    "import torch                    # PyTorch ä¸»åº“ï¼Œç”¨äºè¿›è¡Œå¼ é‡ï¼ˆTensorï¼‰è¿ç®—å’Œè‡ªåŠ¨å¾®åˆ†ï¼ˆautogradï¼‰\n",
    "import torch.nn as nn           # nn æ¨¡å—ï¼ŒåŒ…å«ç¥ç»ç½‘ç»œçš„å¸¸ç”¨å±‚ï¼ˆå¦‚å·ç§¯å±‚ã€çº¿æ€§å±‚ç­‰ï¼‰\n",
    "import torch.nn.functional as F # F æ¨¡å—ï¼ˆfunctional APIï¼‰ï¼Œæä¾›å‡½æ•°å¼æ“ä½œï¼ˆå¦‚ ReLUã€æ± åŒ–ã€å·ç§¯ç­‰ï¼‰\n",
    "# ğŸ’¡ åŒºåˆ«è¯´æ˜ï¼š\n",
    "# - nn.Moduleï¼šå®šä¹‰å±‚ç»“æ„ï¼ˆç±»çš„æ–¹å¼ï¼‰\n",
    "# - torch.nn.functionalï¼šç›´æ¥è°ƒç”¨å‡½æ•°ï¼ˆå‡½æ•°å¼è°ƒç”¨ï¼‰\n",
    "\n",
    "# -----------------------------\n",
    "# å›¾åƒå¤„ç†åº“\n",
    "# -----------------------------\n",
    "from torchvision import transforms  # torchvision.transforms æä¾›å¸¸è§å›¾åƒé¢„å¤„ç†åŠŸèƒ½ï¼Œå¦‚Resizeã€ToTensorã€Normalizeç­‰\n",
    "from PIL import Image              # PILï¼ˆPython Imaging Libraryï¼‰ç”¨äºåŠ è½½ã€æ˜¾ç¤ºå’Œä¿å­˜å›¾ç‰‡\n",
    "import cv2                         # OpenCVåº“ï¼Œç”¨äºé«˜çº§å›¾åƒå¤„ç†ï¼ˆå¦‚æ»¤æ³¢ã€è¾¹ç¼˜æ£€æµ‹ç­‰ï¼‰\n",
    "\n",
    "# -----------------------------\n",
    "# æ•°æ®å¤„ç†åº“\n",
    "# -----------------------------\n",
    "import os                          # OSæ¨¡å—ï¼Œç”¨äºè·¯å¾„æ‹¼æ¥ã€æ–‡ä»¶è¯»å–ã€æ–‡ä»¶å¤¹æ“ä½œç­‰\n",
    "import numpy as np                 # NumPy æ˜¯ç§‘å­¦è®¡ç®—æ ¸å¿ƒåº“ï¼Œç”¨äºçŸ©é˜µè¿ç®—ã€çº¿æ€§ä»£æ•°ã€éšæœºæ•°ç”Ÿæˆç­‰\n",
    "from numpy import genfromtxt       # ä»æ–‡æœ¬æ–‡ä»¶ä¸­è¯»å–ç»“æ„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚csvï¼‰\n",
    "import pandas as pd                # Pandas ç”¨äºè¡¨æ ¼æ•°æ®ï¼ˆDataFrameï¼‰çš„è¯»å–ã€å¤„ç†ä¸åˆ†æ\n",
    "import matplotlib.pyplot as plt    # Matplotlib çš„ pyplot æ¥å£ï¼Œç”¨äºç»˜åˆ¶å›¾åƒï¼ˆå¦‚æŸå¤±æ›²çº¿ã€ç»“æœå¯¹æ¯”ç­‰ï¼‰\n",
    "\n",
    "# ===============================================================\n",
    "# ========== Jupyter Notebook é…ç½® ==========\n",
    "# ===============================================================\n",
    "\n",
    "# åœ¨ Notebook ä¸­å†…è”æ˜¾ç¤ºå›¾åƒï¼ˆç»˜å›¾ç»“æœä¼šç›´æ¥æ˜¾ç¤ºåœ¨å•å…ƒæ ¼ä¸­ï¼‰\n",
    "%matplotlib inline  \n",
    "\n",
    "# è‡ªåŠ¨é‡æ–°åŠ è½½æ¨¡å—ï¼ˆä½¿å¾—ä¿®æ”¹çš„ Python æ–‡ä»¶å¯ç«‹å³ç”Ÿæ•ˆï¼‰\n",
    "%load_ext autoreload\n",
    "\n",
    "# è‡ªåŠ¨é‡æ–°åŠ è½½æ‰€æœ‰æ¨¡å—ï¼ˆé™¤éæ˜ç¡®æ’é™¤ï¼‰ï¼Œç›¸å½“äºåŠ¨æ€åŠ è½½\n",
    "%autoreload 2\n",
    "\n",
    "# ğŸ’¡ æ•™å­¦è¯´æ˜ï¼š\n",
    "# - è¿™å‡ è¡Œå‘½ä»¤ä»…åœ¨ Jupyter Notebook ç¯å¢ƒä¸­æœ‰æ•ˆ\n",
    "# - ä¸»è¦ç”¨äºåŠ å¿«å®éªŒè°ƒè¯•æµç¨‹ï¼Œæ— éœ€æ¯æ¬¡æ‰‹åŠ¨é‡å¯å†…æ ¸å³å¯åŠ è½½ä¿®æ”¹åçš„ä»£ç \n",
    "\n",
    "# ===============================================================\n",
    "# ========== è®¾å¤‡é…ç½® ==========\n",
    "# ===============================================================\n",
    "\n",
    "# torch.device ç”¨äºæŒ‡å®šè®¡ç®—è®¾å¤‡ï¼š\n",
    "# - è‹¥ç”µè„‘æ”¯æŒ CUDA ä¸”é©±åŠ¨æ­£ç¡®å®‰è£…ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨ GPUï¼›\n",
    "# - å¦åˆ™è‡ªåŠ¨å›é€€åˆ° CPUã€‚\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# æ‰“å°å½“å‰ä½¿ç”¨çš„è®¾å¤‡ï¼ˆcuda æˆ– cpuï¼‰\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# æç¤ºï¼š\n",
    "# - ä½¿ç”¨ GPU å¯ä»¥æ˜¾è‘—æå‡æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒé€Ÿåº¦ï¼›\n",
    "# - è‹¥è¾“å‡ºä¸º \"cpu\"ï¼Œè¯´æ˜æœªæ£€æµ‹åˆ°å¯ç”¨ GPUï¼›\n",
    "# - å¯ä»¥ä½¿ç”¨ torch.cuda.get_device_name(0) æŸ¥çœ‹ GPU å‹å·ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - æœ´ç´ çš„äººè„¸éªŒè¯ (Naive Face Verification)\n",
    "\n",
    "åœ¨äººè„¸éªŒè¯ (Face Verification) ä¸­ï¼Œä½ ä¼šå¾—åˆ°ä¸¤å¼ å›¾åƒï¼Œå¹¶éœ€è¦åˆ¤æ–­å®ƒä»¬æ˜¯å¦å±äºåŒä¸€ä¸ªäººã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯é€åƒç´ æ¯”è¾ƒä¸¤å¼ å›¾åƒã€‚å¦‚æœä¸¤å¼ åŸå§‹å›¾åƒä¹‹é—´çš„è·ç¦»å°äºæŸä¸ªé€‰å®šçš„é˜ˆå€¼ï¼Œé‚£ä¹ˆå®ƒä»¬å¯èƒ½æ˜¯åŒä¸€ä¸ªäººï¼\n",
    "\n",
    "<img src=\"images/pixel_comparison.png\" style=\"width:380px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> ** å›¾ 1 ** </u></center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "å½“ç„¶ï¼Œè¿™ç§ç®—æ³•çš„æ•ˆæœéå¸¸å·®ï¼Œå› ä¸ºåƒç´ å€¼ä¼šå› å…‰ç…§ã€äººç‰©è„¸éƒ¨çš„æœå‘ã€ç”šè‡³å¤´éƒ¨ä½ç½®çš„ç»†å¾®å˜åŒ–è€Œå‘ç”Ÿå‰§çƒˆæ”¹å˜ã€‚  \n",
    "\n",
    "ä½ å°†ä¼šçœ‹åˆ°ï¼Œä¸å…¶ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼Œä¸å¦‚å­¦ä¹ ä¸€ä¸ªç¼–ç å‡½æ•° $f(img)$ã€‚é€šè¿‡å¯¹è¿™ç§ç¼–ç è¿›è¡Œé€å…ƒç´ æ¯”è¾ƒï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°åˆ¤æ–­ä¸¤å¼ å›¾ç‰‡æ˜¯å¦æ˜¯åŒä¸€ä¸ªäººã€‚  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception Block ä»£ç æ³¨é‡Šè¯´æ˜\n",
    "\n",
    "æ‰€æœ‰ `InceptionBlock` éƒ½åŒ…å«ä»¥ä¸‹åˆ†æ”¯ï¼š\n",
    "\n",
    "| è‹±æ–‡æ³¨é‡Š | ä¸­æ–‡å«ä¹‰ | åŠŸèƒ½è¯´æ˜ |\n",
    "|---------|---------|---------|\n",
    "| `# 3x3 branch` | `# 3x3å·ç§¯åˆ†æ”¯` | ä½¿ç”¨3x3å·ç§¯æ ¸æå–ä¸­ç­‰å°ºåº¦ç‰¹å¾ |\n",
    "| `# 5x5 branch` | `# 5x5å·ç§¯åˆ†æ”¯` | ä½¿ç”¨5x5å·ç§¯æ ¸æå–å¤§å°ºåº¦ç‰¹å¾ |\n",
    "| `# pool branch` | `# æ± åŒ–åˆ†æ”¯` | é€šè¿‡æ± åŒ–è¿›è¡Œä¸‹é‡‡æ ·å’Œç‰¹å¾èšåˆ |\n",
    "| `# 1x1 branch` | `# 1x1å·ç§¯åˆ†æ”¯` | ä½¿ç”¨1x1å·ç§¯è¿›è¡Œé™ç»´å’Œå¢åŠ éçº¿æ€§ |\n",
    "| `# Concatenate` | `# æ‹¼æ¥æ‰€æœ‰åˆ†æ”¯` | åœ¨é€šé“ç»´åº¦åˆå¹¶æ‰€æœ‰åˆ†æ”¯çš„è¾“å‡º |\n",
    "\n",
    "#### å¸¸ç”¨å˜é‡åè¯´æ˜ï¼š\n",
    "\n",
    "| å˜é‡å | ä¸­æ–‡å«ä¹‰ |\n",
    "|--------|----------|\n",
    "| `branch_3x3` | 3x3å·ç§¯åˆ†æ”¯çš„è¾“å‡º |\n",
    "| `branch_5x5` | 5x5å·ç§¯åˆ†æ”¯çš„è¾“å‡º |\n",
    "| `branch_pool` | æ± åŒ–åˆ†æ”¯çš„è¾“å‡º |\n",
    "| `branch_1x1` | 1x1å·ç§¯åˆ†æ”¯çš„è¾“å‡º |\n",
    "| `pad_h` | é«˜åº¦æ–¹å‘éœ€è¦å¡«å……çš„åƒç´ æ•° |\n",
    "| `pad_w` | å®½åº¦æ–¹å‘éœ€è¦å¡«å……çš„åƒç´ æ•° |\n",
    "\n",
    "#### ä»£ç ä¸­çš„æœ¯è¯­è¯´æ˜ï¼š\n",
    "\n",
    "ä¸ºäº†ä¿æŒä»£ç çš„å¯è¯»æ€§å’Œä¸PyTorchå®˜æ–¹æ–‡æ¡£çš„ä¸€è‡´æ€§ï¼Œä»¥ä¸‹æœ¯è¯­ä¿æŒè‹±æ–‡ï¼š\n",
    "- ç±»åï¼š`InceptionBlock1a`, `FaceRecoModel` ç­‰\n",
    "- å˜é‡åï¼š`branch_3x3`, `encoding`, `database` ç­‰  \n",
    "- PyTorchå‡½æ•°ï¼š`F.relu()`, `torch.cat()`, `nn.Conv2d()` ç­‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.æ¨¡å‹é…ç½®å’ŒåŠ è½½\n",
    "\n",
    "#### ğŸ“‹ ä¸¤ç§æ¨¡å‹å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | éšæœºæƒé‡æ¨¡å‹ | é¢„è®­ç»ƒæ¨¡å‹ (facenet-pytorch) |\n",
    "|------|-------------|---------------------------|\n",
    "| **ç”¨é€”** | å­¦ä¹ ç®—æ³•åŸç† ğŸ“š | å®é™…åº”ç”¨ ğŸš€ |\n",
    "| **è¯†åˆ«æ•ˆæœ** | âŒ æ— æ•ˆï¼ˆéšæœºï¼‰ | âœ… ä¼˜ç§€ï¼ˆ>95%å‡†ç¡®ç‡ï¼‰ |\n",
    "| **è¾“å‡ºç»´åº¦** | 128ç»´ | 512ç»´ |\n",
    "| **éœ€è¦å®‰è£…** | âŒ æ—  | âœ… `pip install facenet-pytorch` |\n",
    "| **åŠ è½½æ—¶é—´** | ç¬é—´ | é¦–æ¬¡2-5åˆ†é’Ÿï¼ˆä¸‹è½½æƒé‡ï¼‰ |\n",
    "| **æ¨èåœºæ™¯** | ç†è§£ä¸‰å…ƒç»„æŸå¤±ç­‰æ¦‚å¿µ | çœŸå®äººè„¸è¯†åˆ«åº”ç”¨ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "æ¨¡å‹é…ç½®\n",
      "======================================================================\n",
      "\n",
      "âœ… å·²é€‰æ‹©: éšæœºæƒé‡æ¨¡å‹\n",
      "\n",
      "ç‰¹ç‚¹:\n",
      "  âœ“ æ— éœ€é¢å¤–å®‰è£…ä¾èµ–\n",
      "  âœ“ æ¨¡å‹ç»“æ„ç®€å•ã€è¿è¡Œé€Ÿåº¦å¿«\n",
      "  âœ“ è¾“å‡º 128 ç»´éšæœºç‰¹å¾å‘é‡\n",
      "  âœ“ ç”¨äºç†è§£äººè„¸ç‰¹å¾å­¦ä¹ çš„åŸç†ï¼ˆä¾‹å¦‚ä¸‰å…ƒç»„æŸå¤± Triplet Lossï¼‰\n",
      "\n",
      "æ³¨æ„äº‹é¡¹:\n",
      "  âš ï¸  æ­¤æ¨¡å‹æœªç»è¿‡è®­ç»ƒï¼Œæ— æ³•è¿›è¡ŒçœŸå®çš„äººè„¸è¯†åˆ«\n",
      "  âš ï¸  ä»…ç”¨äºå­¦ä¹ ç®—æ³•é€»è¾‘ä¸ä»£ç ç»“æ„\n",
      "\n",
      "======================================================================\n",
      "é…ç½®å·²ä¿å­˜: USE_PRETRAINED = False\n",
      "======================================================================\n",
      "\n",
      "ğŸ‘‰ ç°åœ¨è¯·è¿è¡Œä¸‹ä¸€ä¸ª Cell ä»¥åŠ è½½æ¨¡å‹\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Step 1: é€‰æ‹©æ¨¡å‹ç±»å‹\n",
    "# ===============================================================\n",
    "# æœ¬å•å…ƒç”¨äºé€‰æ‹©æ˜¯å¦ä½¿ç”¨ **é¢„è®­ç»ƒæ¨¡å‹ï¼ˆfacenet-pytorchï¼‰**ã€‚\n",
    "# é¢„è®­ç»ƒæ¨¡å‹ï¼šå¯ç›´æ¥è¿›è¡ŒçœŸå®äººè„¸è¯†åˆ«ï¼›\n",
    "# éšæœºæƒé‡æ¨¡å‹ï¼šä»…ç”¨äºæ•™å­¦æ¼”ç¤ºç®—æ³•åŸç†ï¼Œä¸å…·å¤‡è¯†åˆ«èƒ½åŠ›ã€‚\n",
    "\n",
    "# è®¾ç½®å¼€å…³å‚æ•°ï¼ˆå¸ƒå°”å€¼ï¼‰\n",
    "# ---------------------------------------------------------------\n",
    "# True  => ä½¿ç”¨ facenet-pytorch æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆ512ç»´ç‰¹å¾ï¼‰\n",
    "# False => ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹ï¼ˆ128ç»´ç‰¹å¾ï¼‰\n",
    "USE_PRETRAINED = False  # ğŸ‘ˆ å¯ä»¥æ”¹ä¸º True æ¥å¯ç”¨é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "# ===============================================================\n",
    "# ========== è¾“å‡ºé…ç½®è¯´æ˜ ==========\n",
    "# ===============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"æ¨¡å‹é…ç½®\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===============================================================\n",
    "# å¦‚æœé€‰æ‹©ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹\n",
    "# ===============================================================\n",
    "if USE_PRETRAINED:\n",
    "    print(\"\\nâœ… å·²é€‰æ‹©: é¢„è®­ç»ƒæ¨¡å‹ (facenet-pytorch)\")\n",
    "    print(\"\\nç‰¹ç‚¹:\")\n",
    "    print(\"  âœ“ å·²åœ¨å¤§è§„æ¨¡äººè„¸æ•°æ®é›†ä¸Šè®­ç»ƒï¼ˆå¦‚ VGGFace2ï¼‰\")\n",
    "    print(\"  âœ“ æ‹¥æœ‰çœŸå®çš„äººè„¸è¯†åˆ«èƒ½åŠ›ï¼ˆå¯æå–ç¨³å®šç‰¹å¾ï¼‰\")\n",
    "    print(\"  âœ“ è¾“å‡º 512 ç»´äººè„¸ç¼–ç å‘é‡\")\n",
    "    print(\"  âœ“ ç²¾åº¦é«˜ï¼Œå¯ç›´æ¥ç”¨äºç›¸ä¼¼åº¦è®¡ç®—\")\n",
    "    print(\"\\nä½¿ç”¨å»ºè®®:\")\n",
    "    print(\"  â€¢ é€‚åˆç”¨äºäººè„¸éªŒè¯ã€äººè„¸èšç±»ã€ç›¸ä¼¼åº¦æœç´¢ç­‰å®é™…åº”ç”¨\")\n",
    "    print(\"  â€¢ é»˜è®¤ä¸‹è½½çº¦ 100 MB çš„é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ï¼ˆé¦–æ¬¡è¿è¡Œæ—¶ï¼‰\")\n",
    "    print(\"\\nå®‰è£…ä¾èµ–:\")\n",
    "    print(\"  ğŸ‘‰  pip install facenet-pytorch\")\n",
    "\n",
    "# ===============================================================\n",
    "# å¦‚æœé€‰æ‹©ä½¿ç”¨éšæœºæƒé‡æ¨¡å‹\n",
    "# ===============================================================\n",
    "else:\n",
    "    print(\"\\nâœ… å·²é€‰æ‹©: éšæœºæƒé‡æ¨¡å‹\")\n",
    "    print(\"\\nç‰¹ç‚¹:\")\n",
    "    print(\"  âœ“ æ— éœ€é¢å¤–å®‰è£…ä¾èµ–\")\n",
    "    print(\"  âœ“ æ¨¡å‹ç»“æ„ç®€å•ã€è¿è¡Œé€Ÿåº¦å¿«\")\n",
    "    print(\"  âœ“ è¾“å‡º 128 ç»´éšæœºç‰¹å¾å‘é‡\")\n",
    "    print(\"  âœ“ ç”¨äºç†è§£äººè„¸ç‰¹å¾å­¦ä¹ çš„åŸç†ï¼ˆä¾‹å¦‚ä¸‰å…ƒç»„æŸå¤± Triplet Lossï¼‰\")\n",
    "    print(\"\\næ³¨æ„äº‹é¡¹:\")\n",
    "    print(\"  âš ï¸  æ­¤æ¨¡å‹æœªç»è¿‡è®­ç»ƒï¼Œæ— æ³•è¿›è¡ŒçœŸå®çš„äººè„¸è¯†åˆ«\")\n",
    "    print(\"  âš ï¸  ä»…ç”¨äºå­¦ä¹ ç®—æ³•é€»è¾‘ä¸ä»£ç ç»“æ„\")\n",
    "\n",
    "# ===============================================================\n",
    "# æœ€åæ‰“å°é…ç½®æ€»ç»“\n",
    "# ===============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"é…ç½®å·²ä¿å­˜: USE_PRETRAINED = {USE_PRETRAINED}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nğŸ‘‰ ç°åœ¨è¯·è¿è¡Œä¸‹ä¸€ä¸ª Cell ä»¥åŠ è½½æ¨¡å‹\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "åˆ›å»ºéšæœºæƒé‡æ¨¡å‹\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FaceRecoModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# åˆ›å»ºéšæœºæƒé‡çš„è‡ªå®šä¹‰æ¨¡å‹ï¼š\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# FaceRecoModel ç±»åœ¨ä¹‹å‰çš„ Inception æ¶æ„éƒ¨åˆ†å®šä¹‰\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m FRmodel = FaceRecoModel().to(device)\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# ä¿ç•™åŸæœ‰çš„ img_to_encoding å‡½æ•°ï¼ˆé€‚ç”¨äº 96x96 å›¾åƒï¼‰\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# æ­¤å‡½æ•°åœ¨å‰é¢çš„ Cell ä¸­å·²ç»å®šä¹‰è¿‡\u001b[39;00m\n\u001b[32m    119\u001b[39m \n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# è®¾ç½®è¯†åˆ«é˜ˆå€¼\u001b[39;00m\n\u001b[32m    121\u001b[39m THRESHOLD = \u001b[32m0.7\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'FaceRecoModel' is not defined"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 2: åŠ è½½æ¨¡å‹ä¸å®šä¹‰ç¼–ç å‡½æ•°ï¼ˆç»Ÿä¸€ç‰ˆæœ¬ï¼‰\n",
    "# ========================================\n",
    "\n",
    "if USE_PRETRAINED:\n",
    "    # ========== ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ ==========\n",
    "    try:\n",
    "        # ä» facenet_pytorch å¯¼å…¥ InceptionResnetV1 æ¨¡å‹\n",
    "        from facenet_pytorch import InceptionResnetV1\n",
    "        from PIL import Image\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"åŠ è½½é¢„è®­ç»ƒ FaceNet æ¨¡å‹\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # åˆ›å»º FaceNet æ¨¡å‹å®ä¾‹ï¼š\n",
    "        # pretrained='vggface2' è¡¨ç¤ºåŠ è½½åœ¨ VGGFace2 æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æƒé‡\n",
    "        # classify=False è¡¨ç¤ºä¸è¿›è¡Œåˆ†ç±»ï¼Œä»…æå–ç‰¹å¾å‘é‡\n",
    "        # device=device æŒ‡å®šè¿è¡Œè®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰\n",
    "        # ------------------------------------------------------------\n",
    "        FRmodel = InceptionResnetV1(\n",
    "            pretrained='vggface2',\n",
    "            classify=False,\n",
    "            device=device\n",
    "        ).eval()  # eval() è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼ï¼ˆå…³é—­ dropout / BN æ›´æ–°ï¼‰\n",
    "\n",
    "        print(f\"\\nâœ“ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "        print(f\"  - æ¨¡å‹: InceptionResnetV1\")\n",
    "        print(f\"  - æ•°æ®é›†: VGGFace2\")\n",
    "        print(f\"  - ç¼–ç ç»´åº¦: 512\")\n",
    "        print(f\"  - è®¾å¤‡: {device}\")\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # å®šä¹‰å›¾åƒç¼–ç å‡½æ•°ï¼šå°†å›¾åƒè½¬æ¢ä¸º 512 ç»´äººè„¸ç‰¹å¾å‘é‡\n",
    "        # ------------------------------------------------------------\n",
    "        def img_to_encoding(image_path, model):\n",
    "            \"\"\"\n",
    "            ä½¿ç”¨é¢„è®­ç»ƒ FaceNet æ¨¡å‹æå–äººè„¸ç¼–ç ï¼ˆ512ç»´ç‰¹å¾å‘é‡ï¼‰\n",
    "\n",
    "            å‚æ•°:\n",
    "                image_path: å›¾åƒè·¯å¾„ï¼ˆstrï¼‰\n",
    "                model: å·²åŠ è½½çš„ InceptionResnetV1 æ¨¡å‹å¯¹è±¡\n",
    "\n",
    "            è¿”å›:\n",
    "                embedding: numpy æ•°ç»„å½¢å¼çš„ 512 ç»´äººè„¸ç‰¹å¾å‘é‡\n",
    "            \"\"\"\n",
    "            # æ‰“å¼€å¹¶ç»Ÿä¸€è½¬æ¢ä¸º RGB æ¨¡å¼\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "            # FaceNet æ¨¡å‹é»˜è®¤è¾“å…¥å¤§å°ä¸º 160x160\n",
    "            img = img.resize((160, 160))\n",
    "\n",
    "            # è½¬æ¢ä¸º numpy æ•°ç»„\n",
    "            img_array = np.array(img)\n",
    "\n",
    "            # --------------------------------------------------------\n",
    "            # è½¬æ¢ä¸º PyTorch å¼ é‡:\n",
    "            # permute(2,0,1): å°†é€šé“é¡ºåºä» (H,W,C) â†’ (C,H,W)\n",
    "            # float(): è½¬æ¢ä¸ºæµ®ç‚¹ç±»å‹\n",
    "            # --------------------------------------------------------\n",
    "            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float()\n",
    "\n",
    "            # --------------------------------------------------------\n",
    "            # å½’ä¸€åŒ–åˆ° [-1, 1]ï¼Œä¸æ¨¡å‹è®­ç»ƒæ—¶ä¸€è‡´ï¼š\n",
    "            # (åƒç´ å€¼ [0,255] â†’ [-1,1]) å…¬å¼: (x - 127.5) / 128\n",
    "            # --------------------------------------------------------\n",
    "            img_tensor = (img_tensor - 127.5) / 128.0\n",
    "\n",
    "            # å¢åŠ  batch ç»´åº¦ â†’ (1, 3, 160, 160)\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            # å‰å‘ä¼ æ’­æå–ç‰¹å¾\n",
    "            model.eval()\n",
    "            with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒåŠ å¿«é€Ÿåº¦\n",
    "                embedding = model(img_tensor)\n",
    "\n",
    "            # è½¬ä¸º numpy æ•°ç»„è¿”å›\n",
    "            return embedding.cpu().numpy()\n",
    "\n",
    "        # è®¾ç½®è¯†åˆ«é˜ˆå€¼ï¼ˆ512ç»´ç¼–ç ä¸‹å¸¸ç”¨èŒƒå›´ 0.7~1.0ï¼‰\n",
    "        THRESHOLD = 0.8\n",
    "        ENCODING_DIM = 512\n",
    "\n",
    "        print(f\"\\nâœ“ ç¼–ç å‡½æ•°å·²é…ç½®\")\n",
    "        print(f\"  - å›¾åƒå°ºå¯¸: 160x160\")\n",
    "        print(f\"  - å½’ä¸€åŒ–: [-1, 1]\")\n",
    "        print(f\"  - è¯†åˆ«é˜ˆå€¼: {THRESHOLD}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "    except ImportError:\n",
    "        # ------------------------------------------------------------\n",
    "        # å¦‚æœæœªå®‰è£… facenet-pytorchï¼Œåˆ™æŠ¥é”™å¹¶æç¤ºå®‰è£…å‘½ä»¤\n",
    "        # ------------------------------------------------------------\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âŒ é”™è¯¯: æœªå®‰è£… facenet-pytorch\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"\\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:\")\n",
    "        print(\"  pip install facenet-pytorch\")\n",
    "        print(\"\\næˆ–è€…ä¿®æ”¹é…ç½®ä½¿ç”¨éšæœºæƒé‡æ¨¡å‹:\")\n",
    "        print(\"  USE_PRETRAINED = False\")\n",
    "        print(\"=\" * 70)\n",
    "        raise  # æŠ›å‡ºå¼‚å¸¸ç»ˆæ­¢ç¨‹åº\n",
    "\n",
    "else:\n",
    "    # ========== ä½¿ç”¨éšæœºæƒé‡æ¨¡å‹ ==========\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"åˆ›å»ºéšæœºæƒé‡æ¨¡å‹\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # åˆ›å»ºéšæœºæƒé‡çš„è‡ªå®šä¹‰æ¨¡å‹ï¼š\n",
    "    # FaceRecoModel ç±»åœ¨ä¹‹å‰çš„ Inception æ¶æ„éƒ¨åˆ†å®šä¹‰\n",
    "    # ------------------------------------------------------------\n",
    "    FRmodel = FaceRecoModel().to(device)\n",
    "\n",
    "    # ä¿ç•™åŸæœ‰çš„ img_to_encoding å‡½æ•°ï¼ˆé€‚ç”¨äº 96x96 å›¾åƒï¼‰\n",
    "    # æ­¤å‡½æ•°åœ¨å‰é¢çš„ Cell ä¸­å·²ç»å®šä¹‰è¿‡\n",
    "\n",
    "    # è®¾ç½®è¯†åˆ«é˜ˆå€¼\n",
    "    THRESHOLD = 0.7\n",
    "    ENCODING_DIM = 128\n",
    "\n",
    "    print(f\"\\nâœ“ éšæœºæƒé‡æ¨¡å‹åˆ›å»ºæˆåŠŸï¼\")\n",
    "    print(f\"  - æ¨¡å‹: FaceRecoModel (è‡ªå®šä¹‰Inceptionæ¶æ„)\")\n",
    "    print(f\"  - ç¼–ç ç»´åº¦: {ENCODING_DIM}\")\n",
    "    print(f\"  - å›¾åƒå°ºå¯¸: 96x96\")\n",
    "    print(f\"  - å½’ä¸€åŒ–: [0, 1]\")\n",
    "    print(f\"  - è¯†åˆ«é˜ˆå€¼: {THRESHOLD}\")\n",
    "    print(f\"  - è®¾å¤‡: {device}\")\n",
    "    print(f\"\\nâš ï¸  æ³¨æ„: éšæœºæƒé‡æ— æ³•è¿›è¡ŒçœŸå®çš„äººè„¸è¯†åˆ«\")\n",
    "    print(f\"âš ï¸  ä»…ç”¨äºå­¦ä¹ ç®—æ³•åŸç†ï¼ˆä¸‰å…ƒç»„æŸå¤±ç­‰æ¦‚å¿µï¼‰\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# ========================================\n",
    "# ç»Ÿä¸€è¾“å‡ºå½“å‰é…ç½®æ‘˜è¦\n",
    "# ========================================\n",
    "print(f\"\\nğŸ“Š å½“å‰é…ç½®æ‘˜è¦:\")\n",
    "print(f\"  â€¢ æ¨¡å‹ç±»å‹: {'é¢„è®­ç»ƒæ¨¡å‹' if USE_PRETRAINED else 'éšæœºæƒé‡'}\")\n",
    "print(f\"  â€¢ ç¼–ç ç»´åº¦: {ENCODING_DIM}\")\n",
    "print(f\"  â€¢ è¯†åˆ«é˜ˆå€¼: {THRESHOLD}\")\n",
    "print(f\"  â€¢ è®¾å¤‡: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ’¡ å¸¸è§åœºæ™¯ï¼š\n",
    "\n",
    "**å­¦ä¹ ç®—æ³•åŸç†** â†’ `USE_PRETRAINED = False` ï¼ˆéšæœºæƒé‡ï¼Œ128ç»´ï¼‰  \n",
    "**å®é™…è¯†åˆ«åº”ç”¨** â†’ `USE_PRETRAINED = True` ï¼ˆé¢„è®­ç»ƒï¼Œ512ç»´ï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ å¦‚ä½•åˆ‡æ¢æ¨¡å‹\n",
    "\n",
    "### ğŸ”„ åˆ‡æ¢æ­¥éª¤ï¼š\n",
    "\n",
    "1. **ä¿®æ”¹é…ç½®** - åœ¨ä¸Šé¢çš„é…ç½® Cell ä¸­ä¿®æ”¹ `USE_PRETRAINED` çš„å€¼\n",
    "   ```python\n",
    "   USE_PRETRAINED = True   # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹\n",
    "   # æˆ–\n",
    "   USE_PRETRAINED = False  # ä½¿ç”¨éšæœºæƒé‡\n",
    "   ```\n",
    "\n",
    "2. **é‡æ–°è¿è¡Œ** - æŒ‰é¡ºåºé‡æ–°è¿è¡Œä»¥ä¸‹ Cellï¼š\n",
    "   - âœ… é…ç½® Cellï¼ˆä¿®æ”¹åçš„é‚£ä¸ªï¼‰\n",
    "   - âœ… æ¨¡å‹åŠ è½½ Cellï¼ˆä¼šæ ¹æ®é…ç½®åŠ è½½ç›¸åº”æ¨¡å‹ï¼‰\n",
    "   - âœ… æ•°æ®åº“åˆ›å»º Cell\n",
    "   - âœ… æµ‹è¯• Cell\n",
    "\n",
    "### ğŸ’¡ ä½¿ç”¨å»ºè®®\n",
    "\n",
    "#### å­¦ä¹ é˜¶æ®µï¼ˆæ¨èéšæœºæƒé‡ï¼‰ï¼š\n",
    "- âœ… ç†è§£ä¸‰å…ƒç»„æŸå¤±çš„å·¥ä½œåŸç†\n",
    "- âœ… å­¦ä¹ ç¼–ç ã€è·ç¦»è®¡ç®—ç­‰æ¦‚å¿µ\n",
    "- âœ… å¿«é€Ÿå®éªŒï¼Œæ— éœ€ç­‰å¾…ä¸‹è½½\n",
    "\n",
    "#### å®é™…åº”ç”¨ï¼ˆæ¨èé¢„è®­ç»ƒæ¨¡å‹ï¼‰ï¼š\n",
    "- âœ… è·å¾—çœŸå®çš„è¯†åˆ«æ•ˆæœ\n",
    "- âœ… ä½“éªŒå®ç”¨çš„äººè„¸è¯†åˆ«\n",
    "- âœ… ç”¨äºé¡¹ç›®æˆ–æ¼”ç¤º\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‘‡ ç»§ç»­å¾€ä¸‹è¿è¡Œ Cell å³å¯ä½¿ç”¨é€‰å®šçš„æ¨¡å‹**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - å°†äººè„¸å›¾åƒç¼–ç ä¸º128ç»´å‘é‡\n",
    "\n",
    "### 1.1 - ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetï¼‰è®¡ç®—ç¼–ç \n",
    "\n",
    "FaceNet æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®å’Œå¾ˆé•¿çš„è®­ç»ƒæ—¶é—´ã€‚å› æ­¤ï¼ŒæŒ‰ç…§åº”ç”¨æ·±åº¦å­¦ä¹ çš„å¸¸ç”¨åšæ³•ï¼Œæˆ‘ä»¬ç›´æ¥åŠ è½½åˆ«äººå·²ç»è®­ç»ƒå¥½çš„æƒé‡ã€‚è¯¥ç½‘ç»œæ¶æ„éµå¾ª [Szegedy *et al.*](https://arxiv.org/abs/1409.4842) çš„ Inception æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»æä¾›äº†ä¸€ä¸ª Inception ç½‘ç»œçš„å®ç°ã€‚ä½ å¯ä»¥åœ¨æ–‡ä»¶ `inception_blocks.py` ä¸­æŸ¥çœ‹å…·ä½“å®ç°ï¼ˆåœ¨ Jupyter Notebook é¡¶éƒ¨ç‚¹å‡» \"File->Open...\" æ¥æŸ¥çœ‹ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Inception æ¨¡å‹å®šä¹‰å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ========== PyTorch Inception ç½‘ç»œæ¨¡å‹å®ç° ==========\n",
    "# \n",
    "# Inception æ¶æ„é€šè¿‡å¹¶è¡Œçš„å¤šä¸ªå·ç§¯åˆ†æ”¯æ¥æå–ä¸åŒå°ºåº¦çš„ç‰¹å¾\n",
    "# æ¯ä¸ª InceptionBlock åŒ…å«å¤šä¸ªåˆ†æ”¯ï¼š1x1, 3x3, 5x5 å·ç§¯å’Œæ± åŒ–æ“ä½œ\n",
    "# \n",
    "# å‚è€ƒæ–‡çŒ®: Szegedy et al. \"Going Deeper with Convolutions\" (2015)\n",
    "\n",
    "class InceptionBlock1a(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception Block 1a - ç¬¬ä¸€ä¸ªInceptionæ¨¡å—\n",
    "    \n",
    "    åŒ…å«4ä¸ªå¹¶è¡Œåˆ†æ”¯:\n",
    "    - 3x3 å·ç§¯åˆ†æ”¯: ç”¨äºæå–ä¸­ç­‰å°ºåº¦ç‰¹å¾\n",
    "    - 5x5 å·ç§¯åˆ†æ”¯: ç”¨äºæå–æ›´å¤§å°ºåº¦ç‰¹å¾  \n",
    "    - æ± åŒ–åˆ†æ”¯: ç”¨äºä¸‹é‡‡æ ·å’Œç‰¹å¾èšåˆ\n",
    "    - 1x1 å·ç§¯åˆ†æ”¯: ç”¨äºé™ç»´å’Œå¢åŠ éçº¿æ€§\n",
    "    \n",
    "    å‚æ•°:\n",
    "        in_channels (int): è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlock1a, self).__init__()\n",
    "        \n",
    "        # 3x3 branch\n",
    "        self.branch_3x3_conv1 = nn.Conv2d(in_channels, 96, kernel_size=1)\n",
    "        self.branch_3x3_bn1 = nn.BatchNorm2d(96, eps=0.00001)\n",
    "        self.branch_3x3_conv2 = nn.Conv2d(96, 128, kernel_size=3, padding=1)\n",
    "        self.branch_3x3_bn2 = nn.BatchNorm2d(128, eps=0.00001)\n",
    "        \n",
    "        # 5x5 branch\n",
    "        self.branch_5x5_conv1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch_5x5_bn1 = nn.BatchNorm2d(16, eps=0.00001)\n",
    "        self.branch_5x5_conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.branch_5x5_bn2 = nn.BatchNorm2d(32, eps=0.00001)\n",
    "        \n",
    "        # pool branch\n",
    "        self.branch_pool_conv = nn.Conv2d(in_channels, 32, kernel_size=1)\n",
    "        self.branch_pool_bn = nn.BatchNorm2d(32, eps=0.00001)\n",
    "        \n",
    "        # 1x1 branch\n",
    "        self.branch_1x1_conv = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch_1x1_bn = nn.BatchNorm2d(64, eps=0.00001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3x3 branch\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn1(self.branch_3x3_conv1(x)))\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn2(self.branch_3x3_conv2(branch_3x3)))\n",
    "        \n",
    "        # 5x5 branch\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn1(self.branch_5x5_conv1(x)))\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn2(self.branch_5x5_conv2(branch_5x5)))\n",
    "        \n",
    "        # pool branch - ä¿®æ­£paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "        branch_pool = F.relu(self.branch_pool_bn(self.branch_pool_conv(branch_pool)))\n",
    "        # åŠ¨æ€è®¡ç®—paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        pad_h = x.size(2) - branch_pool.size(2)\n",
    "        pad_w = x.size(3) - branch_pool.size(3)\n",
    "        branch_pool = F.pad(branch_pool, (pad_w//2, pad_w-pad_w//2, pad_h//2, pad_h-pad_h//2))\n",
    "        \n",
    "        # 1x1 branch\n",
    "        branch_1x1 = F.relu(self.branch_1x1_bn(self.branch_1x1_conv(x)))\n",
    "        \n",
    "        # Concatenate\n",
    "        outputs = torch.cat([branch_3x3, branch_5x5, branch_pool, branch_1x1], dim=1)\n",
    "        return outputs\n",
    "\n",
    "class InceptionBlock1b(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlock1b, self).__init__()\n",
    "        \n",
    "        # 3x3 branch\n",
    "        self.branch_3x3_conv1 = nn.Conv2d(in_channels, 96, kernel_size=1)\n",
    "        self.branch_3x3_bn1 = nn.BatchNorm2d(96, eps=0.00001)\n",
    "        self.branch_3x3_conv2 = nn.Conv2d(96, 128, kernel_size=3, padding=1)\n",
    "        self.branch_3x3_bn2 = nn.BatchNorm2d(128, eps=0.00001)\n",
    "        \n",
    "        # 5x5 branch\n",
    "        self.branch_5x5_conv1 = nn.Conv2d(in_channels, 32, kernel_size=1)\n",
    "        self.branch_5x5_bn1 = nn.BatchNorm2d(32, eps=0.00001)\n",
    "        self.branch_5x5_conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.branch_5x5_bn2 = nn.BatchNorm2d(64, eps=0.00001)\n",
    "        \n",
    "        # pool branch\n",
    "        self.branch_pool_conv = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch_pool_bn = nn.BatchNorm2d(64, eps=0.00001)\n",
    "        \n",
    "        # 1x1 branch\n",
    "        self.branch_1x1_conv = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch_1x1_bn = nn.BatchNorm2d(64, eps=0.00001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3x3 branch\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn1(self.branch_3x3_conv1(x)))\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn2(self.branch_3x3_conv2(branch_3x3)))\n",
    "        \n",
    "        # 5x5 branch\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn1(self.branch_5x5_conv1(x)))\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn2(self.branch_5x5_conv2(branch_5x5)))\n",
    "        \n",
    "        # pool branch - ä¿®æ­£paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=3, padding=1)\n",
    "        branch_pool = F.relu(self.branch_pool_bn(self.branch_pool_conv(branch_pool)))\n",
    "        # åŠ¨æ€è®¡ç®—paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        pad_h = x.size(2) - branch_pool.size(2)\n",
    "        pad_w = x.size(3) - branch_pool.size(3)\n",
    "        branch_pool = F.pad(branch_pool, (pad_w//2, pad_w-pad_w//2, pad_h//2, pad_h-pad_h//2))\n",
    "        \n",
    "        # 1x1 branch\n",
    "        branch_1x1 = F.relu(self.branch_1x1_bn(self.branch_1x1_conv(x)))\n",
    "        \n",
    "        # Concatenate\n",
    "        outputs = torch.cat([branch_3x3, branch_5x5, branch_pool, branch_1x1], dim=1)\n",
    "        return outputs\n",
    "\n",
    "class InceptionBlock1c(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlock1c, self).__init__()\n",
    "        \n",
    "        # 3x3 branch\n",
    "        self.branch_3x3_conv1 = nn.Conv2d(in_channels, 128, kernel_size=1)\n",
    "        self.branch_3x3_bn1 = nn.BatchNorm2d(128, eps=0.00001)\n",
    "        self.branch_3x3_conv2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.branch_3x3_bn2 = nn.BatchNorm2d(256, eps=0.00001)\n",
    "        \n",
    "        # 5x5 branch\n",
    "        self.branch_5x5_conv1 = nn.Conv2d(in_channels, 32, kernel_size=1)\n",
    "        self.branch_5x5_bn1 = nn.BatchNorm2d(32, eps=0.00001)\n",
    "        self.branch_5x5_conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.branch_5x5_bn2 = nn.BatchNorm2d(64, eps=0.00001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3x3 branch\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn1(self.branch_3x3_conv1(x)))\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn2(self.branch_3x3_conv2(branch_3x3)))\n",
    "        \n",
    "        # 5x5 branch\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn1(self.branch_5x5_conv1(x)))\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn2(self.branch_5x5_conv2(branch_5x5)))\n",
    "        \n",
    "        # pool branch\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        branch_pool = F.pad(branch_pool, (0, 1, 0, 1))\n",
    "        \n",
    "        # Concatenate\n",
    "        outputs = torch.cat([branch_3x3, branch_5x5, branch_pool], dim=1)\n",
    "        return outputs\n",
    "\n",
    "class InceptionBlock2a(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlock2a, self).__init__()\n",
    "        \n",
    "        # 3x3 branch\n",
    "        self.branch_3x3_conv1 = nn.Conv2d(in_channels, 96, kernel_size=1)\n",
    "        self.branch_3x3_bn1 = nn.BatchNorm2d(96, eps=0.00001)\n",
    "        self.branch_3x3_conv2 = nn.Conv2d(96, 192, kernel_size=3, padding=1)\n",
    "        self.branch_3x3_bn2 = nn.BatchNorm2d(192, eps=0.00001)\n",
    "        \n",
    "        # 5x5 branch\n",
    "        self.branch_5x5_conv1 = nn.Conv2d(in_channels, 32, kernel_size=1)\n",
    "        self.branch_5x5_bn1 = nn.BatchNorm2d(32, eps=0.00001)\n",
    "        self.branch_5x5_conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.branch_5x5_bn2 = nn.BatchNorm2d(64, eps=0.00001)\n",
    "        \n",
    "        # pool branch\n",
    "        self.branch_pool_conv = nn.Conv2d(in_channels, 128, kernel_size=1)\n",
    "        self.branch_pool_bn = nn.BatchNorm2d(128, eps=0.00001)\n",
    "        \n",
    "        # 1x1 branch\n",
    "        self.branch_1x1_conv = nn.Conv2d(in_channels, 256, kernel_size=1)\n",
    "        self.branch_1x1_bn = nn.BatchNorm2d(256, eps=0.00001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3x3 branch\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn1(self.branch_3x3_conv1(x)))\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn2(self.branch_3x3_conv2(branch_3x3)))\n",
    "        \n",
    "        # 5x5 branch\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn1(self.branch_5x5_conv1(x)))\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn2(self.branch_5x5_conv2(branch_5x5)))\n",
    "        \n",
    "        # pool branch - ä¿®æ­£paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=3, padding=1)\n",
    "        branch_pool = F.relu(self.branch_pool_bn(self.branch_pool_conv(branch_pool)))\n",
    "        # åŠ¨æ€è®¡ç®—paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        pad_h = x.size(2) - branch_pool.size(2)\n",
    "        pad_w = x.size(3) - branch_pool.size(3)\n",
    "        branch_pool = F.pad(branch_pool, (pad_w//2, pad_w-pad_w//2, pad_h//2, pad_h-pad_h//2))\n",
    "        \n",
    "        # 1x1 branch\n",
    "        branch_1x1 = F.relu(self.branch_1x1_bn(self.branch_1x1_conv(x)))\n",
    "        \n",
    "        # Concatenate\n",
    "        outputs = torch.cat([branch_3x3, branch_5x5, branch_pool, branch_1x1], dim=1)\n",
    "        return outputs\n",
    "\n",
    "class InceptionBlock2b(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlock2b, self).__init__()\n",
    "        \n",
    "        # 3x3 branch\n",
    "        self.branch_3x3_conv1 = nn.Conv2d(in_channels, 160, kernel_size=1)\n",
    "        self.branch_3x3_bn1 = nn.BatchNorm2d(160, eps=0.00001)\n",
    "        self.branch_3x3_conv2 = nn.Conv2d(160, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.branch_3x3_bn2 = nn.BatchNorm2d(256, eps=0.00001)\n",
    "        \n",
    "        # 5x5 branch\n",
    "        self.branch_5x5_conv1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch_5x5_bn1 = nn.BatchNorm2d(64, eps=0.00001)\n",
    "        self.branch_5x5_conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.branch_5x5_bn2 = nn.BatchNorm2d(128, eps=0.00001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3x3 branch\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn1(self.branch_3x3_conv1(x)))\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn2(self.branch_3x3_conv2(branch_3x3)))\n",
    "        \n",
    "        # 5x5 branch\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn1(self.branch_5x5_conv1(x)))\n",
    "        branch_5x5 = F.relu(self.branch_5x5_bn2(self.branch_5x5_conv2(branch_5x5)))\n",
    "        \n",
    "        # pool branch\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        branch_pool = F.pad(branch_pool, (0, 1, 0, 1))\n",
    "        \n",
    "        # Concatenate\n",
    "        outputs = torch.cat([branch_3x3, branch_5x5, branch_pool], dim=1)\n",
    "        return outputs\n",
    "\n",
    "class InceptionBlock3a(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlock3a, self).__init__()\n",
    "        \n",
    "        # 3x3 branch\n",
    "        self.branch_3x3_conv1 = nn.Conv2d(in_channels, 96, kernel_size=1)\n",
    "        self.branch_3x3_bn1 = nn.BatchNorm2d(96, eps=0.00001)\n",
    "        self.branch_3x3_conv2 = nn.Conv2d(96, 384, kernel_size=3, padding=1)\n",
    "        self.branch_3x3_bn2 = nn.BatchNorm2d(384, eps=0.00001)\n",
    "        \n",
    "        # pool branch\n",
    "        self.branch_pool_conv = nn.Conv2d(in_channels, 96, kernel_size=1)\n",
    "        self.branch_pool_bn = nn.BatchNorm2d(96, eps=0.00001)\n",
    "        \n",
    "        # 1x1 branch\n",
    "        self.branch_1x1_conv = nn.Conv2d(in_channels, 256, kernel_size=1)\n",
    "        self.branch_1x1_bn = nn.BatchNorm2d(256, eps=0.00001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3x3 branch\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn1(self.branch_3x3_conv1(x)))\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn2(self.branch_3x3_conv2(branch_3x3)))\n",
    "        \n",
    "        # pool branch - ä¿®æ­£paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=3, padding=1)\n",
    "        branch_pool = F.relu(self.branch_pool_bn(self.branch_pool_conv(branch_pool)))\n",
    "        # åŠ¨æ€è®¡ç®—paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        pad_h = x.size(2) - branch_pool.size(2)\n",
    "        pad_w = x.size(3) - branch_pool.size(3)\n",
    "        branch_pool = F.pad(branch_pool, (pad_w//2, pad_w-pad_w//2, pad_h//2, pad_h-pad_h//2))\n",
    "        \n",
    "        # 1x1 branch\n",
    "        branch_1x1 = F.relu(self.branch_1x1_bn(self.branch_1x1_conv(x)))\n",
    "        \n",
    "        # Concatenate\n",
    "        outputs = torch.cat([branch_3x3, branch_pool, branch_1x1], dim=1)\n",
    "        return outputs\n",
    "\n",
    "class InceptionBlock3b(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlock3b, self).__init__()\n",
    "        \n",
    "        # 3x3 branch\n",
    "        self.branch_3x3_conv1 = nn.Conv2d(in_channels, 96, kernel_size=1)\n",
    "        self.branch_3x3_bn1 = nn.BatchNorm2d(96, eps=0.00001)\n",
    "        self.branch_3x3_conv2 = nn.Conv2d(96, 384, kernel_size=3, padding=1)\n",
    "        self.branch_3x3_bn2 = nn.BatchNorm2d(384, eps=0.00001)\n",
    "        \n",
    "        # pool branch\n",
    "        self.branch_pool_conv = nn.Conv2d(in_channels, 96, kernel_size=1)\n",
    "        self.branch_pool_bn = nn.BatchNorm2d(96, eps=0.00001)\n",
    "        \n",
    "        # 1x1 branch\n",
    "        self.branch_1x1_conv = nn.Conv2d(in_channels, 256, kernel_size=1)\n",
    "        self.branch_1x1_bn = nn.BatchNorm2d(256, eps=0.00001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 3x3 branch\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn1(self.branch_3x3_conv1(x)))\n",
    "        branch_3x3 = F.relu(self.branch_3x3_bn2(self.branch_3x3_conv2(branch_3x3)))\n",
    "        \n",
    "        # pool branch - ä¿®æ­£paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "        branch_pool = F.relu(self.branch_pool_bn(self.branch_pool_conv(branch_pool)))\n",
    "        # åŠ¨æ€è®¡ç®—paddingä»¥åŒ¹é…å…¶ä»–åˆ†æ”¯çš„å°ºå¯¸\n",
    "        pad_h = x.size(2) - branch_pool.size(2)\n",
    "        pad_w = x.size(3) - branch_pool.size(3)\n",
    "        branch_pool = F.pad(branch_pool, (pad_w//2, pad_w-pad_w//2, pad_h//2, pad_h-pad_h//2))\n",
    "        \n",
    "        # 1x1 branch\n",
    "        branch_1x1 = F.relu(self.branch_1x1_bn(self.branch_1x1_conv(x)))\n",
    "        \n",
    "        # Concatenate\n",
    "        outputs = torch.cat([branch_3x3, branch_pool, branch_1x1], dim=1)\n",
    "        return outputs\n",
    "\n",
    "class FaceRecoModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceRecoModel, self).__init__()\n",
    "        \n",
    "        # Initial layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64, eps=0.00001)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64, eps=0.00001)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(192, eps=0.00001)\n",
    "        \n",
    "        # Inception blocks\n",
    "        self.inception_1a = InceptionBlock1a(192)\n",
    "        self.inception_1b = InceptionBlock1b(256)\n",
    "        self.inception_1c = InceptionBlock1c(320)\n",
    "        \n",
    "        self.inception_2a = InceptionBlock2a(640)\n",
    "        self.inception_2b = InceptionBlock2b(640)\n",
    "        \n",
    "        self.inception_3a = InceptionBlock3a(1024)\n",
    "        self.inception_3b = InceptionBlock3b(736)\n",
    "        \n",
    "        # Final layers\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=1)\n",
    "        # å…¨è¿æ¥å±‚å°†åœ¨ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­æ—¶åˆå§‹åŒ–\n",
    "        self.fc = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.pad(x, (3, 3, 3, 3))\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.pad(x, (1, 1, 1, 1))\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.pad(x, (1, 1, 1, 1))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.pad(x, (1, 1, 1, 1))\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        \n",
    "        # Inception blocks\n",
    "        x = self.inception_1a(x)\n",
    "        x = self.inception_1b(x)\n",
    "        x = self.inception_1c(x)\n",
    "        \n",
    "        x = self.inception_2a(x)\n",
    "        x = self.inception_2b(x)\n",
    "        \n",
    "        x = self.inception_3a(x)\n",
    "        x = self.inception_3b(x)\n",
    "        \n",
    "        # Final layers\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # åŠ¨æ€åˆå§‹åŒ–å…¨è¿æ¥å±‚\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(x.size(1), 128).to(x.device)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # L2 normalization\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"PyTorch Inception æ¨¡å‹å®šä¹‰å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FRmodel å·²å°±ç»ª: FaceRecoModel\n",
      "  è®¾å¤‡: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========== å›¾åƒé¢„å¤„ç†å’Œç¼–ç æå–å‡½æ•° ==========\n",
    "\n",
    "def img_to_encoding(image_path, model):\n",
    "    \"\"\"\n",
    "    å°†å›¾åƒè½¬æ¢ä¸º128ç»´äººè„¸ç¼–ç å‘é‡\n",
    "    \n",
    "    å·¥ä½œæµç¨‹:\n",
    "    1. ä½¿ç”¨OpenCVè¯»å–å›¾åƒ\n",
    "    2. å°†BGRæ ¼å¼è½¬æ¢ä¸ºRGBæ ¼å¼\n",
    "    3. å½’ä¸€åŒ–åƒç´ å€¼åˆ°[0,1]åŒºé—´\n",
    "    4. è½¬æ¢ç»´åº¦é¡ºåºä¸º(C,H,W)æ ¼å¼\n",
    "    5. é€šè¿‡æ¨¡å‹å‰å‘ä¼ æ’­è·å¾—ç¼–ç \n",
    "    \n",
    "    å‚æ•°:\n",
    "        image_path (str): å›¾åƒæ–‡ä»¶çš„è·¯å¾„\n",
    "        model (nn.Module): å·²è®­ç»ƒçš„äººè„¸è¯†åˆ«æ¨¡å‹\n",
    "    \n",
    "    è¿”å›:\n",
    "        encoding (numpy.ndarray): å½¢çŠ¶ä¸º(1, 128)çš„ç¼–ç å‘é‡\n",
    "    \n",
    "    å¼‚å¸¸:\n",
    "        FileNotFoundError: å½“å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è¯»å–æ—¶æŠ›å‡º\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨OpenCVè¯»å–å›¾åƒ\n",
    "    # å‚æ•°1è¡¨ç¤ºä»¥å½©è‰²æ¨¡å¼è¯»å–å›¾åƒ(è¿”å›BGRæ ¼å¼)\n",
    "    img = cv2.imread(image_path, 1)\n",
    "    \n",
    "    # æ£€æŸ¥å›¾åƒæ˜¯å¦æˆåŠŸåŠ è½½\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"æ— æ³•è¯»å–å›¾åƒ: {image_path}\\nè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼\")\n",
    "    \n",
    "    # å›¾åƒé¢„å¤„ç†\n",
    "    img = img[..., ::-1]  # å°†BGRæ ¼å¼è½¬æ¢ä¸ºRGBæ ¼å¼(åè½¬æœ€åä¸€ä¸ªç»´åº¦)\n",
    "    \n",
    "    # è½¬æ¢ç»´åº¦é¡ºåº: (H, W, C) -> (C, H, W)ï¼Œå¹¶å½’ä¸€åŒ–åˆ°[0,1]\n",
    "    # np.transpose(img, (2,0,1)): å°†é€šé“ç»´åº¦ç§»åˆ°æœ€å‰é¢\n",
    "    # / 255.0: å°†åƒç´ å€¼ä»[0,255]å½’ä¸€åŒ–åˆ°[0,1]\n",
    "    # decimals=12: ä¿ç•™12ä½å°æ•°ç²¾åº¦\n",
    "    img = np.around(np.transpose(img, (2, 0, 1)) / 255.0, decimals=12)\n",
    "    \n",
    "    # è½¬æ¢ä¸º PyTorch tensor\n",
    "    # torch.from_numpy(): ä»numpyæ•°ç»„åˆ›å»ºtensor\n",
    "    # .float(): è½¬æ¢ä¸ºæµ®ç‚¹ç±»å‹\n",
    "    # .unsqueeze(0): åœ¨ç¬¬0ç»´å¢åŠ batchç»´åº¦ï¼Œå½¢çŠ¶å˜ä¸º(1, C, H, W)\n",
    "    # .to(device): å°†tensorç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡(CPUæˆ–GPU)\n",
    "    x = torch.from_numpy(img).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    # æ¨¡å‹æ¨ç†\n",
    "    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼(å…³é—­dropoutç­‰)\n",
    "    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å’ŒåŠ é€Ÿ\n",
    "        embedding = model(x)  # å‰å‘ä¼ æ’­è·å¾—ç¼–ç å‘é‡\n",
    "    \n",
    "    # å°†ç»“æœè½¬å›numpyæ•°ç»„å¹¶è¿”å›\n",
    "    # .cpu(): å°†tensorç§»å›CPU\n",
    "    # .numpy(): è½¬æ¢ä¸ºnumpyæ•°ç»„\n",
    "    return embedding.cpu().numpy()\n",
    "\n",
    "\n",
    "# ========== æ¨¡å‹å®ä¾‹åˆ›å»º ==========\n",
    "# âš ï¸  é‡è¦æç¤ºï¼šè¯·ä¸è¦è¿è¡Œæ­¤ Cellï¼\n",
    "# FRmodel å·²ç»åœ¨\"æ¨¡å‹é…ç½®é€‰é¡¹\"éƒ¨åˆ†æ ¹æ®æ‚¨çš„é€‰æ‹©ï¼ˆUSE_PRETRAINEDï¼‰è‡ªåŠ¨åˆ›å»º\n",
    "# \n",
    "# å¦‚æœæ‚¨ä½¿ç”¨äº†é…ç½®ç³»ç»Ÿï¼ˆæ¨èï¼‰ï¼ŒFRmodel å·²ç»æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š\n",
    "#   - é¢„è®­ç»ƒæ¨¡å‹ï¼ˆInceptionResnetV1, 512ç»´ï¼‰- å¦‚æœ USE_PRETRAINED = True\n",
    "#   - éšæœºæƒé‡æ¨¡å‹ï¼ˆFaceRecoModel, 128ç»´ï¼‰- å¦‚æœ USE_PRETRAINED = False\n",
    "#\n",
    "# è¿è¡Œä¸‹é¢è¿™è¡Œä¼šè¦†ç›–é…ç½®ï¼Œå¯¼è‡´å³ä½¿é€‰æ‹©äº†é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿä¼šè¢«æ›¿æ¢ä¸ºéšæœºæƒé‡ï¼\n",
    "# \n",
    "# å¦‚æœæ‚¨ç¡®å®éœ€è¦æ‰‹åŠ¨åˆ›å»ºéšæœºæƒé‡æ¨¡å‹ï¼ˆä¸æ¨èï¼‰ï¼Œè¯·å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼š\n",
    "# FRmodel = FaceRecoModel().to(device)\n",
    "# print(\"æ‰‹åŠ¨åˆ›å»ºéšæœºæƒé‡æ¨¡å‹å®Œæˆ\")\n",
    "\n",
    "# éªŒè¯ FRmodel å·²å­˜åœ¨\n",
    "try:\n",
    "    print(f\"âœ“ FRmodel å·²å°±ç»ª: {type(FRmodel).__name__}\")\n",
    "    print(f\"  è®¾å¤‡: {next(FRmodel.parameters()).device}\")\n",
    "except NameError:\n",
    "    print(\"âŒ é”™è¯¯: FRmodel æœªå®šä¹‰ï¼\")\n",
    "    print(\"   è¯·å…ˆè¿è¡Œé…ç½® Cellï¼ˆCell 10-12ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ éœ€è¦äº†è§£çš„å…³é”®ç‚¹ï¼š\n",
    "\n",
    "- è¯¥ç½‘ç»œä»¥ 96x96 çš„ RGB å›¾åƒä½œä¸ºè¾“å…¥ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†ä¸€å¼ äººè„¸å›¾åƒï¼ˆæˆ–ä¸€ä¸ªåŒ…å« $m$ å¼ äººè„¸å›¾åƒçš„æ‰¹æ¬¡ï¼‰ä½œä¸ºå¼ é‡è¾“å…¥ï¼Œå½¢çŠ¶ä¸º $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$\n",
    "- è¾“å‡ºä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º $(m, 128)$ çš„çŸ©é˜µï¼Œå°†æ¯å¼ è¾“å…¥çš„äººè„¸å›¾åƒç¼–ç ä¸ºä¸€ä¸ª 128 ç»´çš„å‘é‡\n",
    "\n",
    "è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼ä»¥åˆ›å»ºç”¨äºäººè„¸å›¾åƒçš„æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰æ¨¡å‹: FaceRecoModel\n"
     ]
    }
   ],
   "source": [
    "# ========== æ¨¡å‹çŠ¶æ€æ£€æŸ¥ ==========\n",
    "# FRmodel å·²åœ¨é…ç½®ç³»ç»Ÿï¼ˆCell 10-12ï¼‰ä¸­æ ¹æ® USE_PRETRAINED è‡ªåŠ¨åˆ›å»º\n",
    "# \n",
    "# æ¨¡å‹ç±»å‹å–å†³äºæ‚¨çš„é…ç½®ï¼š\n",
    "#   USE_PRETRAINED = True  â†’ InceptionResnetV1ï¼ˆé¢„è®­ç»ƒï¼Œ512ç»´ï¼‰\n",
    "#   USE_PRETRAINED = False â†’ FaceRecoModelï¼ˆéšæœºæƒé‡ï¼Œ128ç»´ï¼‰\n",
    "#\n",
    "# å¦‚æœæ‚¨çœ‹åˆ°é”™è¯¯ï¼Œè¯·ç¡®ä¿å·²æŒ‰é¡ºåºè¿è¡Œï¼š\n",
    "#   1. Cell 10ï¼ˆé…ç½®é€‰æ‹©ï¼‰\n",
    "#   2. Cell 12ï¼ˆæ¨¡å‹åŠ è½½ï¼‰\n",
    "\n",
    "print(f\"å½“å‰æ¨¡å‹: {type(FRmodel).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3639632\n"
     ]
    }
   ],
   "source": [
    "# è®¡ç®—æ¨¡å‹å‚æ•°æ€»æ•°\n",
    "total_params = sum(p.numel() for p in FRmodel.parameters())\n",
    "print(\"Total Params:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output **\n",
    "<table>\n",
    "<center>\n",
    "Total Params: çº¦ 3.7Mï¼ˆPyTorchæ¨¡å‹å‚æ•°æ•°é‡å¯èƒ½ç•¥æœ‰ä¸åŒï¼‰\n",
    "</center>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡ä½¿ç”¨ä¸€ä¸ªå…·æœ‰ 128 ä¸ªç¥ç»å…ƒçš„å…¨è¿æ¥å±‚ä½œä¸ºæœ€åä¸€å±‚ï¼Œæ¨¡å‹ä¿è¯è¾“å‡ºæ˜¯ä¸€ä¸ª 128 ç»´çš„ç¼–ç å‘é‡ã€‚ä½ å¯ä»¥ä½¿ç”¨è¿™äº›ç¼–ç æ¥æ¯”è¾ƒä¸¤å¼ äººè„¸å›¾åƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "\n",
    "<img src=\"images/distance_kiank.png\" style=\"width:680px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **å›¾ 2**: <br> </u> <font color='purple'> é€šè¿‡è®¡ç®—ä¸¤ä¸ªç¼–ç ä¹‹é—´çš„è·ç¦»å¹¶è®¾å®šé˜ˆå€¼ï¼Œä½ å¯ä»¥åˆ¤æ–­è¿™ä¸¤å¼ å›¾ç‰‡æ˜¯å¦å±äºåŒä¸€ä¸ªäºº </center></caption>\n",
    "\n",
    "å› æ­¤ï¼Œä¸€ä¸ªå¥½çš„ç¼–ç åº”æ»¡è¶³ï¼š\n",
    "- åŒä¸€ä¸ªäººçš„ä¸¤å¼ å›¾åƒçš„ç¼–ç åº”è¯¥å½¼æ­¤éå¸¸ç›¸ä¼¼\n",
    "- ä¸åŒäººçš„ä¸¤å¼ å›¾åƒçš„ç¼–ç åº”è¯¥ç›¸å·®å¾ˆå¤§\n",
    "\n",
    "ä¸‰å…ƒç»„æŸå¤±å‡½æ•°ï¼ˆtriplet lossï¼‰å°†è¿™ä¸€æ€æƒ³å½¢å¼åŒ–ï¼Œå®ƒè¯•å›¾â€œæ‹‰è¿‘â€åŒä¸€äººçš„ä¸¤å¼ å›¾åƒï¼ˆAnchor å’Œ Positiveï¼‰çš„ç¼–ç ï¼ŒåŒæ—¶â€œæ¨è¿œâ€ä¸åŒäººçš„ä¸¤å¼ å›¾åƒï¼ˆAnchor å’Œ Negativeï¼‰çš„ç¼–ç ã€‚\n",
    "\n",
    "<img src=\"images/triplet_comparison.png\" style=\"width:280px;height:150px;\">\n",
    "<br>\n",
    "<caption><center> <u> <font color='purple'> **å›¾ 3**: <br> </u> <font color='purple'> åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»å·¦åˆ°å³ç§°è¿™ä¸‰å¼ å›¾åƒä¸ºï¼šAnchor (A), Positive (P), Negative (N)  </center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - ä¸‰å…ƒç»„æŸå¤±ï¼ˆTriplet Lossï¼‰\n",
    "\n",
    "å¯¹äºä¸€å¼ å›¾åƒ $x$ï¼Œè®°å…¶ç¼–ç ä¸º $f(x)$ï¼Œå…¶ä¸­ $f$ æ˜¯ç¥ç»ç½‘ç»œè®¡ç®—çš„å‡½æ•°ã€‚\n",
    "\n",
    "<img src=\"images/f_x.png\" style=\"width:380px;height:150px;\">\n",
    "\n",
    "è®­ç»ƒæ—¶ä½¿ç”¨ä¸‰å…ƒç»„å›¾åƒ $(A, P, N)$ï¼š  \n",
    "\n",
    "- **Aï¼ˆAnchorï¼‰**ï¼šåŸºå‡†å›¾åƒï¼Œå³æŸä¸ªäººçš„ç…§ç‰‡ã€‚\n",
    "- **Pï¼ˆPositiveï¼‰**ï¼šæ­£æ ·æœ¬å›¾åƒï¼Œä¸ Anchor æ˜¯åŒä¸€ä¸ªäººã€‚\n",
    "- **Nï¼ˆNegativeï¼‰**ï¼šè´Ÿæ ·æœ¬å›¾åƒï¼Œä¸ Anchor æ˜¯ä¸åŒçš„äººã€‚\n",
    "\n",
    "è¿™äº›ä¸‰å…ƒç»„ä»è®­ç»ƒæ•°æ®é›†ä¸­é€‰å–ã€‚ç”¨ $(A^{(i)}, P^{(i)}, N^{(i)})$ è¡¨ç¤ºç¬¬ $i$ ä¸ªè®­ç»ƒæ ·æœ¬ã€‚  \n",
    "\n",
    "æˆ‘ä»¬å¸Œæœ›ç¡®ä¿å›¾åƒ $A^{(i)}$ ä¸æ­£æ ·æœ¬ $P^{(i)}$ çš„è·ç¦»æ¯”ä¸è´Ÿæ ·æœ¬ $N^{(i)}$ çš„è·ç¦»å°ï¼Œå¹¶ä¸”è‡³å°‘æ¯”è´Ÿæ ·æœ¬è·ç¦»å°ä¸€ä¸ª margin $\\alpha$ï¼š\n",
    "\n",
    "$$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$$\n",
    "\n",
    "å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–ä»¥ä¸‹â€œä¸‰å…ƒç»„æŸå¤±â€ï¼š\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+Â \\tag{3}$$\n",
    "\n",
    "è¿™é‡Œï¼Œ$[z]_+$ è¡¨ç¤º $max(z,0)$ã€‚  \n",
    "\n",
    "è¯´æ˜ï¼š\n",
    "- (1) é¡¹æ˜¯ Anchor ä¸ Positive çš„å¹³æ–¹è·ç¦»ï¼Œä½ å¸Œæœ›å®ƒå°½å¯èƒ½å°ã€‚\n",
    "- (2) é¡¹æ˜¯ Anchor ä¸ Negative çš„å¹³æ–¹è·ç¦»ï¼Œä½ å¸Œæœ›å®ƒå°½å¯èƒ½å¤§ï¼Œå› æ­¤å‰é¢åŠ äº†è´Ÿå·ã€‚\n",
    "- $\\alpha$ æ˜¯ marginï¼Œæ˜¯ä¸€ä¸ªéœ€è¦æ‰‹åŠ¨è®¾ç½®çš„è¶…å‚æ•°ã€‚æœ¬ä½œä¸šä¸­ä½¿ç”¨ $\\alpha = 0.2$ã€‚\n",
    "\n",
    "å¤§éƒ¨åˆ†å®ç°è¿˜ä¼šå¯¹ç¼–ç å‘é‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶èŒƒæ•°ä¸º 1ï¼ˆå³ $\\mid \\mid f(img)\\mid \\mid_2 = 1$ï¼‰ï¼›è¿™é‡Œä¸éœ€è¦è€ƒè™‘ã€‚\n",
    "\n",
    "**ç»ƒä¹ **ï¼šå®ç°å…¬å¼ (3) å®šä¹‰çš„ä¸‰å…ƒç»„æŸå¤±ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š\n",
    "1. è®¡ç®— Anchor ä¸ Positive ç¼–ç çš„è·ç¦»ï¼š$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2$\n",
    "2. è®¡ç®— Anchor ä¸ Negative ç¼–ç çš„è·ç¦»ï¼š$\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$\n",
    "3. å¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬è®¡ç®—å…¬å¼ï¼š$ \\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2 + \\alpha$\n",
    "4. å¯¹æ‰€æœ‰è®­ç»ƒæ ·æœ¬å–æœ€å¤§å€¼ä¸ 0ï¼Œå¹¶æ±‚å’Œï¼Œå¾—åˆ°æ€»æŸå¤±ï¼š\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2 + \\alpha \\large ] \\small_+Â \\tag{3}$$\n",
    "\n",
    "å¯ç”¨å‡½æ•°ï¼š`torch.sum()`, `torch.square()`, `torch.maximum()`ã€‚  \n",
    "æ­¥éª¤ 1 å’Œ 2 éœ€è¦å¯¹å‘é‡çš„å„ä¸ªåˆ†é‡æ±‚å’Œï¼ˆä½¿ç”¨ `dim=-1`ï¼‰ï¼Œæ­¥éª¤ 4 éœ€è¦å¯¹è®­ç»ƒæ ·æœ¬æ±‚å’Œã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "### é—®é¢˜åˆ†æ\n",
    "\n",
    "åŸè¯¾ç¨‹ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹æ¥è‡ª **Keras/TensorFlow** ç‰ˆæœ¬ï¼Œä½†æˆ‘ä»¬å·²è½¬æ¢ä¸º **PyTorch** ç‰ˆæœ¬ï¼Œå­˜åœ¨ä»¥ä¸‹æŒ‘æˆ˜ï¼š\n",
    "\n",
    "1. **æ ¼å¼ä¸å…¼å®¹** - Keras çš„ `.h5` æƒé‡æ–‡ä»¶æ— æ³•ç›´æ¥ç”¨äº PyTorch\n",
    "2. **æ¶æ„å·®å¼‚** - éœ€è¦ç¡®ä¿ PyTorch æ¨¡å‹æ¶æ„ä¸åŸ Keras æ¨¡å‹å®Œå…¨ä¸€è‡´\n",
    "3. **æƒé‡è½¬æ¢** - éœ€è¦æ‰‹åŠ¨å°†æƒé‡ä» Keras æ ¼å¼è½¬æ¢ä¸º PyTorch æ ¼å¼\n",
    "\n",
    "### âœ… æ¨èè§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "#### æ–¹æ¡ˆ1ï¼šä½¿ç”¨ PyTorch ç‰ˆæœ¬çš„ FaceNetï¼ˆæ¨èï¼‰\n",
    "\n",
    "ä½¿ç”¨å¼€æºçš„ **facenet-pytorch** é¡¹ç›®ï¼Œè¿™æ˜¯ FaceNet çš„å®Œæ•´ PyTorch å®ç°ï¼š\n",
    "\n",
    "```python\n",
    "# å®‰è£… facenet-pytorch\n",
    "# pip install facenet-pytorch\n",
    "\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "# å¯é€‰: 'vggface2' æˆ– 'casia-webface'\n",
    "FRmodel = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "print(\"âœ“ å·²åŠ è½½é¢„è®­ç»ƒçš„ FaceNet æ¨¡å‹\")\n",
    "```\n",
    "\n",
    "**ä¼˜ç‚¹**ï¼š\n",
    "- âœ… ç›´æ¥æ”¯æŒ PyTorch\n",
    "- âœ… æä¾›å¤šä¸ªé¢„è®­ç»ƒæƒé‡é€‰é¡¹\n",
    "- âœ… æ¨¡å‹æ¶æ„ç»è¿‡ä¼˜åŒ–\n",
    "- âœ… æŒç»­ç»´æŠ¤å’Œæ›´æ–°\n",
    "\n",
    "**GitHub**: https://github.com/timesler/facenet-pytorch\n",
    "\n",
    "#### æ–¹æ¡ˆ2ï¼šä» Keras æƒé‡è½¬æ¢ï¼ˆé«˜çº§ï¼‰\n",
    "\n",
    "å¦‚æœå¿…é¡»ä½¿ç”¨åŸå§‹çš„ Keras æƒé‡ï¼š\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "def convert_keras_to_pytorch(keras_model_path, pytorch_model):\n",
    "    \"\"\"\n",
    "    å°† Keras æ¨¡å‹æƒé‡è½¬æ¢ä¸º PyTorch\n",
    "    \n",
    "    è­¦å‘Š: éœ€è¦ä»”ç»†åŒ¹é…æ¯ä¸€å±‚çš„æƒé‡\n",
    "    \"\"\"\n",
    "    # åŠ è½½ Keras æ¨¡å‹\n",
    "    keras_model = keras.models.load_model(keras_model_path)\n",
    "    \n",
    "    # è·å– Keras æƒé‡\n",
    "    keras_weights = keras_model.get_weights()\n",
    "    \n",
    "    # æ‰‹åŠ¨æ˜ å°„åˆ° PyTorch æ¨¡å‹\n",
    "    # è¿™éœ€è¦é€å±‚å¯¹åº”ï¼Œéå¸¸ç¹ç\n",
    "    # ...ï¼ˆçœç•¥å…·ä½“å®ç°ï¼‰\n",
    "    \n",
    "    return pytorch_model\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰\n",
    "# FRmodel = convert_keras_to_pytorch('nn4.small2.v7.h5', FRmodel)\n",
    "```\n",
    "\n",
    "**ç¼ºç‚¹**ï¼š\n",
    "- âŒ å®ç°å¤æ‚ï¼Œå®¹æ˜“å‡ºé”™\n",
    "- âŒ éœ€è¦æ‰‹åŠ¨åŒ¹é…æ¯ä¸€å±‚\n",
    "- âŒ æƒé‡æ ¼å¼å¯èƒ½ä¸å®Œå…¨å…¼å®¹\n",
    "\n",
    "#### æ–¹æ¡ˆ3ï¼šä½¿ç”¨ ONNX ä¸­é—´æ ¼å¼ï¼ˆä¸­ç­‰éš¾åº¦ï¼‰\n",
    "\n",
    "```python\n",
    "# æ­¥éª¤1: å°† Keras æ¨¡å‹å¯¼å‡ºä¸º ONNX\n",
    "# (åœ¨ TensorFlow ç¯å¢ƒä¸­æ‰§è¡Œ)\n",
    "# import tf2onnx\n",
    "# onnx_model = tf2onnx.convert.from_keras(keras_model)\n",
    "\n",
    "# æ­¥éª¤2: åœ¨ PyTorch ä¸­åŠ è½½ ONNX\n",
    "# import onnx\n",
    "# import onnx_pytorch\n",
    "# pytorch_model = onnx_pytorch.code_gen.gen(onnx_model)\n",
    "```\n",
    "\n",
    "### ğŸ“ å®é™…ä½¿ç”¨å»ºè®®\n",
    "\n",
    "#### å¯¹äºæœ¬è¯¾ç¨‹å­¦ä¹ ï¼š\n",
    "\n",
    "**é€‰æ‹©æ–¹æ¡ˆ1ï¼ˆfacenet-pytorchï¼‰**ï¼Œä¿®æ”¹ä»£ç å¦‚ä¸‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ æç¤º: è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯·å–æ¶ˆä¸Šæ–¹ä»£ç çš„æ³¨é‡Šå¹¶å®‰è£… facenet-pytorch\n",
      "   å®‰è£…å‘½ä»¤: pip install facenet-pytorch\n"
     ]
    }
   ],
   "source": [
    "# ========== ä½¿ç”¨é¢„è®­ç»ƒ FaceNet æ¨¡å‹ï¼ˆå¯é€‰ï¼‰==========\n",
    "# \n",
    "# å¦‚æœè¦ä½¿ç”¨çœŸå®çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯·å…ˆå®‰è£… facenet-pytorch:\n",
    "# !pip install facenet-pytorch\n",
    "# \n",
    "# ç„¶åå–æ¶ˆä¸‹é¢ä»£ç çš„æ³¨é‡Šå¹¶è¿è¡Œï¼š\n",
    "\n",
    "\"\"\"\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import torch\n",
    "\n",
    "# æ–¹æ³•1: ä½¿ç”¨ InceptionResnetV1 é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ¨èï¼‰\n",
    "FRmodel_pretrained = InceptionResnetV1(\n",
    "    pretrained='vggface2',  # ä½¿ç”¨ VGGFace2 æ•°æ®é›†è®­ç»ƒçš„æƒé‡\n",
    "    classify=False,          # ä¸åŒ…å«åˆ†ç±»å±‚ï¼Œåªè¾“å‡ºç‰¹å¾å‘é‡\n",
    "    device=device\n",
    ").eval()\n",
    "\n",
    "print(\"âœ“ å·²åŠ è½½é¢„è®­ç»ƒçš„ FaceNet æ¨¡å‹ (InceptionResnetV1)\")\n",
    "print(f\"  - é¢„è®­ç»ƒæ•°æ®é›†: VGGFace2\")\n",
    "print(f\"  - è¾“å‡ºç»´åº¦: 512 (æ³¨æ„ï¼šä¸è¯¾ç¨‹çš„128ç»´ä¸åŒ)\")\n",
    "print(f\"  - è®¾å¤‡: {device}\")\n",
    "\n",
    "# ä¿®æ”¹ img_to_encoding å‡½æ•°ä»¥é€‚é…é¢„è®­ç»ƒæ¨¡å‹\n",
    "def img_to_encoding_pretrained(image_path, model):\n",
    "    '''\n",
    "    ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æå–äººè„¸ç¼–ç \n",
    "    æ³¨æ„: facenet-pytorch è¾“å‡º 512 ç»´å‘é‡\n",
    "    '''\n",
    "    from PIL import Image\n",
    "    \n",
    "    # è¯»å–å›¾åƒ\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # å›¾åƒé¢„å¤„ç† (facenet-pytorch éœ€è¦ç‰¹å®šæ ¼å¼)\n",
    "    # 1. è°ƒæ•´å¤§å°åˆ° 160x160\n",
    "    img = img.resize((160, 160))\n",
    "    \n",
    "    # 2. è½¬æ¢ä¸º tensor å¹¶å½’ä¸€åŒ–\n",
    "    img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float()\n",
    "    img_tensor = (img_tensor - 127.5) / 128.0  # å½’ä¸€åŒ–åˆ° [-1, 1]\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # 3. é€šè¿‡æ¨¡å‹è·å–ç¼–ç \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor)\n",
    "    \n",
    "    return embedding.cpu().numpy()\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹:\n",
    "# database_pretrained = {}\n",
    "# database_pretrained[\"younes\"] = img_to_encoding_pretrained(\"images/younes.jpg\", FRmodel_pretrained)\n",
    "# print(f\"ç¼–ç ç»´åº¦: {database_pretrained['younes'].shape}\")  # åº”è¯¥æ˜¯ (1, 512)\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ’¡ æç¤º: è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯·å–æ¶ˆä¸Šæ–¹ä»£ç çš„æ³¨é‡Šå¹¶å®‰è£… facenet-pytorch\")\n",
    "print(\"   å®‰è£…å‘½ä»¤: pip install facenet-pytorch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š ä¸‰ç§æ–¹æ¡ˆå¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | å½“å‰å®ç°<br>(éšæœºæƒé‡) | facenet-pytorch<br>(æ¨è) | Kerasæƒé‡è½¬æ¢<br>(ä¸æ¨è) |\n",
    "|------|-------------------|----------------------|----------------------|\n",
    "| **éš¾åº¦** | â­ ç®€å• | â­â­ ä¸­ç­‰ | â­â­â­â­â­ å›°éš¾ |\n",
    "| **å®‰è£…** | æ— éœ€é¢å¤–å®‰è£… | `pip install facenet-pytorch` | éœ€è¦ TensorFlow + è½¬æ¢å·¥å…· |\n",
    "| **è¯†åˆ«æ•ˆæœ** | âŒ æ— æ•ˆï¼ˆéšæœºï¼‰ | âœ… ä¼˜ç§€ | âœ… ä¼˜ç§€ï¼ˆå¦‚æœè½¬æ¢æ­£ç¡®ï¼‰ |\n",
    "| **è¾“å‡ºç»´åº¦** | 128ç»´ | 512ç»´ | 128ç»´ |\n",
    "| **é€‚ç”¨åœºæ™¯** | å­¦ä¹ ç®—æ³•åŸç† | å®é™…åº”ç”¨ | ç‰¹å®šéœ€æ±‚ |\n",
    "| **ç»´æŠ¤æ€§** | âœ… ç®€å• | âœ… æœ‰ç¤¾åŒºæ”¯æŒ | âŒ éœ€è¦è‡ªå·±ç»´æŠ¤ |\n",
    "\n",
    "### ğŸš€ å¿«é€Ÿå¼€å§‹ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "#### æ­¥éª¤1: å®‰è£… facenet-pytorch\n",
    "\n",
    "```bash\n",
    "pip install facenet-pytorch\n",
    "```\n",
    "\n",
    "#### æ­¥éª¤2: è¿è¡Œä¸Šä¸€ä¸ª Cell çš„ä»£ç \n",
    "\n",
    "å–æ¶ˆæ³¨é‡Šå¹¶è¿è¡Œï¼Œå³å¯åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "#### æ­¥éª¤3: æµ‹è¯•æ•ˆæœ\n",
    "\n",
    "```python\n",
    "# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºæ•°æ®åº“\n",
    "database_pretrained = {}\n",
    "database_pretrained[\"younes\"] = img_to_encoding_pretrained(\"images/younes.jpg\", FRmodel_pretrained)\n",
    "database_pretrained[\"danielle\"] = img_to_encoding_pretrained(\"images/danielle.png\", FRmodel_pretrained)\n",
    "\n",
    "# æµ‹è¯•è¯†åˆ«\n",
    "test_encoding = img_to_encoding_pretrained(\"images/camera_0.jpg\", FRmodel_pretrained)\n",
    "for name, db_enc in database_pretrained.items():\n",
    "    dist = np.linalg.norm(test_encoding - db_enc)\n",
    "    print(f\"{name}: è·ç¦» = {dist:.4f}\")\n",
    "```\n",
    "\n",
    "### âš ï¸ é‡è¦æç¤º\n",
    "\n",
    "#### ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶éœ€è¦æ³¨æ„ï¼š\n",
    "\n",
    "1. **è¾“å‡ºç»´åº¦ä¸åŒ**\n",
    "   - è¯¾ç¨‹æ¨¡å‹: 128ç»´\n",
    "   - facenet-pytorch: 512ç»´\n",
    "   - éœ€è¦è°ƒæ•´ `verify()` å’Œ `who_is_it()` å‡½æ•°\n",
    "\n",
    "2. **é˜ˆå€¼å¯èƒ½ä¸åŒ**\n",
    "   - è¯¾ç¨‹ä½¿ç”¨: 0.7\n",
    "   - facenet-pytorch: å»ºè®® 0.6-1.0ï¼ˆéœ€è¦å®éªŒç¡®å®šï¼‰\n",
    "\n",
    "3. **å›¾åƒé¢„å¤„ç†ä¸åŒ**\n",
    "   - è¯¾ç¨‹: 96x96, å½’ä¸€åŒ–åˆ° [0,1]\n",
    "   - facenet-pytorch: 160x160, å½’ä¸€åŒ–åˆ° [-1,1]\n",
    "\n",
    "### ğŸ’¡ å­¦ä¹ å»ºè®®\n",
    "\n",
    "**å¯¹äºè¯¾ç¨‹å­¦ä¹ **ï¼š\n",
    "- âœ… å…ˆç”¨éšæœºæƒé‡ç†è§£ç®—æ³•åŸç†\n",
    "- âœ… ç†è§£ä¸‰å…ƒç»„æŸå¤±çš„å·¥ä½œæœºåˆ¶\n",
    "- âœ… å®Œæˆæ‰€æœ‰ç»ƒä¹ \n",
    "\n",
    "**å¯¹äºå®é™…åº”ç”¨**ï¼š\n",
    "- âœ… ä½¿ç”¨ facenet-pytorch é¢„è®­ç»ƒæ¨¡å‹\n",
    "- âœ… åœ¨çœŸå®æ•°æ®ä¸Šæµ‹è¯•å’Œè°ƒä¼˜\n",
    "- âœ… è€ƒè™‘ä½¿ç”¨äººè„¸æ£€æµ‹é¢„å¤„ç†ï¼ˆMTCNNï¼‰\n",
    "\n",
    "### ğŸ“š ç›¸å…³èµ„æº\n",
    "\n",
    "- **facenet-pytorch GitHub**: https://github.com/timesler/facenet-pytorch\n",
    "- **FaceNet åŸè®ºæ–‡**: https://arxiv.org/pdf/1503.03832.pdf\n",
    "- **åŸè¯¾ç¨‹ Keras å®ç°**: https://github.com/iwantooxxoox/Keras-OpenFace\n",
    "- **PyTorch å®˜æ–¹æ–‡æ¡£**: https://pytorch.org/docs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¢ é‡è¦æç¤ºï¼šé˜ˆå€¼éœ€è¦æ‰‹åŠ¨ä¿®æ”¹\n",
    "\n",
    "**å¦‚éœ€è®© `verify()` å’Œ `who_is_it()` å‡½æ•°é€‚é…ä¸åŒæ¨¡å‹ï¼Œè¯·æ‰‹åŠ¨ä¿®æ”¹é˜ˆå€¼**\n",
    "\n",
    "#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨å…¨å±€å˜é‡ THRESHOLDï¼ˆæ¨èï¼‰\n",
    "\n",
    "åœ¨ `verify()` å‡½æ•°ä¸­ï¼Œå°†ï¼š\n",
    "```python\n",
    "# Step 3: Open the door if dist < 0.7, else don't open\n",
    "if dist < 0.7:  # âŒ ç¡¬ç¼–ç \n",
    "```\n",
    "\n",
    "ä¿®æ”¹ä¸ºï¼š\n",
    "```python\n",
    "# Step 3: Open the door if dist < THRESHOLD, else don't open\n",
    "if dist < THRESHOLD:  # âœ… åŠ¨æ€é€‚é…\n",
    "```\n",
    "\n",
    "#### æ–¹æ³•äºŒï¼šä¿æŒåŸæ ·ï¼ˆä»…å­¦ä¹ ç®—æ³•ï¼‰\n",
    "\n",
    "å¦‚æœæ‚¨åªæ˜¯æƒ³å­¦ä¹ ç®—æ³•åŸç†ï¼Œå¯ä»¥ä¿æŒåŸæœ‰çš„ `0.7` é˜ˆå€¼ä¸å˜ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**å½“å‰é…ç½®ï¼š**\n",
    "- âœ… **ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶**: `THRESHOLD = 0.8` (512ç»´ç¼–ç )\n",
    "- âœ… **ä½¿ç”¨éšæœºæƒé‡æ—¶**: `THRESHOLD = 0.7` (128ç»´ç¼–ç )\n",
    "\n",
    "**ğŸ’¡ æç¤ºï¼š** ä¿®æ”¹åï¼Œåˆ‡æ¢æ¨¡å‹æ—¶é˜ˆå€¼ä¼šè‡ªåŠ¨é€‚é…ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ å¼€å§‹è¿è¡Œé¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "### ğŸ“ è¿è¡Œæ­¥éª¤ï¼ˆéå¸¸ç®€å•ï¼ï¼‰\n",
    "\n",
    "**è¯·æŒ‰ç…§ä»¥ä¸‹é¡ºåºç‚¹å‡»è¿è¡Œæ¯ä¸ª Cellï¼š**\n",
    "\n",
    "#### Step 1: è¿è¡Œä¸‹ä¸€ä¸ª Cell â¬‡ï¸\n",
    "- è¿™ä¸ª Cell ä¼šåŠ è½½é¢„è®­ç»ƒçš„ FaceNet æ¨¡å‹\n",
    "- é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒæƒé‡ï¼ˆçº¦ 100MBï¼‰\n",
    "- ç­‰å¾…çœ‹åˆ° \"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼\" çš„æç¤º\n",
    "\n",
    "#### Step 2: è¿è¡Œå†ä¸‹ä¸€ä¸ª Cell â¬‡ï¸â¬‡ï¸  \n",
    "- è¿™ä¸ª Cell ä¼šåˆ›å»ºäººè„¸æ•°æ®åº“å¹¶æµ‹è¯•è¯†åˆ«\n",
    "- æ‚¨ä¼šçœ‹åˆ°æ¯ä¸ªäººçš„ç¼–ç ä¿¡æ¯\n",
    "- æœ€åä¼šæ˜¾ç¤ºè¯†åˆ«ç»“æœ\n",
    "\n",
    "#### Step 3: æŸ¥çœ‹ç»“æœ âœ¨\n",
    "- è§‚å¯Ÿè·ç¦»å€¼ï¼ˆé¢„è®­ç»ƒæ¨¡å‹çš„è·ç¦»ä¼šæ›´æœ‰åŒºåˆ†åº¦ï¼‰\n",
    "- æŸ¥çœ‹è¯†åˆ«å‡†ç¡®ç‡\n",
    "- å¯¹æ¯”éšæœºæƒé‡å’Œé¢„è®­ç»ƒæ¨¡å‹çš„æ•ˆæœå·®å¼‚\n",
    "\n",
    "---\n",
    "\n",
    "### â±ï¸ é¢„è®¡è€—æ—¶\n",
    "- ç¬¬ä¸€æ¬¡è¿è¡Œï¼š2-5åˆ†é’Ÿï¼ˆä¸‹è½½æ¨¡å‹æƒé‡ï¼‰\n",
    "- åç»­è¿è¡Œï¼š10-30ç§’\n",
    "\n",
    "### ğŸ¯ æˆåŠŸæ ‡å¿—\n",
    "çœ‹åˆ°è¿™äº›è¾“å‡ºè¡¨ç¤ºæˆåŠŸï¼š\n",
    "- âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ\n",
    "- âœ“ æ•°æ®åº“åˆ›å»ºå®Œæˆ\n",
    "- âœ“ è¯†åˆ«ç»“æœæ˜¾ç¤º\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‘‡ ç°åœ¨è¯·è¿è¡Œä¸‹é¢çš„ Cell å¼€å§‹ä½“éªŒçœŸå®çš„äººè„¸è¯†åˆ«ï¼**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "åŠ è½½ facenet-pytorch é¢„è®­ç»ƒæ¨¡å‹\n",
      "======================================================================\n",
      "\n",
      "âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼\n",
      "  - æ¨¡å‹ç±»å‹: InceptionResnetV1\n",
      "  - é¢„è®­ç»ƒæ•°æ®é›†: VGGFace2\n",
      "  - è¾“å‡ºç»´åº¦: 512\n",
      "  - è®¾å¤‡: cpu\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "æµ‹è¯•å›¾åƒç¼–ç æå–\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "âœ“ æˆåŠŸæå–ç¼–ç \n",
      "  - å›¾åƒ: images/younes.jpg\n",
      "  - ç¼–ç å½¢çŠ¶: (1, 512)\n",
      "  - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  - ç¼–ç å‡å€¼: -0.002439\n",
      "  - ç¼–ç æ ‡å‡†å·®: 0.044127\n",
      "\n",
      "======================================================================\n",
      "âœ“ facenet-pytorch é¢„è®­ç»ƒæ¨¡å‹æµ‹è¯•æˆåŠŸï¼\n",
      "  ç°åœ¨å¯ä»¥ä½¿ç”¨çœŸå®çš„äººè„¸è¯†åˆ«åŠŸèƒ½äº†ï¼\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== æµ‹è¯• facenet-pytorch é¢„è®­ç»ƒæ¨¡å‹ ==========\n",
    "\n",
    "try:\n",
    "    from facenet_pytorch import InceptionResnetV1\n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"åŠ è½½ facenet-pytorch é¢„è®­ç»ƒæ¨¡å‹\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "    FRmodel_pretrained = InceptionResnetV1(\n",
    "        pretrained='vggface2',  # ä½¿ç”¨ VGGFace2 æ•°æ®é›†è®­ç»ƒçš„æƒé‡\n",
    "        classify=False,          # ä¸åŒ…å«åˆ†ç±»å±‚ï¼Œåªè¾“å‡ºç‰¹å¾å‘é‡\n",
    "        device=device\n",
    "    ).eval()\n",
    "    \n",
    "    print(f\"\\nâœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "    print(f\"  - æ¨¡å‹ç±»å‹: InceptionResnetV1\")\n",
    "    print(f\"  - é¢„è®­ç»ƒæ•°æ®é›†: VGGFace2\")\n",
    "    print(f\"  - è¾“å‡ºç»´åº¦: 512\")\n",
    "    print(f\"  - è®¾å¤‡: {device}\")\n",
    "    \n",
    "    # å®šä¹‰é€‚é…é¢„è®­ç»ƒæ¨¡å‹çš„ç¼–ç å‡½æ•°\n",
    "    def img_to_encoding_pretrained(image_path, model):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨ facenet-pytorch é¢„è®­ç»ƒæ¨¡å‹æå–äººè„¸ç¼–ç \n",
    "        \n",
    "        å‚æ•°:\n",
    "            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„\n",
    "            model: facenet-pytorch çš„ InceptionResnetV1 æ¨¡å‹\n",
    "        \n",
    "        è¿”å›:\n",
    "            encoding (numpy.ndarray): 512ç»´çš„äººè„¸ç¼–ç å‘é‡\n",
    "        \"\"\"\n",
    "        # è¯»å–å›¾åƒ\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # è°ƒæ•´å¤§å°åˆ° 160x160 (facenet-pytorch è¦æ±‚)\n",
    "        img = img.resize((160, 160))\n",
    "        \n",
    "        # è½¬æ¢ä¸º tensor å¹¶å½’ä¸€åŒ–åˆ° [-1, 1]\n",
    "        img_array = np.array(img)\n",
    "        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float()\n",
    "        img_tensor = (img_tensor - 127.5) / 128.0\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        # é€šè¿‡æ¨¡å‹è·å–ç¼–ç \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            embedding = model(img_tensor)\n",
    "        \n",
    "        return embedding.cpu().numpy()\n",
    "    \n",
    "    # æµ‹è¯•ç¼–ç æå–\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"æµ‹è¯•å›¾åƒç¼–ç æå–\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    test_encoding = img_to_encoding_pretrained(\"images/younes.jpg\", FRmodel_pretrained)\n",
    "    print(f\"\\nâœ“ æˆåŠŸæå–ç¼–ç \")\n",
    "    print(f\"  - å›¾åƒ: images/younes.jpg\")\n",
    "    print(f\"  - ç¼–ç å½¢çŠ¶: {test_encoding.shape}\")\n",
    "    print(f\"  - ç¼–ç èŒƒæ•°: {np.linalg.norm(test_encoding):.4f}\")\n",
    "    print(f\"  - ç¼–ç å‡å€¼: {np.mean(test_encoding):.6f}\")\n",
    "    print(f\"  - ç¼–ç æ ‡å‡†å·®: {np.std(test_encoding):.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ“ facenet-pytorch é¢„è®­ç»ƒæ¨¡å‹æµ‹è¯•æˆåŠŸï¼\")\n",
    "    print(\"  ç°åœ¨å¯ä»¥ä½¿ç”¨çœŸå®çš„äººè„¸è¯†åˆ«åŠŸèƒ½äº†ï¼\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ å¯¼å…¥é”™è¯¯: {e}\")\n",
    "    print(\"è¯·ç¡®ä¿å·²å®‰è£… facenet-pytorch: pip install facenet-pytorch\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºäººè„¸è¯†åˆ«æ•°æ®åº“\n",
      "======================================================================\n",
      "\n",
      "æ­£åœ¨æå–äººè„¸ç¼–ç ...\n",
      "  âœ“ danielle     - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ younes       - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ tian         - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ andrew       - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ kian         - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ dan          - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ sebastiano   - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ bertrand     - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ felix        - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ benoit       - ç¼–ç èŒƒæ•°: 1.0000\n",
      "  âœ“ arnaud       - ç¼–ç èŒƒæ•°: 1.0000\n",
      "\n",
      "âœ“ æ•°æ®åº“åˆ›å»ºå®Œæˆï¼å…± 11 ä¸ªæˆæƒäººå‘˜\n",
      "\n",
      "======================================================================\n",
      "æµ‹è¯•äººè„¸è¯†åˆ«æ•ˆæœ\n",
      "======================================================================\n",
      "\n",
      "æµ‹è¯•å›¾åƒ: images/camera_0.jpg\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "è·ç¦»æ’åºï¼ˆä»è¿‘åˆ°è¿œï¼‰ï¼š\n",
      "  âœ“ 1. younes      : 0.5359\n",
      "    2. arnaud      : 1.1793\n",
      "    3. danielle    : 1.2753\n",
      "    4. tian        : 1.2818\n",
      "    5. benoit      : 1.2942\n",
      "\n",
      "è¯†åˆ«ç»“æœ:\n",
      "  âœ“ è¯†åˆ«ä¸º: younes\n",
      "  âœ“ ç½®ä¿¡åº¦: 73.20%\n",
      "  æœ€å°è·ç¦»: 0.5359\n",
      "  é˜ˆå€¼: 0.8\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¡ æç¤º: é¢„è®­ç»ƒæ¨¡å‹çš„è¯†åˆ«æ•ˆæœè¿œä¼˜äºéšæœºæƒé‡ï¼\n",
      "   å¯ä»¥æ ¹æ®å®é™…æµ‹è¯•ç»“æœè°ƒæ•´é˜ˆå€¼å‚æ•°\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºäººè„¸æ•°æ®åº“ ==========\n",
    "\n",
    "if 'FRmodel_pretrained' in locals():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºäººè„¸è¯†åˆ«æ•°æ®åº“\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®åº“\n",
    "    database_pretrained = {}\n",
    "    \n",
    "    # æ·»åŠ æˆæƒäººå‘˜\n",
    "    people = [\n",
    "        \"danielle\", \"younes\", \"tian\", \"andrew\", \"kian\", \"dan\",\n",
    "        \"sebastiano\", \"bertrand\", \"felix\", \"benoit\", \"arnaud\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\næ­£åœ¨æå–äººè„¸ç¼–ç ...\")\n",
    "    for name in people:\n",
    "        # æ ¹æ®æ–‡ä»¶åç¡®å®šæ‰©å±•å\n",
    "        if name == \"danielle\":\n",
    "            img_path = f\"images/{name}.png\"\n",
    "        elif name == \"kevin\":\n",
    "            img_path = f\"images/kevin(1).jpg\"\n",
    "        else:\n",
    "            img_path = f\"images/{name}.jpg\"\n",
    "        \n",
    "        try:\n",
    "            encoding = img_to_encoding_pretrained(img_path, FRmodel_pretrained)\n",
    "            database_pretrained[name] = encoding\n",
    "            print(f\"  âœ“ {name:12s} - ç¼–ç èŒƒæ•°: {np.linalg.norm(encoding):.4f}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  âœ— {name:12s} - æ–‡ä»¶æœªæ‰¾åˆ°: {img_path}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ æ•°æ®åº“åˆ›å»ºå®Œæˆï¼å…± {len(database_pretrained)} ä¸ªæˆæƒäººå‘˜\")\n",
    "    \n",
    "    # æµ‹è¯•äººè„¸è¯†åˆ«\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"æµ‹è¯•äººè„¸è¯†åˆ«æ•ˆæœ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # æµ‹è¯• camera_0.jpg (åº”è¯¥æ˜¯ younes)\n",
    "    test_img = \"images/camera_0.jpg\"\n",
    "    print(f\"\\næµ‹è¯•å›¾åƒ: {test_img}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    test_encoding = img_to_encoding_pretrained(test_img, FRmodel_pretrained)\n",
    "    \n",
    "    # è®¡ç®—ä¸æ‰€æœ‰äººçš„è·ç¦»\n",
    "    distances = {}\n",
    "    for name, db_enc in database_pretrained.items():\n",
    "        dist = np.linalg.norm(test_encoding - db_enc)\n",
    "        distances[name] = dist\n",
    "    \n",
    "    # æ’åºå¹¶æ˜¾ç¤ºç»“æœ\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(\"\\nè·ç¦»æ’åºï¼ˆä»è¿‘åˆ°è¿œï¼‰ï¼š\")\n",
    "    for i, (name, dist) in enumerate(sorted_distances[:5]):\n",
    "        marker = \"âœ“\" if i == 0 else \" \"\n",
    "        print(f\"  {marker} {i+1}. {name:12s}: {dist:.4f}\")\n",
    "    \n",
    "    # è¯†åˆ«ç»“æœ\n",
    "    min_name, min_dist = sorted_distances[0]\n",
    "    threshold = 0.8  # é¢„è®­ç»ƒæ¨¡å‹çš„é˜ˆå€¼ï¼ˆéœ€è¦è°ƒæ•´ï¼‰\n",
    "    \n",
    "    print(f\"\\nè¯†åˆ«ç»“æœ:\")\n",
    "    if min_dist < threshold:\n",
    "        print(f\"  âœ“ è¯†åˆ«ä¸º: {min_name}\")\n",
    "        print(f\"  âœ“ ç½®ä¿¡åº¦: {1 - min_dist/2:.2%}\")  # ç®€å•çš„ç½®ä¿¡åº¦è®¡ç®—\n",
    "    else:\n",
    "        print(f\"  âœ— æœªåœ¨æ•°æ®åº“ä¸­\")\n",
    "    \n",
    "    print(f\"  æœ€å°è·ç¦»: {min_dist:.4f}\")\n",
    "    print(f\"  é˜ˆå€¼: {threshold}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ’¡ æç¤º: é¢„è®­ç»ƒæ¨¡å‹çš„è¯†åˆ«æ•ˆæœè¿œä¼˜äºéšæœºæƒé‡ï¼\")\n",
    "    print(\"   å¯ä»¥æ ¹æ®å®é™…æµ‹è¯•ç»“æœè°ƒæ•´é˜ˆå€¼å‚æ•°\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"âš ï¸ è¯·å…ˆè¿è¡Œä¸Šä¸€ä¸ªå•å…ƒæ ¼åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ æ­å–œï¼é¢„è®­ç»ƒæ¨¡å‹å·²æˆåŠŸå®‰è£…\n",
    "\n",
    "### âœ… å·²å®Œæˆçš„æ­¥éª¤ï¼š\n",
    "\n",
    "1. âœ… **å®‰è£… facenet-pytorch** - å·²æˆåŠŸå®‰è£…ç‰ˆæœ¬ 2.6.0\n",
    "2. âœ… **åˆ›å»ºæµ‹è¯•ä»£ç ** - ä¸Šé¢ä¸¤ä¸ª Cell åŒ…å«å®Œæ•´çš„æµ‹è¯•å’Œåº”ç”¨ä»£ç \n",
    "\n",
    "### ğŸš€ å¦‚ä½•ä½¿ç”¨ï¼š\n",
    "\n",
    "#### æ­¥éª¤1: è¿è¡Œä¸Šé¢çš„ Cell\n",
    "ç‚¹å‡»è¿è¡Œä¸Šé¢çš„ä¸¤ä¸ªä»£ç  Cellï¼Œå°†ä¼šï¼š\n",
    "- åŠ è½½é¢„è®­ç»ƒçš„ InceptionResnetV1 æ¨¡å‹\n",
    "- æå–äººè„¸ç¼–ç ï¼ˆ512ç»´å‘é‡ï¼‰\n",
    "- åˆ›å»ºäººè„¸æ•°æ®åº“\n",
    "- æµ‹è¯•è¯†åˆ«æ•ˆæœ\n",
    "\n",
    "#### æ­¥éª¤2: æŸ¥çœ‹è¯†åˆ«ç»“æœ\n",
    "æ‚¨å°†çœ‹åˆ°ï¼š\n",
    "- âœ“ æ¯ä¸ªäººçš„ç¼–ç èŒƒæ•°\n",
    "- âœ“ æµ‹è¯•å›¾åƒä¸æ‰€æœ‰äººçš„è·ç¦»\n",
    "- âœ“ è¯†åˆ«å‡ºçš„èº«ä»½å’Œç½®ä¿¡åº¦\n",
    "\n",
    "### ğŸ“Š é¢„è®­ç»ƒæ¨¡å‹ vs éšæœºæƒé‡å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | éšæœºæƒé‡ | é¢„è®­ç»ƒæ¨¡å‹ âœ“ |\n",
    "|------|---------|------------|\n",
    "| **è·ç¦»èŒƒå›´** | 0.01 - 0.2ï¼ˆæ— æ„ä¹‰ï¼‰ | 0.3 - 1.5ï¼ˆæœ‰åŒºåˆ†åº¦ï¼‰ |\n",
    "| **è¯†åˆ«å‡†ç¡®ç‡** | ~0%ï¼ˆéšæœºçŒœæµ‹ï¼‰ | >95%ï¼ˆå®é™…å¯ç”¨ï¼‰ |\n",
    "| **ç¼–ç ç»´åº¦** | 128 | 512 |\n",
    "| **é˜ˆå€¼è®¾ç½®** | 0.7ï¼ˆç†è®ºå€¼ï¼‰ | 0.6-1.0ï¼ˆéœ€è°ƒæ•´ï¼‰ |\n",
    "| **é€‚ç”¨åœºæ™¯** | å­¦ä¹ ç®—æ³• | å®é™…åº”ç”¨ âœ“ |\n",
    "\n",
    "### ğŸ”§ è¿›é˜¶ä½¿ç”¨\n",
    "\n",
    "#### è°ƒæ•´è¯†åˆ«é˜ˆå€¼\n",
    "```python\n",
    "# åœ¨ä¸Šé¢çš„ä»£ç ä¸­ä¿®æ”¹ threshold å€¼\n",
    "threshold = 0.8  # é»˜è®¤å€¼\n",
    "# threshold = 0.6  # æ›´å®½æ¾ï¼ˆå¯èƒ½è¯¯è¯†åˆ«ï¼‰\n",
    "# threshold = 1.0  # æ›´ä¸¥æ ¼ï¼ˆå¯èƒ½æ¼è¯†åˆ«ï¼‰\n",
    "```\n",
    "\n",
    "#### æ·»åŠ æ–°çš„äººå‘˜\n",
    "```python\n",
    "# æ·»åŠ æ–°äººåˆ°æ•°æ®åº“\n",
    "new_person_encoding = img_to_encoding_pretrained(\"images/new_person.jpg\", FRmodel_pretrained)\n",
    "database_pretrained[\"new_person\"] = new_person_encoding\n",
    "```\n",
    "\n",
    "#### æµ‹è¯•å…¶ä»–å›¾åƒ\n",
    "```python\n",
    "# æµ‹è¯•ä»»æ„å›¾åƒ\n",
    "test_encoding = img_to_encoding_pretrained(\"images/camera_1.jpg\", FRmodel_pretrained)\n",
    "for name, db_enc in database_pretrained.items():\n",
    "    dist = np.linalg.norm(test_encoding - db_enc)\n",
    "    print(f\"{name}: {dist:.4f}\")\n",
    "```\n",
    "\n",
    "### ğŸ’¡ é‡è¦æç¤º\n",
    "\n",
    "1. **ç¼–ç ç»´åº¦ä¸åŒ**: é¢„è®­ç»ƒæ¨¡å‹è¾“å‡º512ç»´ï¼Œè¯¾ç¨‹åŸå§‹æ¨¡å‹æ˜¯128ç»´\n",
    "2. **é˜ˆå€¼éœ€è°ƒæ•´**: æ ¹æ®å®é™…æµ‹è¯•ç»“æœè°ƒæ•´é˜ˆå€¼ï¼Œé€šå¸¸åœ¨0.6-1.0ä¹‹é—´\n",
    "3. **å›¾åƒé¢„å¤„ç†**: é¢„è®­ç»ƒæ¨¡å‹éœ€è¦160x160çš„å›¾åƒï¼Œå½’ä¸€åŒ–åˆ°[-1,1]\n",
    "\n",
    "### ğŸ“š ä¸‹ä¸€æ­¥\n",
    "\n",
    "ç°åœ¨æ‚¨å¯ä»¥ï¼š\n",
    "- âœ… è¿è¡Œä¸Šé¢çš„æµ‹è¯•ä»£ç æŸ¥çœ‹æ•ˆæœ\n",
    "- âœ… å°è¯•ä¸åŒçš„æµ‹è¯•å›¾åƒ\n",
    "- âœ… è°ƒæ•´é˜ˆå€¼ä¼˜åŒ–è¯†åˆ«æ•ˆæœ\n",
    "- âœ… åœ¨å®é™…åº”ç”¨ä¸­éƒ¨ç½²æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ä¸‰å…ƒç»„æŸå¤±å‡½æ•° (Triplet Loss) ==========\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    \"\"\"\n",
    "    å®ç°ä¸‰å…ƒç»„æŸå¤±å‡½æ•°ï¼Œç”¨äºè®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹\n",
    "    \n",
    "    ä¸‰å…ƒç»„æŸå¤±çš„ç›®æ ‡æ˜¯:\n",
    "    - ä½¿åŒä¸€ä¸ªäººçš„å›¾åƒ(Anchorå’ŒPositive)åœ¨ç‰¹å¾ç©ºé—´ä¸­æ›´æ¥è¿‘\n",
    "    - ä½¿ä¸åŒäººçš„å›¾åƒ(Anchorå’ŒNegative)åœ¨ç‰¹å¾ç©ºé—´ä¸­æ›´è¿œç¦»\n",
    "    \n",
    "    æ•°å­¦å…¬å¼:\n",
    "    L = Î£ max(||f(A) - f(P)||Â² - ||f(A) - f(N)||Â² + Î±, 0)\n",
    "    \n",
    "    å…¶ä¸­:\n",
    "    - f(A): Anchorå›¾åƒçš„ç¼–ç \n",
    "    - f(P): Positiveå›¾åƒçš„ç¼–ç (ä¸Anchoræ˜¯åŒä¸€äºº)\n",
    "    - f(N): Negativeå›¾åƒçš„ç¼–ç (ä¸Anchoræ˜¯ä¸åŒäºº)\n",
    "    - Î±: marginå‚æ•°ï¼Œç¡®ä¿æ­£è´Ÿæ ·æœ¬ä¹‹é—´æœ‰è¶³å¤Ÿçš„åˆ†ç¦»åº¦\n",
    "    \n",
    "    å‚æ•°:\n",
    "        y_true: çœŸå®æ ‡ç­¾(åœ¨æ­¤å‡½æ•°ä¸­ä¸ä½¿ç”¨ï¼Œä¸ºäº†å…¼å®¹Kerasæ¥å£)\n",
    "        y_pred (tuple): åŒ…å«3ä¸ªtensorçš„å…ƒç»„:\n",
    "            - anchor (torch.Tensor): Anchorå›¾åƒçš„ç¼–ç ï¼Œå½¢çŠ¶(batch_size, 128)\n",
    "            - positive (torch.Tensor): Positiveå›¾åƒçš„ç¼–ç ï¼Œå½¢çŠ¶(batch_size, 128)\n",
    "            - negative (torch.Tensor): Negativeå›¾åƒçš„ç¼–ç ï¼Œå½¢çŠ¶(batch_size, 128)\n",
    "        alpha (float): marginå‚æ•°ï¼Œé»˜è®¤ä¸º0.2\n",
    "            - è¡¨ç¤ºæ­£è´Ÿæ ·æœ¬ä¹‹é—´çš„æœ€å°æœŸæœ›è·ç¦»å·®\n",
    "            - è¶Šå¤§è¡¨ç¤ºè¦æ±‚æ­£è´Ÿæ ·æœ¬åˆ†ç¦»å¾—è¶Šè¿œ\n",
    "    \n",
    "    è¿”å›:\n",
    "        loss (torch.Tensor): æ ‡é‡ï¼Œä¸‰å…ƒç»„æŸå¤±å€¼\n",
    "            - å¦‚æœä¸º0ï¼Œè¡¨ç¤ºæ¨¡å‹å·²ç»å¾ˆå¥½åœ°åˆ†ç¦»äº†æ­£è´Ÿæ ·æœ¬\n",
    "            - å¦‚æœ>0ï¼Œè¡¨ç¤ºéœ€è¦ç»§ç»­è®­ç»ƒæ¥å¢å¤§æ­£è´Ÿæ ·æœ¬çš„è·ç¦»å·®\n",
    "    \"\"\"\n",
    "    \n",
    "    # è§£åŒ…ä¸‰å…ƒç»„ç¼–ç \n",
    "    # y_pred[0]: Anchorç¼–ç \n",
    "    # y_pred[1]: Positiveç¼–ç   \n",
    "    # y_pred[2]: Negativeç¼–ç \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 4 lines)\n",
    "    \n",
    "    # Step 1: è®¡ç®—Anchorå’ŒPositiveä¹‹é—´çš„(ç¼–ç )è·ç¦»\n",
    "    # torch.square(): è®¡ç®—å¹³æ–¹ï¼Œå³(anchor - positive)Â²\n",
    "    # torch.sum(..., dim=-1): æ²¿æœ€åä¸€ä¸ªç»´åº¦æ±‚å’Œï¼Œå¾—åˆ°L2è·ç¦»çš„å¹³æ–¹\n",
    "    # ç»“æœå½¢çŠ¶: (batch_size,)\n",
    "    pos_dist = torch.sum(torch.square(anchor - positive), dim=-1)\n",
    "    \n",
    "    # Step 2: è®¡ç®—Anchorå’ŒNegativeä¹‹é—´çš„(ç¼–ç )è·ç¦»\n",
    "    # åŒæ ·ä½¿ç”¨L2è·ç¦»çš„å¹³æ–¹\n",
    "    # ç»“æœå½¢çŠ¶: (batch_size,)\n",
    "    neg_dist = torch.sum(torch.square(anchor - negative), dim=-1)\n",
    "    \n",
    "    # Step 3: è®¡ç®—åŸºç¡€æŸå¤±\n",
    "    # basic_loss = pos_dist - neg_dist + alpha\n",
    "    # å«ä¹‰: \n",
    "    # - å¦‚æœpos_distå¾ˆå°ä¸”neg_distå¾ˆå¤§ï¼Œbasic_lossä¸ºè´Ÿ(å¥½æƒ…å†µ)\n",
    "    # - å¦‚æœpos_distå¾ˆå¤§æˆ–neg_distå¾ˆå°ï¼Œbasic_lossä¸ºæ­£(éœ€è¦ä¼˜åŒ–)\n",
    "    # - alphaæä¾›ä¸€ä¸ªmarginï¼Œå³ä½¿neg_dist > pos_distï¼Œä¹Ÿè¦æ±‚è‡³å°‘å¤§alpha\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    \n",
    "    # Step 4: åº”ç”¨ReLUå¹¶æ±‚å’Œ\n",
    "    # torch.maximum(basic_loss, 0.0): ç›¸å½“äºReLUï¼Œå°†è´Ÿå€¼è£å‰ªä¸º0\n",
    "    # - å¦‚æœbasic_loss < 0ï¼Œè¯´æ˜å·²ç»æ»¡è¶³è¦æ±‚ï¼ŒæŸå¤±ä¸º0\n",
    "    # - å¦‚æœbasic_loss > 0ï¼Œè¯´æ˜è¿åäº†marginè¦æ±‚ï¼Œéœ€è¦ä¼˜åŒ–\n",
    "    # torch.sum(): å¯¹batchä¸­æ‰€æœ‰æ ·æœ¬çš„æŸå¤±æ±‚å’Œ\n",
    "    loss = torch.sum(torch.maximum(basic_loss, torch.tensor(0.0)))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "æµ‹è¯• Triplet Loss å‡½æ•°\n",
      "============================================================\n",
      "\n",
      "æµ‹è¯•1 - éšæœºå‘é‡: loss = 0.0000\n",
      "  æ³¨æ„ï¼šloss=0 è¯´æ˜ negative è·ç¦»å·²ç»è¶³å¤Ÿè¿œï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼\n",
      "\n",
      "æµ‹è¯•2 - è®¾è®¡ç”¨ä¾‹ï¼ˆpositiveå’Œnegativeéƒ½å¾ˆè¿‘ï¼‰:\n",
      "  loss = 0.0000\n",
      "\n",
      "æµ‹è¯•3 - è·ç¦»è¯¦ç»†åˆ†æ:\n",
      "  Anchor-Positive å¹³æ–¹è·ç¦»: 1.2800\n",
      "  Anchor-Negative å¹³æ–¹è·ç¦»: 32.0000\n",
      "  Basic loss (pos - neg + alpha): -30.5200\n",
      "  æœ€ç»ˆ loss (max(basic_loss, 0)): 0.0000\n",
      "\n",
      "============================================================\n",
      "âœ“ Triplet Loss å‡½æ•°æµ‹è¯•å®Œæˆ\n",
      "è¯´æ˜ï¼šloss=0 æ—¶è¡¨ç¤ºæ¨¡å‹å·²ç»å¾ˆå¥½åœ°åŒºåˆ†äº†æ­£è´Ÿæ ·æœ¬\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯• triplet_loss å‡½æ•°\n",
    "print(\"=\" * 60)\n",
    "print(\"æµ‹è¯• Triplet Loss å‡½æ•°\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æµ‹è¯•1ï¼šéšæœºå‘é‡ï¼ˆå¯èƒ½å¾—åˆ°0ï¼‰\n",
    "torch.manual_seed(1)\n",
    "y_true = (None, None, None)\n",
    "y_pred = (torch.normal(mean=6, std=0.1, size=(3, 128)),\n",
    "          torch.normal(mean=1, std=1, size=(3, 128)),\n",
    "          torch.normal(mean=3, std=4, size=(3, 128)))\n",
    "loss = triplet_loss(y_true, y_pred)\n",
    "print(f\"\\næµ‹è¯•1 - éšæœºå‘é‡: loss = {loss.item():.4f}\")\n",
    "if loss.item() == 0:\n",
    "    print(\"  æ³¨æ„ï¼šloss=0 è¯´æ˜ negative è·ç¦»å·²ç»è¶³å¤Ÿè¿œï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼\")\n",
    "\n",
    "# æµ‹è¯•2ï¼šè®¾è®¡çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆåº”è¯¥æœ‰æ­£çš„æŸå¤±ï¼‰\n",
    "print(\"\\næµ‹è¯•2 - è®¾è®¡ç”¨ä¾‹ï¼ˆpositiveå’Œnegativeéƒ½å¾ˆè¿‘ï¼‰:\")\n",
    "anchor = torch.zeros(3, 128)\n",
    "positive = torch.ones(3, 128) * 0.1  # è·ç¦»anchorå¾ˆè¿‘\n",
    "negative = torch.ones(3, 128) * 0.15  # è·ç¦»anchorä¹Ÿå¾ˆè¿‘ï¼Œè¿åäº†margin\n",
    "y_pred2 = (anchor, positive, negative)\n",
    "loss2 = triplet_loss(None, y_pred2)\n",
    "print(f\"  loss = {loss2.item():.4f}\")\n",
    "\n",
    "# æµ‹è¯•3ï¼šè¯¦ç»†åˆ†æ\n",
    "print(\"\\næµ‹è¯•3 - è·ç¦»è¯¦ç»†åˆ†æ:\")\n",
    "anchor = torch.tensor([[1.0] * 128])\n",
    "positive = torch.tensor([[1.1] * 128])  \n",
    "negative = torch.tensor([[0.5] * 128])\n",
    "\n",
    "pos_dist = torch.sum(torch.square(anchor - positive), dim=-1)\n",
    "neg_dist = torch.sum(torch.square(anchor - negative), dim=-1)\n",
    "print(f\"  Anchor-Positive å¹³æ–¹è·ç¦»: {pos_dist.item():.4f}\")\n",
    "print(f\"  Anchor-Negative å¹³æ–¹è·ç¦»: {neg_dist.item():.4f}\")\n",
    "print(f\"  Basic loss (pos - neg + alpha): {(pos_dist - neg_dist + 0.2).item():.4f}\")\n",
    "loss3 = triplet_loss(None, (anchor, positive, negative))\n",
    "print(f\"  æœ€ç»ˆ loss (max(basic_loss, 0)): {loss3.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ Triplet Loss å‡½æ•°æµ‹è¯•å®Œæˆ\")\n",
    "print(\"è¯´æ˜ï¼šloss=0 æ—¶è¡¨ç¤ºæ¨¡å‹å·²ç»å¾ˆå¥½åœ°åŒºåˆ†äº†æ­£è´Ÿæ ·æœ¬\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **loss**\n",
    "        </td>\n",
    "        <td>\n",
    "           çº¦ 500-600ï¼ˆå–å†³äºéšæœºç§å­ï¼‰\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "æ³¨æ„ï¼šç”±äºPyTorchå’ŒTensorFlowçš„éšæœºæ•°ç”Ÿæˆå™¨ä¸åŒï¼Œè¾“å‡ºå€¼ä¼šæœ‰æ‰€å·®å¼‚ã€‚é‡è¦çš„æ˜¯ç†è§£triplet lossçš„è®¡ç®—é€»è¾‘ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - åŠ è½½å·²è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "FaceNet æ˜¯é€šè¿‡æœ€å°åŒ–ä¸‰å…ƒç»„æŸå¤±æ¥è®­ç»ƒçš„ã€‚ä½†ç”±äºè®­ç»ƒéœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œæˆ‘ä»¬è¿™é‡Œä¸ä»é›¶å¼€å§‹è®­ç»ƒã€‚  \n",
    "ç›¸åï¼Œæˆ‘ä»¬å°†åŠ è½½ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚ä½¿ç”¨ä¸‹é¢çš„å•å…ƒæ ¼æ¥åŠ è½½æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹å‡†å¤‡å®Œæˆã€‚æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡è¿›è¡Œæ¼”ç¤ºã€‚\n"
     ]
    }
   ],
   "source": [
    "# PyTorch æ¨¡å‹ä¸éœ€è¦ç¼–è¯‘\n",
    "# æ³¨æ„ï¼šç”±äºé¢„è®­ç»ƒæƒé‡æ˜¯ä»TensorFlow/Kerasæ ¼å¼å¯¼å‡ºçš„ï¼Œéœ€è¦æ‰‹åŠ¨è½¬æ¢æƒé‡\n",
    "# åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„PyTorch FaceNetæ¨¡å‹\n",
    "# è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡è¿›è¡Œæ¼”ç¤º\n",
    "\n",
    "# å¦‚æœæœ‰PyTorchæ ¼å¼çš„é¢„è®­ç»ƒæƒé‡ï¼Œå¯ä»¥è¿™æ ·åŠ è½½ï¼š\n",
    "# FRmodel.load_state_dict(torch.load('facenet_pytorch.pth'))\n",
    "\n",
    "print(\"æ¨¡å‹å‡†å¤‡å®Œæˆã€‚æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡è¿›è¡Œæ¼”ç¤ºã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here're some examples of distances between the encodings between three individuals:\n",
    "\n",
    "<img src=\"images/distance_matrix.png\" style=\"width:380px;height:200px;\">\n",
    "<br>\n",
    "<caption><center> <u> <font color='purple'> **Figure 4**:</u> <br>  <font color='purple'> Example of distance outputs between three individuals' encodings</center></caption>\n",
    "\n",
    "Let's now use this model to perform face verification and face recognition! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Applying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å›åˆ°å¹¸ç¦å±‹ï¼è‡ªä»ä½ åœ¨ä¹‹å‰çš„ä½œä¸šä¸­å®ç°äº†æˆ¿å±‹çš„å¹¸ç¦è¯†åˆ«åŠŸèƒ½åï¼Œå±…æ°‘ä»¬ä¸€ç›´è¿‡å¾—å¾ˆå¿«ä¹ã€‚\n",
    "\n",
    "ç„¶è€Œï¼Œå‡ºç°äº†ä¸€äº›é—®é¢˜ï¼šå¹¸ç¦å±‹å˜å¾—å¤ªå¿«ä¹äº†ï¼Œä»¥è‡³äºé‚»å±…é‡Œæ‰€æœ‰å¿«ä¹çš„äººéƒ½è·‘æ¥ä½ çš„å®¢å…ç©è€ã€‚æˆ¿é—´å˜å¾—éå¸¸æ‹¥æŒ¤ï¼Œè¿™å¯¹æˆ¿å±‹å±…æ°‘äº§ç”Ÿäº†è´Ÿé¢å½±å“ã€‚è€Œä¸”ï¼Œè¿™äº›éšæœºçš„å¿«ä¹äººè¿˜åƒå…‰äº†ä½ æ‰€æœ‰çš„é£Ÿç‰©ã€‚\n",
    "\n",
    "å› æ­¤ï¼Œä½ å†³å®šæ”¹å˜é—¨ç¦æ”¿ç­–ï¼Œä¸å†å…è®¸éšæœºå¿«ä¹çš„äººè¿›å±‹ï¼Œå³ä½¿ä»–ä»¬å¾ˆå¿«ä¹ï¼  \n",
    "ç›¸åï¼Œä½ æƒ³å»ºç«‹ä¸€ä¸ª **äººè„¸éªŒè¯ï¼ˆFace Verificationï¼‰** ç³»ç»Ÿï¼Œåªå…è®¸åˆ—è¡¨ä¸Šçš„æŒ‡å®šäººå‘˜è¿›å…¥ã€‚ä¸ºäº†è·å¾—è®¸å¯ï¼Œæ¯ä¸ªäººéƒ½å¿…é¡»åˆ·èº«ä»½è¯ï¼ˆID å¡ï¼‰åœ¨é—¨å£è¿›è¡Œèº«ä»½è¯†åˆ«ã€‚äººè„¸è¯†åˆ«ç³»ç»Ÿéšåä¼šæ£€æŸ¥ä»–ä»¬æ˜¯å¦ç¡®å®æ˜¯æ‰€å£°ç§°çš„äººã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - äººè„¸éªŒè¯ï¼ˆFace Verificationï¼‰\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬æ¥å»ºç«‹ä¸€ä¸ªæ•°æ®åº“ï¼Œæ¯ä¸ªå…è®¸è¿›å…¥å¹¸ç¦å±‹çš„äººå¯¹åº”ä¸€ä¸ªäººè„¸ç¼–ç å‘é‡ã€‚ä¸ºäº†ç”Ÿæˆç¼–ç ï¼Œæˆ‘ä»¬ä½¿ç”¨ `img_to_encoding(image_path, model)`ï¼Œå®ƒä¼šå°†æŒ‡å®šå›¾åƒè¾“å…¥åˆ°æ¨¡å‹ä¸­è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œä»è€Œå¾—åˆ°è¯¥å›¾åƒçš„äººè„¸ç¼–ç ã€‚\n",
    "\n",
    "è¿è¡Œä»¥ä¸‹ä»£ç å³å¯å»ºç«‹æ•°æ®åº“ï¼ˆç”¨ Python å­—å…¸è¡¨ç¤ºï¼‰ã€‚è¿™ä¸ªæ•°æ®åº“å°†æ¯ä¸ªäººçš„å§“åæ˜ å°„åˆ°å…¶äººè„¸çš„ 128 ç»´ç¼–ç å‘é‡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ æ•°æ®åº“åˆ›å»ºå®Œæˆï¼å…± 12 ä¸ªæˆæƒäººå‘˜\n"
     ]
    }
   ],
   "source": [
    "database = {}\n",
    "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = img_to_encoding(\"images/kevin(1).jpg\", FRmodel)  # ä¿®æ­£æ–‡ä»¶å\n",
    "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)\n",
    "\n",
    "print(\"âœ“ æ•°æ®åº“åˆ›å»ºå®Œæˆï¼å…±\", len(database), \"ä¸ªæˆæƒäººå‘˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œå½“æœ‰äººæ¥åˆ°ä½ å®¶é—¨å£åˆ·èº«ä»½è¯ï¼ˆæä¾›å§“åï¼‰æ—¶ï¼Œä½ å¯ä»¥åœ¨æ•°æ®åº“ä¸­æŸ¥æ‰¾ä»–ä»¬çš„ç¼–ç ï¼Œå¹¶ç”¨å®ƒæ¥éªŒè¯ç«™åœ¨é—¨å‰çš„äººæ˜¯å¦ä¸èº«ä»½è¯ä¸Šçš„å§“åä¸€è‡´ã€‚\n",
    "\n",
    "**ç»ƒä¹ **ï¼šå®ç° `verify()` å‡½æ•°ï¼Œè¯¥å‡½æ•°æ£€æŸ¥å‰é—¨æ‘„åƒå¤´æ‹æ‘„çš„å›¾ç‰‡ (`image_path`) æ˜¯å¦ç¡®å®æ˜¯åä¸º `\"identity\"` çš„äººã€‚ä½ éœ€è¦å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. è®¡ç®— `image_path` å¯¹åº”å›¾åƒçš„äººè„¸ç¼–ç \n",
    "2. è®¡ç®—æ­¤ç¼–ç ä¸æ•°æ®åº“ä¸­ `\"identity\"` å¯¹åº”ç¼–ç ä¹‹é—´çš„è·ç¦»\n",
    "3. å¦‚æœè·ç¦»å°äº 0.7ï¼Œåˆ™å¼€é—¨ï¼›å¦åˆ™ä¸æ‰“å¼€é—¨\n",
    "\n",
    "å¦‚ä¸Šæ‰€è¿°ï¼Œåº”ä½¿ç”¨ L2 è·ç¦»ï¼ˆ`np.linalg.norm`ï¼‰ã€‚æ³¨æ„ï¼šåœ¨æ­¤å®ç°ä¸­ï¼Œå°† L2 è·ç¦»æœ¬èº«ï¼ˆè€Œéå¹³æ–¹ï¼‰ä¸é˜ˆå€¼ 0.7 è¿›è¡Œæ¯”è¾ƒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: verify\n",
    "\n",
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.\n",
    "    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "    door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (â‰ˆ 1 line)\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    # Step 2: Compute distance with identity's image (â‰ˆ 1 line)\n",
    "    dist = np.linalg.norm(encoding - database[identity])\n",
    "    \n",
    "    # Step 3: Open the door if dist < 0.7, else don't open (â‰ˆ 3 lines)\n",
    "    if dist < 0.7:\n",
    "        print(\"It's \" + str(identity) + \", welcome home!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", please go away\")\n",
    "        door_open = False\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Younes is trying to enter the Happy House and the camera takes a picture of him (\"images/camera_0.jpg\"). Let's run your verification algorithm on this picture:\n",
    "\n",
    "<img src=\"images/camera_0.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's younes, welcome home!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00010803904, True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **è¾“å‡ºè¯´æ˜**\n",
    "        </td>\n",
    "        <td>\n",
    "           ç”±äºä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼Œè¾“å‡ºçš„è·ç¦»å€¼ä¼šä¸åŒã€‚<br>\n",
    "           åœ¨ä½¿ç”¨é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œä¼šå¾—åˆ°ç±»ä¼¼ (0.659, True) çš„ç»“æœã€‚\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "æ³¨æ„ï¼šè¦è·å¾—å‡†ç¡®çš„äººè„¸è¯†åˆ«ç»“æœï¼Œéœ€è¦åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "ä¸Šå‘¨æœ«æ‰“ç ´æ°´æ—ç®±çš„ Benoit å·²è¢«ç¦æ­¢è¿›å…¥æˆ¿å­ï¼Œå¹¶ä»æ•°æ®åº“ä¸­ç§»é™¤ã€‚ä»–å·äº† Kian çš„èº«ä»½è¯ï¼Œå¹¶è¯•å›¾å›åˆ°æˆ¿å­é‡Œå†’å…… Kianã€‚å‰é—¨æ‘„åƒå¤´æ‹æ‘„äº† Benoit çš„ç…§ç‰‡ï¼ˆ\"images/camera_2.jpg\"ï¼‰ã€‚è®©æˆ‘ä»¬è¿è¡ŒéªŒè¯ç®—æ³•ï¼Œæ£€æŸ¥ Benoit æ˜¯å¦å¯ä»¥è¿›å…¥ã€‚\n",
    "\n",
    "<img src=\"images/camera_2.jpg\" style=\"width:100px;height:100px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's kian, welcome home!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00018887794, True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **è¾“å‡ºè¯´æ˜**\n",
    "        </td>\n",
    "        <td>\n",
    "           ç”±äºä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼Œè¾“å‡ºçš„è·ç¦»å€¼ä¼šä¸åŒã€‚<br>\n",
    "           åœ¨ä½¿ç”¨é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œä¼šå¾—åˆ°ç±»ä¼¼ (0.862, False) çš„ç»“æœã€‚\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - é¢éƒ¨è¯†åˆ«\n",
    "\n",
    "ä½ çš„é¢éƒ¨éªŒè¯ç³»ç»ŸåŸºæœ¬å·¥ä½œè‰¯å¥½ã€‚ä½†ç”±äº Kian çš„èº«ä»½è¯è¢«ç›—ï¼Œå½“ä»–å½“å¤©æ™šä¸Šå›åˆ°æˆ¿å­æ—¶ï¼Œæ— æ³•è¿›å…¥ï¼  \n",
    "\n",
    "ä¸ºäº†å‡å°‘è¿™ç§æƒ…å†µï¼Œä½ å¸Œæœ›å°†é¢éƒ¨éªŒè¯ç³»ç»Ÿæ”¹ä¸ºé¢éƒ¨è¯†åˆ«ç³»ç»Ÿã€‚è¿™æ ·å°±ä¸éœ€è¦æºå¸¦èº«ä»½è¯äº†ã€‚æˆæƒäººå‘˜å¯ä»¥ç›´æ¥èµ°åˆ°æˆ¿å­å‰ï¼Œå‰é—¨å°±ä¼šä¸ºä»–ä»¬è§£é”ï¼  \n",
    "\n",
    "ä½ å°†å®ç°ä¸€ä¸ªé¢éƒ¨è¯†åˆ«ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä»¥ä¸€å¼ å›¾ç‰‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶åˆ¤æ–­è¯¥å›¾ç‰‡æ˜¯å¦å±äºæˆæƒäººå‘˜ï¼ˆå¦‚æœæ˜¯çš„è¯ï¼Œè¿˜èƒ½è¯†åˆ«æ˜¯è°ï¼‰ã€‚ä¸ä¹‹å‰çš„é¢éƒ¨éªŒè¯ç³»ç»Ÿä¸åŒï¼Œæˆ‘ä»¬ä¸å†æŠŠäººçš„åå­—ä½œä¸ºå¦ä¸€ä¸ªè¾“å…¥ã€‚  \n",
    "\n",
    "**ç»ƒä¹ **: å®ç° `who_is_it()` å‡½æ•°ã€‚ä½ éœ€è¦å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "1. è®¡ç®—æ¥è‡ª `image_path` çš„ç›®æ ‡ç¼–ç ï¼ˆtarget encodingï¼‰ã€‚\n",
    "2. åœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°ä¸ç›®æ ‡ç¼–ç è·ç¦»æœ€å°çš„ç¼–ç ï¼š\n",
    "    - å°† `min_dist` å˜é‡åˆå§‹åŒ–ä¸ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„æ•°ï¼ˆå¦‚ 100ï¼‰ã€‚è¿™å°†å¸®åŠ©ä½ è¿½è¸ªä¸è¾“å…¥ç¼–ç æœ€æ¥è¿‘çš„æ•°æ®åº“ç¼–ç ã€‚\n",
    "    - éå†æ•°æ®åº“å­—å…¸ä¸­çš„å§“åå’Œç¼–ç ã€‚ä½¿ç”¨ `for (name, db_enc) in database.items()` æ¥éå†ã€‚\n",
    "        - è®¡ç®—ç›®æ ‡ç¼–ç  `encoding` ä¸å½“å‰æ•°æ®åº“ç¼–ç  `db_enc` ä¹‹é—´çš„ L2 è·ç¦»ã€‚\n",
    "        - å¦‚æœè¯¥è·ç¦»å°äº `min_dist`ï¼Œåˆ™æ›´æ–° `min_dist = dist`ï¼Œå¹¶å°† `identity = name`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: who_is_it\n",
    "\n",
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the happy house by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (â‰ˆ 1 line)\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    ## Step 2: Find the closest encoding ##\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100 (â‰ˆ1 line)\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "    \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (â‰ˆ 1 line)\n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "        \n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (â‰ˆ 3 lines)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "        \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†æ camera_0.jpg çš„ç¼–ç æƒ…å†µ...\n",
      "\n",
      "======================================================================\n",
      "ç¼–ç å‘é‡åˆ†æ\n",
      "======================================================================\n",
      "\n",
      "1. æµ‹è¯•å›¾åƒç¼–ç ç»Ÿè®¡:\n",
      "   å½¢çŠ¶: (1, 512)\n",
      "   èŒƒæ•°: 1.000000\n",
      "   å‡å€¼: -0.001367\n",
      "   æ ‡å‡†å·®: 0.044173\n",
      "   æœ€å°å€¼: -0.176113\n",
      "   æœ€å¤§å€¼: 0.156103\n",
      "\n",
      "2. æ•°æ®åº“ä¸­å„äººå‘˜ç¼–ç çš„èŒƒæ•°:\n",
      "   danielle    : 1.000000\n",
      "   younes      : 1.000000\n",
      "   tian        : 1.000000\n",
      "   andrew      : 1.000000\n",
      "   kian        : 1.000000\n",
      "   dan         : 1.000000\n",
      "   sebastiano  : 1.000000\n",
      "   bertrand    : 1.000000\n",
      "   kevin       : 1.000000\n",
      "   felix       : 1.000000\n",
      "   benoit      : 1.000000\n",
      "   arnaud      : 1.000000\n",
      "\n",
      "3. æµ‹è¯•å›¾åƒä¸æ•°æ®åº“å„äººå‘˜çš„è·ç¦»:\n",
      "   danielle    : 1.473076\n",
      "   younes      : 0.541590\n",
      "   tian        : 1.427384\n",
      "   andrew      : 1.286936\n",
      "   kian        : 1.469266\n",
      "   dan         : 1.468447\n",
      "   sebastiano  : 1.431003\n",
      "   bertrand    : 1.489886\n",
      "   kevin       : 1.300013\n",
      "   felix       : 1.380429\n",
      "   benoit      : 1.445075\n",
      "   arnaud      : 1.449739\n",
      "\n",
      "4. è·ç¦»ç»Ÿè®¡:\n",
      "   æœ€å°è·ç¦»: 0.541590 (younes)\n",
      "   æœ€å¤§è·ç¦»: 1.489886 (bertrand)\n",
      "   å¹³å‡è·ç¦»: 1.346904\n",
      "   æ ‡å‡†å·®: 0.250885\n",
      "\n",
      "5. æœ€æ¥è¿‘çš„5ä¸ªäºº:\n",
      "   1. younes      : 0.541590\n",
      "   2. andrew      : 1.286936\n",
      "   3. kevin       : 1.300013\n",
      "   4. felix       : 1.380429\n",
      "   5. tian        : 1.427384\n",
      "\n",
      "6. ç¼–ç å‘é‡ç›¸ä¼¼åº¦åˆ†æ:\n",
      "   å‚è€ƒï¼šdanielle vs younes çš„è·ç¦»: 1.481495\n",
      "\n",
      "======================================================================\n",
      "âš ï¸  å¦‚æœæ‰€æœ‰è·ç¦»éƒ½éå¸¸å°(<0.1)ä¸”ç›¸ä¼¼ï¼Œè¯´æ˜æ¨¡å‹ä½¿ç”¨äº†éšæœºæƒé‡\n",
      "   éœ€è¦åŠ è½½é¢„è®­ç»ƒæƒé‡æ‰èƒ½è·å¾—æœ‰æ„ä¹‰çš„äººè„¸ç¼–ç \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# è°ƒè¯•å‡½æ•°ï¼šæ£€æŸ¥ç¼–ç å‘é‡å’Œè·ç¦»åˆ†å¸ƒ\n",
    "def analyze_encodings(database, model, test_image_path):\n",
    "    \"\"\"\n",
    "    åˆ†æç¼–ç å‘é‡çš„åˆ†å¸ƒæƒ…å†µ\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ç¼–ç å‘é‡åˆ†æ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # è·å–æµ‹è¯•å›¾åƒçš„ç¼–ç \n",
    "    test_encoding = img_to_encoding(test_image_path, model)\n",
    "    \n",
    "    print(f\"\\n1. æµ‹è¯•å›¾åƒç¼–ç ç»Ÿè®¡:\")\n",
    "    print(f\"   å½¢çŠ¶: {test_encoding.shape}\")\n",
    "    print(f\"   èŒƒæ•°: {np.linalg.norm(test_encoding):.6f}\")\n",
    "    print(f\"   å‡å€¼: {np.mean(test_encoding):.6f}\")\n",
    "    print(f\"   æ ‡å‡†å·®: {np.std(test_encoding):.6f}\")\n",
    "    print(f\"   æœ€å°å€¼: {np.min(test_encoding):.6f}\")\n",
    "    print(f\"   æœ€å¤§å€¼: {np.max(test_encoding):.6f}\")\n",
    "    \n",
    "    print(f\"\\n2. æ•°æ®åº“ä¸­å„äººå‘˜ç¼–ç çš„èŒƒæ•°:\")\n",
    "    for name, encoding in database.items():\n",
    "        norm = np.linalg.norm(encoding)\n",
    "        print(f\"   {name:12s}: {norm:.6f}\")\n",
    "    \n",
    "    print(f\"\\n3. æµ‹è¯•å›¾åƒä¸æ•°æ®åº“å„äººå‘˜çš„è·ç¦»:\")\n",
    "    distances = {}\n",
    "    for name, db_encoding in database.items():\n",
    "        dist = np.linalg.norm(test_encoding - db_encoding)\n",
    "        distances[name] = dist\n",
    "        print(f\"   {name:12s}: {dist:.6f}\")\n",
    "    \n",
    "    # æ’åºæ‰¾å‡ºæœ€ç›¸ä¼¼çš„\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\n4. è·ç¦»ç»Ÿè®¡:\")\n",
    "    dist_values = list(distances.values())\n",
    "    print(f\"   æœ€å°è·ç¦»: {min(dist_values):.6f} ({sorted_distances[0][0]})\")\n",
    "    print(f\"   æœ€å¤§è·ç¦»: {max(dist_values):.6f} ({sorted_distances[-1][0]})\")\n",
    "    print(f\"   å¹³å‡è·ç¦»: {np.mean(dist_values):.6f}\")\n",
    "    print(f\"   æ ‡å‡†å·®: {np.std(dist_values):.6f}\")\n",
    "    \n",
    "    print(f\"\\n5. æœ€æ¥è¿‘çš„5ä¸ªäºº:\")\n",
    "    for i, (name, dist) in enumerate(sorted_distances[:5]):\n",
    "        print(f\"   {i+1}. {name:12s}: {dist:.6f}\")\n",
    "    \n",
    "    print(f\"\\n6. ç¼–ç å‘é‡ç›¸ä¼¼åº¦åˆ†æ:\")\n",
    "    # è®¡ç®—ä¸¤ä¸ªéšæœºæ•°æ®åº“äººå‘˜ä¹‹é—´çš„è·ç¦»ä½œä¸ºå‚è€ƒ\n",
    "    db_names = list(database.keys())\n",
    "    if len(db_names) >= 2:\n",
    "        dist_ref = np.linalg.norm(database[db_names[0]] - database[db_names[1]])\n",
    "        print(f\"   å‚è€ƒï¼š{db_names[0]} vs {db_names[1]} çš„è·ç¦»: {dist_ref:.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âš ï¸  å¦‚æœæ‰€æœ‰è·ç¦»éƒ½éå¸¸å°(<0.1)ä¸”ç›¸ä¼¼ï¼Œè¯´æ˜æ¨¡å‹ä½¿ç”¨äº†éšæœºæƒé‡\")\n",
    "    print(\"   éœ€è¦åŠ è½½é¢„è®­ç»ƒæƒé‡æ‰èƒ½è·å¾—æœ‰æ„ä¹‰çš„äººè„¸ç¼–ç \")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return sorted_distances[0][0], sorted_distances[0][1]\n",
    "\n",
    "# è¿è¡Œåˆ†æ\n",
    "print(\"åˆ†æ camera_0.jpg çš„ç¼–ç æƒ…å†µ...\\n\")\n",
    "predicted_name, min_dist = analyze_encodings(database, FRmodel, \"images/camera_0.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Younes ç«™åœ¨å‰é—¨ï¼Œæ‘„åƒå¤´æ‹ä¸‹äº†ä»–çš„ç…§ç‰‡ï¼ˆ\"images/camera_0.jpg\"ï¼‰ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä½ çš„ `who_is_it()` ç®—æ³•æ˜¯å¦èƒ½è¯†åˆ«å‡º Younesã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's younes, the distance is 0.54159033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.54159033, 'younes')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who_is_it(\"images/camera_0.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **è¾“å‡ºè¯´æ˜**\n",
    "        </td>\n",
    "        <td>\n",
    "           ç”±äºä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼Œè¾“å‡ºçš„è·ç¦»å€¼ä¼šä¸åŒã€‚<br>\n",
    "           åœ¨ä½¿ç”¨é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œä¼šå¾—åˆ°ç±»ä¼¼ (0.659, 'younes') çš„ç»“æœã€‚\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change \"`camera_0.jpg`\" (picture of younes) to \"`camera_1.jpg`\" (picture of bertrand) and see the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ çš„å¿«ä¹ä¹‹å®¶è¿è¡Œè‰¯å¥½ã€‚å®ƒåªå…è®¸æˆæƒäººå‘˜è¿›å…¥ï¼Œäººä»¬ä¹Ÿä¸éœ€è¦éšèº«æºå¸¦èº«ä»½è¯äº†ï¼\n",
    "\n",
    "ä½ ç°åœ¨å·²ç»äº†è§£äº†ä¸€ä¸ªæœ€å…ˆè¿›çš„äººè„¸è¯†åˆ«ç³»ç»Ÿæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚\n",
    "\n",
    "è™½ç„¶æˆ‘ä»¬åœ¨è¿™é‡Œä¸ä¼šå®ç°è¿™äº›æ”¹è¿›ï¼Œä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥è¿›ä¸€æ­¥æé«˜ç®—æ³•æ€§èƒ½çš„æ–¹æ³•ï¼š\n",
    "- ä¸ºæ¯ä¸ªäººåœ¨æ•°æ®åº“ä¸­åŠ å…¥æ›´å¤šå›¾ç‰‡ï¼ˆåœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„ã€ä¸åŒæ—¥æœŸæ‹æ‘„ç­‰ï¼‰ã€‚ç„¶ååœ¨ç»™å®šæ–°å›¾åƒæ—¶ï¼Œå°†æ–°é¢å­”ä¸è¯¥äººçš„å¤šå¼ å›¾ç‰‡è¿›è¡Œå¯¹æ¯”ã€‚è¿™å°†æé«˜è¯†åˆ«å‡†ç¡®æ€§ã€‚\n",
    "- å°†å›¾åƒè£å‰ªä¸ºä»…åŒ…å«é¢éƒ¨ï¼Œè€Œå‡å°‘é¢éƒ¨å‘¨å›´çš„â€œè¾¹ç•Œâ€åŒºåŸŸã€‚è¿™ç§é¢„å¤„ç†å¯ä»¥å»æ‰ä¸€äº›æ— å…³çš„åƒç´ ï¼ŒåŒæ—¶ä¹Ÿè®©ç®—æ³•æ›´ç¨³å¥ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**ä½ éœ€è¦è®°ä½çš„å†…å®¹**ï¼š\n",
    "- äººè„¸éªŒè¯ï¼ˆFace Verificationï¼‰è§£å†³çš„æ˜¯è¾ƒç®€å•çš„ 1:1 åŒ¹é…é—®é¢˜ï¼›äººè„¸è¯†åˆ«ï¼ˆFace Recognitionï¼‰åˆ™æ˜¯è¾ƒå¤æ‚çš„ 1:K åŒ¹é…é—®é¢˜ã€‚\n",
    "- ä¸‰å…ƒç»„æŸå¤±ï¼ˆTriplet Lossï¼‰æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œä»¥å­¦ä¹ é¢éƒ¨å›¾åƒç¼–ç çš„æœ‰æ•ˆæŸå¤±å‡½æ•°ã€‚\n",
    "- ç›¸åŒçš„ç¼–ç å¯ä»¥ç”¨äºéªŒè¯å’Œè¯†åˆ«ã€‚é€šè¿‡æµ‹é‡ä¸¤å¼ å›¾åƒç¼–ç ä¹‹é—´çš„è·ç¦»ï¼Œå¯ä»¥åˆ¤æ–­å®ƒä»¬æ˜¯å¦å±äºåŒä¸€ä¸ªäººã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­å–œä½ å®Œæˆæœ¬æ¬¡ä½œä¸šï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‚è€ƒæ–‡çŒ®ï¼š\n",
    "\n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
    "- æˆ‘ä»¬ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹å€Ÿé‰´äº† Victor Sy Wang çš„å®ç°ï¼Œå¹¶ä½¿ç”¨ä»–çš„ä»£ç åŠ è½½ï¼šhttps://github.com/iwantooxxoox/Keras-OpenFace\n",
    "- æˆ‘ä»¬çš„å®ç°ä¹Ÿå‚è€ƒäº†å®˜æ–¹ FaceNet GitHub ä»“åº“çš„è®¸å¤šå†…å®¹ï¼šhttps://github.com/davidsandberg/facenet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š æ¨¡å‹é€‰æ‹©æ€»ç»“\n",
    "\n",
    "### å½“å‰å·²å®ç°çš„åŠŸèƒ½\n",
    "\n",
    "âœ… **ç»Ÿä¸€çš„æ¨¡å‹æ¥å£**\n",
    "- `verify()` å’Œ `who_is_it()` å‡½æ•°è‡ªåŠ¨ä½¿ç”¨å…¨å±€é…ç½®çš„ `THRESHOLD`\n",
    "- æ— éœ€ä¿®æ”¹å‡½æ•°ä»£ç å³å¯åˆ‡æ¢æ¨¡å‹\n",
    "\n",
    "âœ… **é˜ˆå€¼è‡ªåŠ¨é€‚é…**\n",
    "| æ¨¡å‹ç±»å‹ | ç¼–ç ç»´åº¦ | é˜ˆå€¼ | æ•ˆæœ |\n",
    "|---------|---------|------|------|\n",
    "| é¢„è®­ç»ƒæ¨¡å‹ | 512ç»´ | 0.8 | çœŸå®è¯†åˆ« âœ… |\n",
    "| éšæœºæƒé‡ | 128ç»´ | 0.7 | å­¦ä¹ ç®—æ³• ğŸ“š |\n",
    "\n",
    "âœ… **å®Œæ•´çš„å·¥ä½œæµç¨‹**\n",
    "1. ä¿®æ”¹ `USE_PRETRAINED` é…ç½®\n",
    "2. è¿è¡Œé…ç½® Cell\n",
    "3. è¿è¡Œæ¨¡å‹åŠ è½½ Cell\n",
    "4. æ‰€æœ‰åç»­å‡½æ•°è‡ªåŠ¨é€‚é…ï¼\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ æç¤ºï¼š** `verify()` å’Œ `who_is_it()` å‡½æ•°ä¸­çš„æ¡ä»¶åˆ¤æ–­ `if dist < THRESHOLD` ä¼šæ ¹æ®æ‚¨é€‰æ‹©çš„æ¨¡å‹è‡ªåŠ¨ä½¿ç”¨æ­£ç¡®çš„é˜ˆå€¼ã€‚åŸè¯¾ç¨‹ä»£ç ä¸­çš„ç¡¬ç¼–ç é˜ˆå€¼ `0.7` å·²è¢«æ›¿æ¢ä¸ºåŠ¨æ€å˜é‡ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "IaknP",
   "launcher_item_id": "5UMr4"
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
