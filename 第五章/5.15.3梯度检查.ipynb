{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度检查\n",
    "\n",
    "在本节中，你将学习如何实现和使用梯度检查。\n",
    "\n",
    "你所在的团队致力于让移动支付在全球普及，并被要求构建一个深度学习模型用于检测欺诈——当有人进行支付时，你需要判断这笔支付是否存在欺诈行为，比如用户账户是否被黑客入侵。\n",
    "\n",
    "但是反向传播算法的实现非常复杂，有时会出现错误。由于这是一个关键任务应用，你们公司的 CEO 希望确认你的反向传播实现是正确的。CEO 说：“给我一个证据，证明你的反向传播真的有效！”为了给他这个保证，你将使用“梯度检查”技术。\n",
    "\n",
    "让我们开始吧！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的 Python 库\n",
    "\n",
    "import numpy as np  \n",
    "# 导入 NumPy 库，并用别名 np 方便调用\n",
    "# NumPy 是 Python 中进行数值计算的核心库，提供数组运算、矩阵运算、随机数生成等功能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数工具\n",
    "- 可以生成单独的gc_utils.py文件，然后用 from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector 导入 \n",
    "- 包含以下函数：\n",
    "- sigmoid                 -> 实现 sigmoid 激活函数\n",
    "- relu                    -> 实现 ReLU 激活函数\n",
    "- dictionary_to_vector    -> 将参数字典转换成向量（方便梯度检查等操作）\n",
    "- vector_to_dictionary    -> 将向量重新转回参数字典\n",
    "- gradients_to_vector     -> 将梯度字典转换成向量（与 dictionary_to_vector 配合使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def dictionary_to_vector(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
    "        \n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    parameters[\"W1\"] = theta[:20].reshape((5,4))\n",
    "    parameters[\"b1\"] = theta[20:25].reshape((5,1))\n",
    "    parameters[\"W2\"] = theta[25:40].reshape((3,5))\n",
    "    parameters[\"b2\"] = theta[40:43].reshape((3,1))\n",
    "    parameters[\"W3\"] = theta[43:46].reshape((1,3))\n",
    "    parameters[\"b3\"] = theta[46:47].reshape((1,1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\"]:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试用例\n",
    "- 可以生成testCases.py文件，然后用 from testCases import *  导入所有内容\n",
    "- 这些内容一般包含预定义的测试用例，用于验证函数的正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_cost_with_regularization_test_case():\n",
    "    np.random.seed(1)\n",
    "    Y_assess = np.array([[1, 1, 0, 1, 0]])\n",
    "    W1 = np.random.randn(2, 3)\n",
    "    b1 = np.random.randn(2, 1)\n",
    "    W2 = np.random.randn(3, 2)\n",
    "    b2 = np.random.randn(3, 1)\n",
    "    W3 = np.random.randn(1, 3)\n",
    "    b3 = np.random.randn(1, 1)\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
    "    a3 = np.array([[ 0.40682402,  0.01629284,  0.16722898,  0.10118111,  0.40682402]])\n",
    "    return a3, Y_assess, parameters\n",
    "\n",
    "def backward_propagation_with_regularization_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(3, 5)\n",
    "    Y_assess = np.array([[1, 1, 0, 1, 0]])\n",
    "    cache = (np.array([[-1.52855314,  3.32524635,  2.13994541,  2.60700654, -0.75942115],\n",
    "         [-1.98043538,  4.1600994 ,  0.79051021,  1.46493512, -0.45506242]]),\n",
    "  np.array([[ 0.        ,  3.32524635,  2.13994541,  2.60700654,  0.        ],\n",
    "         [ 0.        ,  4.1600994 ,  0.79051021,  1.46493512,  0.        ]]),\n",
    "  np.array([[-1.09989127, -0.17242821, -0.87785842],\n",
    "         [ 0.04221375,  0.58281521, -1.10061918]]),\n",
    "  np.array([[ 1.14472371],\n",
    "         [ 0.90159072]]),\n",
    "  np.array([[ 0.53035547,  5.94892323,  2.31780174,  3.16005701,  0.53035547],\n",
    "         [-0.69166075, -3.47645987, -2.25194702, -2.65416996, -0.69166075],\n",
    "         [-0.39675353, -4.62285846, -2.61101729, -3.22874921, -0.39675353]]),\n",
    "  np.array([[ 0.53035547,  5.94892323,  2.31780174,  3.16005701,  0.53035547],\n",
    "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]),\n",
    "  np.array([[ 0.50249434,  0.90085595],\n",
    "         [-0.68372786, -0.12289023],\n",
    "         [-0.93576943, -0.26788808]]),\n",
    "  np.array([[ 0.53035547],\n",
    "         [-0.69166075],\n",
    "         [-0.39675353]]),\n",
    "  np.array([[-0.3771104 , -4.10060224, -1.60539468, -2.18416951, -0.3771104 ]]),\n",
    "  np.array([[ 0.40682402,  0.01629284,  0.16722898,  0.10118111,  0.40682402]]),\n",
    "  np.array([[-0.6871727 , -0.84520564, -0.67124613]]),\n",
    "  np.array([[-0.0126646]]))\n",
    "    return X_assess, Y_assess, cache\n",
    "\n",
    "def forward_propagation_with_dropout_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(3, 5)\n",
    "    W1 = np.random.randn(2, 3)\n",
    "    b1 = np.random.randn(2, 1)\n",
    "    W2 = np.random.randn(3, 2)\n",
    "    b2 = np.random.randn(3, 1)\n",
    "    W3 = np.random.randn(1, 3)\n",
    "    b3 = np.random.randn(1, 1)\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
    "    \n",
    "    return X_assess, parameters\n",
    "\n",
    "def backward_propagation_with_dropout_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(3, 5)\n",
    "    Y_assess = np.array([[1, 1, 0, 1, 0]])\n",
    "    cache = (np.array([[-1.52855314,  3.32524635,  2.13994541,  2.60700654, -0.75942115],\n",
    "           [-1.98043538,  4.1600994 ,  0.79051021,  1.46493512, -0.45506242]]), np.array([[ True, False,  True,  True,  True],\n",
    "           [ True,  True,  True,  True, False]], dtype=bool), np.array([[ 0.        ,  0.        ,  4.27989081,  5.21401307,  0.        ],\n",
    "           [ 0.        ,  8.32019881,  1.58102041,  2.92987024,  0.        ]]), np.array([[-1.09989127, -0.17242821, -0.87785842],\n",
    "           [ 0.04221375,  0.58281521, -1.10061918]]), np.array([[ 1.14472371],\n",
    "           [ 0.90159072]]), np.array([[ 0.53035547,  8.02565606,  4.10524802,  5.78975856,  0.53035547],\n",
    "           [-0.69166075, -1.71413186, -3.81223329, -4.61667916, -0.69166075],\n",
    "           [-0.39675353, -2.62563561, -4.82528105, -6.0607449 , -0.39675353]]), np.array([[ True, False,  True, False,  True],\n",
    "           [False,  True, False,  True,  True],\n",
    "           [False, False,  True, False, False]], dtype=bool), np.array([[ 1.06071093,  0.        ,  8.21049603,  0.        ,  1.06071093],\n",
    "           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]), np.array([[ 0.50249434,  0.90085595],\n",
    "           [-0.68372786, -0.12289023],\n",
    "           [-0.93576943, -0.26788808]]), np.array([[ 0.53035547],\n",
    "           [-0.69166075],\n",
    "           [-0.39675353]]), np.array([[-0.7415562 , -0.0126646 , -5.65469333, -0.0126646 , -0.7415562 ]]), np.array([[ 0.32266394,  0.49683389,  0.00348883,  0.49683389,  0.32266394]]), np.array([[-0.6871727 , -0.84520564, -0.67124613]]), np.array([[-0.0126646]]))\n",
    "\n",
    "\n",
    "    return X_assess, Y_assess, cache\n",
    "\n",
    "def gradient_check_n_test_case(): \n",
    "    np.random.seed(1)\n",
    "    x = np.random.randn(4,3)\n",
    "    y = np.array([1, 1, 0])\n",
    "    W1 = np.random.randn(5,4) \n",
    "    b1 = np.random.randn(5,1) \n",
    "    W2 = np.random.randn(3,5) \n",
    "    b2 = np.random.randn(3,1) \n",
    "    W3 = np.random.randn(1,3) \n",
    "    b3 = np.random.randn(1,1) \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    return x, y, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 梯度检查的原理是什么？\r\n",
    "\r\n",
    "反向传播计算梯度 $\\frac{\\partial J}{\\partial \\theta}$，其中 $\\theta$ 表示模型的参数。$J$ 是通过前向传播和损失函数计算得到的。\r\n",
    "\r\n",
    "因为前向传播相对容易实现，你对其实现非常有信心，因此几乎可以100%确定你计算的损失 $J$ 是正确的。  \r\n",
    "所以你可以利用计算 $J$ 的代码来验证计算梯度 $\\frac{\\partial J}{\\partial \\theta}$ 的代码是否正确。\r\n",
    "\r\n",
    "回顾导数（或梯度）的定义：\r\n",
    "$$ \r\n",
    "\\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}\r\n",
    "$$\r\n",
    "\r\n",
    "如果你不熟悉 \"$\\displaystyle \\lim_{\\varepsilon \\to 0}$\" 的写法，它只是表示“当 $\\varepsilon$ 非常非常小时”。\r\n",
    "\r\n",
    "我们知道：\r\n",
    "\r\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ 是你想确保计算正确的值。\r\n",
    "- 你可以计算 $J(\\theta + \\varepsilon)$ 和 $J(\\theta - \\varepsilon)$（当 $\\theta$ 是实数时），因为你对计算 $J$ 的实现非常有信心。\r\n",
    "\r\n",
    "现在，我们用公式 (1) 和一个很小的 $\\varepsilon$，来向你的 CEO 证明你计算 $\\frac{\\partial J}{\\partial \\theta}$ 的代码是正确的！\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 一维梯度检查\r\n",
    "\r\n",
    "考虑一个一维线性函数 $J(\\theta) = \\theta x$。模型只包含一个实数参数 $\\theta$，输入为 $x$。\r\n",
    "\r\n",
    "你将实现计算 $J(.)$ 及其导数 $\\frac{\\partial J}{\\partial \\theta}$ 的代码。然后利用梯度检查确保你对 $J$ 的导数计算是正确的。\r\n",
    "\r\n",
    "<img src=\"images/1Dgrad_kiank.png\" style=\"width:600px;height:250px;\">\r\n",
    "<caption><center> <u> 图1 </u>：一维线性模型示意图<br> </center></caption>\r\n",
    "\r\n",
    "上图展示了关键计算步骤：先用 $x$ 计算函数值 $J(x)$（“前向传播”），再计算其对参数 $\\theta$ 的导数 $\\frac{\\partial J}{\\partial \\theta}$（“反向传播”）。\r\n",
    "\r\n",
    "**练习**：实现该简单函数的“前向传播”和“反向传播”，即分别用两个函数计算 $J(.)$（前向传播）及其对 $\\theta$ 的导数（反向传播）。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    实现一个简单的线性前向传播，计算代价函数 J\n",
    "    在这里：J(θ) = θ * x\n",
    "\n",
    "    参数：\n",
    "    x      -- 一个实数，表示输入值\n",
    "    theta  -- 一个实数，表示参数 θ\n",
    "\n",
    "    返回：\n",
    "    J      -- 代价函数的值，根据公式 J(θ) = θ * x 计算\n",
    "    \"\"\"\n",
    "\n",
    "    # np.dot(theta, x) 表示向量或标量的点乘运算\n",
    "    # 在这里 theta 和 x 都是实数，所以 np.dot(theta, x) 实际上就是 θ × x\n",
    "    J = np.dot(theta, x)\n",
    "\n",
    "    # 返回计算得到的 J 值\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：现在实现图1中的反向传播步骤（导数计算）。即计算函数 $J(\\theta) = \\theta x$ 关于参数 $\\theta$ 的导数。为了帮你省去推导过程，结果应该是：  \r\n",
    "$$\r\n",
    "d\\theta = \\frac{\\partial J}{\\partial \\theta} = x\r\n",
    "$$\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    计算代价函数 J 相对于参数 θ 的导数（梯度）。\n",
    "    在这里，J(θ) = θ * x，所以 ∂J/∂θ = x。\n",
    "\n",
    "    参数：\n",
    "        x      -- 一个实数，表示输入值\n",
    "        theta  -- 一个实数，表示参数 θ\n",
    "\n",
    "    返回：\n",
    "        dtheta -- 相对于 θ 的成本梯度（即 ∂J/∂θ）\n",
    "    \"\"\"\n",
    "\n",
    "    # 因为 J(θ) = θ * x，所以对 θ 求导：\n",
    "    # ∂J/∂θ = x\n",
    "    # 在这里直接令 dtheta = x\n",
    "    dtheta = x\n",
    "\n",
    "    # 返回计算得到的梯度 dtheta\n",
    "    return dtheta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：为了验证 `backward_propagation()` 函数是否正确计算了梯度 $\\frac{\\partial J}{\\partial \\theta}$，我们来实现梯度检查。\r\n",
    "\r\n",
    "**说明**：  \r\n",
    "- 首先使用公式 (1) 和一个很小的 $\\varepsilon$ 计算“gradapprox”。步骤如下：\r\n",
    "    1. 计算 $\\theta^{+} = \\theta + \\varepsilon$\r\n",
    "    2. 计算 $\\theta^{-} = \\theta - \\varepsilon$\r\n",
    "    3. 计算 $J^{+} = J(\\theta^{+})$\r\n",
    "    4. 计算 $J^{-} = J(\\theta^{-})$\r\n",
    "    5. 计算近似梯度：$gradapprox = \\frac{J^{+} - J^{-}}{2 \\varepsilon}$\r\n",
    "- 然后用反向传播计算梯度，结果存入变量 `grad`\r\n",
    "- 最后计算 `gradapprox` 和 `grad` 的相对差异，公式为：\r\n",
    "$$\r\n",
    "difference = \\frac{\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2} \\tag{2}\r\n",
    "$$\r\n",
    "计算此公式需要三步：\r\n",
    "   - 1'. 用 `np.linalg.norm(...)` 计算分子\r\n",
    "   - 2'. 用 `np.linalg.norm(...)` 两次计算分母的两部分\r\n",
    "   - 3'. 两者相除\r\n",
    "\r\n",
    "- 如果 `difference` 很小（比如小于 $10^{-7}$），你可以非常有信心你的梯度计算是正确的。否则，梯度计算可能有错误。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    实现梯度检查（Gradient Checking），用数值方法近似计算梯度，并与反向传播计算结果进行对比。\n",
    "\n",
    "    参数：\n",
    "        x        -- 一个实数输入\n",
    "        theta    -- 参数 θ（实数）\n",
    "        epsilon  -- 极小的偏移值，用于计算数值近似梯度（默认 1e-7）\n",
    "\n",
    "    返回：\n",
    "        difference -- 数值梯度与反向传播梯度之间的相对误差\n",
    "    \"\"\"\n",
    "\n",
    "    # ====== 使用数值方法计算近似梯度 gradapprox ======\n",
    "\n",
    "    # Step 1: θ 加上微小值 epsilon，得到 thetaplus\n",
    "    thetaplus = theta + epsilon\n",
    "\n",
    "    # Step 2: θ 减去微小值 epsilon，得到 thetaminus\n",
    "    thetaminus = theta - epsilon\n",
    "\n",
    "    # Step 3: 计算 J_plus = J(thetaplus)\n",
    "    #         这里 forward_propagation 用来计算代价函数值\n",
    "    J_plus = forward_propagation(x, thetaplus)\n",
    "\n",
    "    # Step 4: 计算 J_minus = J(thetaminus)\n",
    "    J_minus = forward_propagation(x, thetaminus)\n",
    "\n",
    "    # Step 5: 根据公式 gradapprox ≈ [J(θ+ε) - J(θ-ε)] / (2ε)\n",
    "    gradapprox = (J_plus - J_minus) / (2 * epsilon)\n",
    "\n",
    "\n",
    "    # ====== 使用反向传播计算梯度 grad ======\n",
    "\n",
    "    # 调用 backward_propagation 得到解析解梯度 grad\n",
    "    grad = backward_propagation(x, theta)\n",
    "\n",
    "\n",
    "    # ====== 比较数值梯度 gradapprox 与 解析梯度 grad ======\n",
    "\n",
    "    # Step 1': 计算两者差的 L2 范数（||grad - gradapprox||）\n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "\n",
    "    # Step 2': 计算两者和的 L2 范数（||grad|| + ||gradapprox||）\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "\n",
    "    # Step 3': 计算相对误差 difference\n",
    "    difference = numerator / denominator\n",
    "\n",
    "\n",
    "    # ====== 输出梯度检查结果 ======\n",
    "    if difference < 1e-7:\n",
    "        print(\"梯度检查：梯度正常!\")   # 误差极小，梯度实现正确\n",
    "    else:\n",
    "        print(\"梯度检查：梯度超出阈值!\")  # 误差较大，可能梯度实现有问题\n",
    "\n",
    "    # 返回 difference 值\n",
    "    return difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度检查：梯度正常!\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜！计算出的差异小于 $10^{-7}$ 的阈值，因此你可以非常有信心 `backward_propagation()` 中的梯度计算是正确的。\r\n",
    "\r\n",
    "现在，更一般的情况是，你的代价函数 $J$ 具有多维输入。在训练神经网络时，参数 $\\theta$ 实际上包括多个权重矩阵 $W^{[l]}$ 和偏置向量 $b^{[l]}$！  \r\n",
    "了解如何对高维输入做梯度检查非常重要。让我们开始吧！\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 多维梯度检查\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "下图描述了你的欺诈检测模型的前向传播和反向传播过程。\r\n",
    "\r\n",
    "<img src=\"images/NDgrad_kiank.png\" style=\"width:600px;height:400px;\">\r\n",
    "<caption><center> <u> 图2 </u>：深度神经网络<br>*线性层 -> ReLU激活 -> 线性层 -> ReLU激活 -> 线性层 -> Sigmoid激活*</center></caption>\r\n",
    "\r\n",
    "接下来，让我们来看一下你实现的前向传播和反向传播代码。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    实现前向传播（并计算成本）\n",
    "\n",
    "    参数：\n",
    "        X -- 输入数据，形状为 (输入特征数, 样本数量 m)\n",
    "        Y -- 标签数据，形状为 (1, m)，取值为 0 或 1\n",
    "        parameters -- 字典，包含模型的权重和偏置：\n",
    "            W1 -- 第一层权重矩阵，形状为 (5, 4)\n",
    "            b1 -- 第一层偏置向量，形状为 (5, 1)\n",
    "            W2 -- 第二层权重矩阵，形状为 (3, 5)\n",
    "            b2 -- 第二层偏置向量，形状为 (3, 1)\n",
    "            W3 -- 第三层权重矩阵，形状为 (1, 3)\n",
    "            b3 -- 第三层偏置向量，形状为 (1, 1)\n",
    "\n",
    "    返回：\n",
    "        cost -- 使用逻辑回归公式计算得到的成本值（标量）\n",
    "        cache -- 包含中间计算结果的元组，用于反向传播\n",
    "    \"\"\"\n",
    "    \n",
    "    # 获取样本数量 m（X.shape[1] 表示列数，即样本数）\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # 从参数字典中取出各层的权重和偏置\n",
    "    W1 = parameters[\"W1\"]  # 第一层权重\n",
    "    b1 = parameters[\"b1\"]  # 第一层偏置\n",
    "    W2 = parameters[\"W2\"]  # 第二层权重\n",
    "    b2 = parameters[\"b2\"]  # 第二层偏置\n",
    "    W3 = parameters[\"W3\"]  # 第三层权重\n",
    "    b3 = parameters[\"b3\"]  # 第三层偏置\n",
    "    \n",
    "    # ====== 第一层 LINEAR -> RELU ======\n",
    "    # Z1 = W1·X + b1   （矩阵乘法 + 广播加法）\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    # A1 = ReLU(Z1)    （激活函数）\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    # ====== 第二层 LINEAR -> RELU ======\n",
    "    # Z2 = W2·A1 + b2\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    # A2 = ReLU(Z2)\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    # ====== 第三层 LINEAR -> SIGMOID ======\n",
    "    # Z3 = W3·A2 + b3\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    # A3 = sigmoid(Z3) （输出为预测值，取值范围 0~1）\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    # ====== 计算逻辑回归的成本函数 ======\n",
    "    # logprobs = -Y*log(A3) - (1-Y)*log(1-A3)  （逐元素计算）\n",
    "    logprobs = np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    # 对所有样本求均值，得到最终成本\n",
    "    cost = (1 / m) * np.sum(logprobs)\n",
    "    \n",
    "    # 保存中间计算结果到 cache（便于反向传播）\n",
    "    cache = (Z1, A1, W1, b1,\n",
    "             Z2, A2, W2, b2,\n",
    "             Z3, A3, W3, b3)\n",
    "\n",
    "    # 返回成本和缓存\n",
    "    return cost, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，运行反向传播。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    实现图中所示的反向传播（Backward Propagation）。\n",
    "\n",
    "    参数：\n",
    "        X -- 输入数据，形状为 (输入节点数, 样本数 m)\n",
    "        Y -- 标签（真实值），形状为 (输出节点数, 样本数 m)\n",
    "        cache -- 来自 forward_propagation_n() 的缓存变量元组，\n",
    "                 包含前向传播中所有需要反向传播计算的中间变量\n",
    "\n",
    "    返回：\n",
    "        gradients -- 一个字典，存储了每一层的梯度：\n",
    "                     dW1, db1, dW2, db2, dW3, db3 以及中间变量的梯度\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取样本数量 m（用列数 shape[1] 表示）\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # 从 cache 中解包前向传播的中间结果\n",
    "    (Z1, A1, W1, b1, \n",
    "     Z2, A2, W2, b2, \n",
    "     Z3, A3, W3, b3) = cache\n",
    "\n",
    "    # ===== 第3层（输出层）梯度计算 =====\n",
    "    # dZ3 = A3 - Y，因为最后一层是 sigmoid + 交叉熵，导数公式简化为预测值 - 真实值\n",
    "    dZ3 = A3 - Y\n",
    "\n",
    "    # dW3 = 1/m * dZ3 @ A2^T\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "\n",
    "    # db3 = 1/m * ∑(dZ3) （按列求和，保持维度）\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    # ===== 第2层梯度计算 =====\n",
    "    # dA2 = W3^T @ dZ3 （链式法则）\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "\n",
    "    # dZ2 = dA2 * ReLU'(Z2)，ReLU 的导数是 Z>0 时为1，否则为0\n",
    "    # 这里用 np.int64(A2 > 0) 代替 Z2>0，因为 A2=ReLU(Z2) 时正值对应正输入\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "\n",
    "    # dW2 = 1/m * dZ2 @ A1^T\n",
    "    # 注意这里额外乘了 2（为了制造梯度检查的错误，用于测试）\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2\n",
    "\n",
    "    # db2 = 1/m * ∑(dZ2)\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    # ===== 第1层梯度计算 =====\n",
    "    # dA1 = W2^T @ dZ2\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "\n",
    "    # dZ1 = dA1 * ReLU'(Z1)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "\n",
    "    # dW1 = 1/m * dZ1 @ X^T\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "\n",
    "    # db1 = 4/m * ∑(dZ1) （这里的 4 也是故意加错的，方便做梯度检查测试）\n",
    "    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    # ===== 将所有梯度存到字典中 =====\n",
    "    gradients = {\n",
    "        \"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "        \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "        \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1\n",
    "    }\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "你在欺诈检测测试集上得到了某些结果，但你对自己的模型还不是百分之百确定。没人是完美的！  \r\n",
    "让我们实现梯度检查（gradient checking）来验证你的梯度是否正确。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梯度检查是如何工作的？**\r\n",
    "\r\n",
    "如步骤 1) 和 2) 所示，你希望将 \"gradapprox\" 与反向传播计算的梯度进行比较。公式仍然是：\r\n",
    "\r\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\r\n",
    "\r\n",
    "但是，$\\theta$ 不再是标量，它是一个名为 \"parameters\" 的字典。我们为你实现了一个函数 `dictionary_to_vector()`，它将 \"parameters\" 字典转换为一个向量 \"values\"，方法是将所有参数（W1, b1, W2, b2, W3, b3）展开成向量并连接起来。\r\n",
    "\r\n",
    "对应的逆函数是 `vector_to_dictionary()`，可以将向量转换回 \"parameters\" 字典。\r\n",
    "\r\n",
    "<img src=\"images/dictionary_to_vector.png\" style=\"width:600px;height:400px;\">\r\n",
    "<caption><center> <u> **图 2** </u>: **dictionary_to_vector() 与 vector_to_dictionary()**<br> 你将在 `gradient_check_n()` 中使用这些函数</center></caption>\r\n",
    "\r\n",
    "我们还将 \"gradients\" 字典转换为向量 \"grad\"，使用了 `gradients_to_vector()`。你不需要担心这部分。\r\n",
    "\r\n",
    "**练习**：实现 `gradient_check_n()`。\r\n",
    "\r\n",
    "**说明**：以下伪代码将帮助你实现梯度检查。\r\n",
    "\r\n",
    "对于每个 i 在 num_parameters 中：\r\n",
    "- 计算 $J_{plus}[i]$：\r\n",
    "    1. 设置 $\\theta^{+} = \\text{np.copy(parameters\\_values)}$  \r\n",
    "    2. 将 $\\theta^{+}_i$ 设置为 $\\theta^{+}_i + \\varepsilon$  \r\n",
    "    3. 使用 `forward_propagation_n(x, y, vector_to_dictionary(\\theta^{+}))` 计算 $J^{+}_i$  \r\n",
    "- 计算 $J_{minus}[i]$：对 $\\theta^{-}$ 做相同操作  \r\n",
    "- 计算 $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\r\n",
    "\r\n",
    "这样，你得到一个向量 gradapprox，其中 gradapprox[i] 是参数 parameter_values[i] 对应梯度的近似值。现在，你可以将这个 gradapprox 向量与反向传播得到的 gradients 向量进行比较。就像一维情况（步骤 1', 2', 3'）一样，计算：\r\n",
    "\r\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梯度检查是如何工作的？**\n",
    "\n",
    "如同1）和2）节所述，你希望将“gradapprox”与反向传播计算的梯度进行比较。公式依然是：\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}\n",
    "$$\n",
    "\n",
    "不过，这次的 $\\theta$ 不再是标量，而是一个名为“parameters”的字典。我们为你实现了函数 `dictionary_to_vector()`，它能将“parameters”字典中的所有参数（$W^{[1]}$, $b^{[1]}$, $W^{[2]}$, $b^{[2]}$, $W^{[3]}$, $b^{[3]}$）展平并拼接成一个向量 `values`。\n",
    "\n",
    "对应的逆函数是 `vector_to_dictionary()`，可以将向量还原成参数字典。\n",
    "\n",
    "<img src=\"images/dictionary_to_vector.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> 图2 </u>：`dictionary_to_vector()` 与 `vector_to_dictionary()` <br> 你将在 `gradient_check_n()` 中用到这些函数 </center></caption>\n",
    "\n",
    "我们还用 `gradients_to_vector()` 将梯度字典转换为向量 `grad`，你不需要担心这部分细节。\n",
    "\n",
    "---\n",
    "\n",
    "**练习**：实现 `gradient_check_n()`。\n",
    "\n",
    "**说明**：下面是实现梯度检查的伪代码：\n",
    "\n",
    "对参数向量的每个元素 i：\n",
    "- 计算 $J^+_i$：\n",
    "  1. 将$\\theta^{+}$设为 `np.copy(parameters_values)`  \n",
    "  2. 将$\\theta^{+}_i$设为$\\theta^{+}_i + \\varepsilon$\n",
    "  3. 使用 forward_propagation_n(x, y, vector_to_dictionary($\\theta^{+}$))计算\n",
    "- 计算 $J^-_i$，方法同上，但减去 $\\varepsilon$\n",
    "- 计算梯度近似：\n",
    "$$\n",
    "gradapprox[i] = \\frac{J^+_i - J^-_i}{2 \\varepsilon}\n",
    "$$\n",
    "\n",
    "这样得到一个梯度近似向量 `gradapprox`，其中 `gradapprox[i]` 是对参数向量第 i 个元素梯度的估计值。现在你可以把它和反向传播计算得到的梯度向量 `grad` 进行比较。就像1维情况一样，计算相对差异：\n",
    "\n",
    "$$\n",
    "difference = \\frac{\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2} \\tag{3}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    对 backward_propagation_n 函数计算的梯度进行梯度检查。\n",
    "    \n",
    "    参数：\n",
    "        parameters -- 包含参数 \"W1\",\"b1\",\"W2\",\"b2\",\"W3\",\"b3\" 的字典\n",
    "        gradients -- backward_propagation_n 输出的梯度字典\n",
    "        X -- 输入数据集，形状为 (输入节点数, 样本数)\n",
    "        Y -- 标签数据集，形状为 (输出节点数, 样本数)\n",
    "        epsilon -- 计算数值梯度的微小偏移量\n",
    "    \n",
    "    返回：\n",
    "        difference -- 数值梯度与反向传播梯度之间的差异\n",
    "    \"\"\"\n",
    "    \n",
    "    # 将字典参数展平为向量形式\n",
    "    parameters_values, keys = dictionary_to_vector(parameters)  # keys 用于后续映射，但这里不使用\n",
    "    # 将反向传播得到的梯度展平为向量\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    # 参数总数量\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "\n",
    "    # 初始化数值梯度存储向量\n",
    "    J_plus = np.zeros((num_parameters,1))       # 存储向量中每个参数加 epsilon 后的成本\n",
    "    J_minus = np.zeros((num_parameters,1))      # 存储向量中每个参数减 epsilon 后的成本\n",
    "    gradapprox = np.zeros((num_parameters,1))   # 存储数值梯度\n",
    "\n",
    "    # 计算每个参数的数值梯度\n",
    "    for i in range(num_parameters):\n",
    "        # ----------------- Step 1: J_plus -----------------\n",
    "        thetaplus = np.copy(parameters_values)     # 复制参数向量\n",
    "        thetaplus[i][0] += epsilon                 # 当前参数加 epsilon\n",
    "        # 使用加 epsilon 的参数计算前向传播成本\n",
    "        J_plus[i], cache = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))  # cache 用不到\n",
    "\n",
    "        # ----------------- Step 2: J_minus -----------------\n",
    "        thetaminus = np.copy(parameters_values)    # 复制参数向量\n",
    "        thetaminus[i][0] -= epsilon                # 当前参数减 epsilon\n",
    "        # 使用减 epsilon 的参数计算前向传播成本\n",
    "        J_minus[i], cache = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))  # cache 用不到\n",
    "\n",
    "        # ----------------- Step 3: 数值梯度 -----------------\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "\n",
    "    # ----------------- Step 4: 比较梯度 -----------------\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                  # 计算梯度差的范数\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox) # 分母为梯度范数和数值梯度范数之和\n",
    "    difference = numerator / denominator                             # 最终差异度量\n",
    "\n",
    "    # 输出梯度检查结果\n",
    "    if difference > 1e-7:\n",
    "        print (\"\\033[93m\" + \"反向传播存在错误! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"反向传播正确! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m反向传播存在错误! difference = 0.2850931566540251\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起来我们给你的 `backward_propagation_n` 代码中存在错误！幸好你实现了梯度检查。返回 `backward_propagation` 尝试找到并修正错误 *(提示：检查 dW2 和 db1)*。当你认为修正完成后，重新运行梯度检查。记住，如果修改了代码，需要重新执行定义 `backward_propagation_n()` 的单元格。  \n",
    "\n",
    "\n",
    "**注意**  \n",
    "- 梯度检查速度慢！使用 $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ 近似梯度计算非常耗时。因此，我们不会在训练的每次迭代中运行梯度检查，只会偶尔运行以确认梯度是否正确。  \n",
    "- 至少按照我们展示的方法，梯度检查不适用于 dropout。通常，你会在不使用 dropout 的情况下运行梯度检查，以确保反向传播正确，然后再加入 dropout。\n",
    "\n",
    "恭喜，你可以确认你的深度学习欺诈检测模型已正确工作！你甚至可以用它来说服你的 CEO。 :)\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "**你应该从本节中记住的内容**：  \n",
    "- 梯度检查用于验证反向传播计算得到的梯度与通过前向传播计算的梯度近似值之间的接近程度。  \n",
    "- 梯度检查速度慢，因此我们不会在训练的每次迭代中运行它。通常仅用于确认代码正确，然后关闭梯度检查，使用反向传播进行实际学习。\n",
    "</font>\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "n6NBD",
   "launcher_item_id": "yfOsE"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
