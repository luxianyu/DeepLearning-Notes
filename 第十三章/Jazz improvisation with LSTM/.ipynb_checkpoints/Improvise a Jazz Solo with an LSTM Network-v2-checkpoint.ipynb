{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用 LSTM 网络即兴演奏爵士独奏\n",
    "\n",
    "欢迎来到本周的最后一个编程作业！在这个 notebook 中，你将实现一个使用 LSTM 生成音乐的模型。完成作业后，你甚至可以听到自己生成的音乐。\n",
    "\n",
    "**你将学习：**\n",
    "- 将 LSTM 应用于音乐生成。\n",
    "- 使用深度学习生成你自己的爵士音乐。\n",
    "\n",
    "请运行以下单元以加载本次作业所需的所有包。这可能需要几分钟时间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author:     Ji-Sung Kim, Evan Chow\n",
    "Project:    jazzml / (used in) deepjazz\n",
    "Purpose:    Extract, manipulate, process musical grammar\n",
    "\n",
    "Directly taken then cleaned up from Evan Chow's jazzml, \n",
    "https://github.com/evancchow/jazzml,with permission.\n",
    "'''\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from itertools import groupby\n",
    "from music21 import *\n",
    "import copy, random, pdb\n",
    "\n",
    "#from preprocess import *\n",
    "\n",
    "''' Helper function to determine if a note is a scale tone. '''\n",
    "def __is_scale_tone(chord, note):\n",
    "    # Method: generate all scales that have the chord notes th check if note is\n",
    "    # in names\n",
    "\n",
    "    # Derive major or minor scales (minor if 'other') based on the quality\n",
    "    # of the chord.\n",
    "    scaleType = scale.DorianScale() # i.e. minor pentatonic\n",
    "    if chord.quality == 'major':\n",
    "        scaleType = scale.MajorScale()\n",
    "    # Can change later to deriveAll() for flexibility. If so then use list\n",
    "    # comprehension of form [x for a in b for x in a].\n",
    "    scales = scaleType.derive(chord) # use deriveAll() later for flexibility\n",
    "    allPitches = list(set([pitch for pitch in scales.getPitches()]))\n",
    "    allNoteNames = [i.name for i in allPitches] # octaves don't matter\n",
    "\n",
    "    # Get note name. Return true if in the list of note names.\n",
    "    noteName = note.name\n",
    "    return (noteName in allNoteNames)\n",
    "\n",
    "''' Helper function to determine if a note is an approach tone. '''\n",
    "def __is_approach_tone(chord, note):\n",
    "    # Method: see if note is +/- 1 a chord tone.\n",
    "\n",
    "    for chordPitch in chord.pitches:\n",
    "        stepUp = chordPitch.transpose(1)\n",
    "        stepDown = chordPitch.transpose(-1)\n",
    "        if (note.name == stepDown.name or \n",
    "            note.name == stepDown.getEnharmonic().name or\n",
    "            note.name == stepUp.name or\n",
    "            note.name == stepUp.getEnharmonic().name):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "''' Helper function to determine if a note is a chord tone. '''\n",
    "def __is_chord_tone(lastChord, note):\n",
    "    return (note.name in (p.name for p in lastChord.pitches))\n",
    "\n",
    "''' Helper function to generate a chord tone. '''\n",
    "def __generate_chord_tone(lastChord):\n",
    "    lastChordNoteNames = [p.nameWithOctave for p in lastChord.pitches]\n",
    "    return note.Note(random.choice(lastChordNoteNames))\n",
    "\n",
    "''' Helper function to generate a scale tone. '''\n",
    "def __generate_scale_tone(lastChord):\n",
    "    # Derive major or minor scales (minor if 'other') based on the quality\n",
    "    # of the lastChord.\n",
    "    scaleType = scale.WeightedHexatonicBlues() # minor pentatonic\n",
    "    if lastChord.quality == 'major':\n",
    "        scaleType = scale.MajorScale()\n",
    "    # Can change later to deriveAll() for flexibility. If so then use list\n",
    "    # comprehension of form [x for a in b for x in a].\n",
    "    scales = scaleType.derive(lastChord) # use deriveAll() later for flexibility\n",
    "    allPitches = list(set([pitch for pitch in scales.getPitches()]))\n",
    "    allNoteNames = [i.name for i in allPitches] # octaves don't matter\n",
    "\n",
    "    # Return a note (no octave here) in a scale that matches the lastChord.\n",
    "    sNoteName = random.choice(allNoteNames)\n",
    "    lastChordSort = lastChord.sortAscending()\n",
    "    sNoteOctave = random.choice([i.octave for i in lastChordSort.pitches])\n",
    "    sNote = note.Note((\"%s%s\" % (sNoteName, sNoteOctave)))\n",
    "    return sNote\n",
    "\n",
    "''' Helper function to generate an approach tone. '''\n",
    "def __generate_approach_tone(lastChord):\n",
    "    sNote = __generate_scale_tone(lastChord)\n",
    "    aNote = sNote.transpose(random.choice([1, -1]))\n",
    "    return aNote\n",
    "\n",
    "''' Helper function to generate a random tone. '''\n",
    "def __generate_arbitrary_tone(lastChord):\n",
    "    return __generate_scale_tone(lastChord) # fix later, make random note.\n",
    "\n",
    "\n",
    "''' Given the notes in a measure ('measure') and the chords in that measure\n",
    "    ('chords'), generate a list of abstract grammatical symbols to represent \n",
    "    that measure as described in GTK's \"Learning Jazz Grammars\" (2009). \n",
    "\n",
    "    Inputs: \n",
    "    1) \"measure\" : a stream.Voice object where each element is a\n",
    "        note.Note or note.Rest object.\n",
    "\n",
    "        >>> m1\n",
    "        <music21.stream.Voice 328482572>\n",
    "        >>> m1[0]\n",
    "        <music21.note.Rest rest>\n",
    "        >>> m1[1]\n",
    "        <music21.note.Note C>\n",
    "\n",
    "        Can have instruments and other elements, removes them here.\n",
    "\n",
    "    2) \"chords\" : a stream.Voice object where each element is a chord.Chord.\n",
    "\n",
    "        >>> c1\n",
    "        <music21.stream.Voice 328497548>\n",
    "        >>> c1[0]\n",
    "        <music21.chord.Chord E-4 G4 C4 B-3 G#2>\n",
    "        >>> c1[1]\n",
    "        <music21.chord.Chord B-3 F4 D4 A3>\n",
    "\n",
    "        Can have instruments and other elements, removes them here. \n",
    "\n",
    "    Outputs:\n",
    "    1) \"fullGrammar\" : a string that holds the abstract grammar for measure.\n",
    "        Format: \n",
    "        (Remember, these are DURATIONS not offsets!)\n",
    "        \"R,0.125\" : a rest element of  (1/32) length, or 1/8 quarter note. \n",
    "        \"C,0.125<M-2,m-6>\" : chord note of (1/32) length, generated\n",
    "                             anywhere from minor 6th down to major 2nd down.\n",
    "                             (interval <a,b> is not ordered). '''\n",
    "\n",
    "def parse_melody(fullMeasureNotes, fullMeasureChords):\n",
    "    # Remove extraneous elements.x\n",
    "    measure = copy.deepcopy(fullMeasureNotes)\n",
    "    chords = copy.deepcopy(fullMeasureChords)\n",
    "    measure.removeByNotOfClass([note.Note, note.Rest])\n",
    "    chords.removeByNotOfClass([chord.Chord])\n",
    "\n",
    "    # Information for the start of the measure.\n",
    "    # 1) measureStartTime: the offset for measure's start, e.g. 476.0.\n",
    "    # 2) measureStartOffset: how long from the measure start to the first element.\n",
    "    measureStartTime = measure[0].offset - (measure[0].offset % 4)\n",
    "    measureStartOffset  = measure[0].offset - measureStartTime\n",
    "\n",
    "    # Iterate over the notes and rests in measure, finding the grammar for each\n",
    "    # note in the measure and adding an abstract grammatical string for it. \n",
    "\n",
    "    fullGrammar = \"\"\n",
    "    prevNote = None # Store previous note. Need for interval.\n",
    "    numNonRests = 0 # Number of non-rest elements. Need for updating prevNote.\n",
    "    for ix, nr in enumerate(measure):\n",
    "        # Get the last chord. If no last chord, then (assuming chords is of length\n",
    "        # >0) shift first chord in chords to the beginning of the measure.\n",
    "        try: \n",
    "            lastChord = [n for n in chords if n.offset <= nr.offset][-1]\n",
    "        except IndexError:\n",
    "            chords[0].offset = measureStartTime\n",
    "            lastChord = [n for n in chords if n.offset <= nr.offset][-1]\n",
    "\n",
    "        # FIRST, get type of note, e.g. R for Rest, C for Chord, etc.\n",
    "        # Dealing with solo notes here. If unexpected chord: still call 'C'.\n",
    "        elementType = ' '\n",
    "        # R: First, check if it's a rest. Clearly a rest --> only one possibility.\n",
    "        if isinstance(nr, note.Rest):\n",
    "            elementType = 'R'\n",
    "        # C: Next, check to see if note pitch is in the last chord.\n",
    "        elif nr.name in lastChord.pitchNames or isinstance(nr, chord.Chord):\n",
    "            elementType = 'C'\n",
    "        # L: (Complement tone) Skip this for now.\n",
    "        # S: Check if it's a scale tone.\n",
    "        elif __is_scale_tone(lastChord, nr):\n",
    "            elementType = 'S'\n",
    "        # A: Check if it's an approach tone, i.e. +-1 halfstep chord tone.\n",
    "        elif __is_approach_tone(lastChord, nr):\n",
    "            elementType = 'A'\n",
    "        # X: Otherwise, it's an arbitrary tone. Generate random note.\n",
    "        else:\n",
    "            elementType = 'X'\n",
    "\n",
    "        # SECOND, get the length for each element. e.g. 8th note = R8, but\n",
    "        # to simplify things you'll use the direct num, e.g. R,0.125\n",
    "        if (ix == (len(measure)-1)):\n",
    "            # formula for a in \"a - b\": start of measure (e.g. 476) + 4\n",
    "            diff = measureStartTime + 4.0 - nr.offset\n",
    "        else:\n",
    "            diff = measure[ix + 1].offset - nr.offset\n",
    "\n",
    "        # Combine into the note info.\n",
    "        noteInfo = \"%s,%.3f\" % (elementType, nr.quarterLength) # back to diff\n",
    "\n",
    "        # THIRD, get the deltas (max range up, max range down) based on where\n",
    "        # the previous note was, +- minor 3. Skip rests (don't affect deltas).\n",
    "        intervalInfo = \"\"\n",
    "        if isinstance(nr, note.Note):\n",
    "            numNonRests += 1\n",
    "            if numNonRests == 1:\n",
    "                prevNote = nr\n",
    "            else:\n",
    "                noteDist = interval.Interval(noteStart=prevNote, noteEnd=nr)\n",
    "                noteDistUpper = interval.add([noteDist, \"m3\"])\n",
    "                noteDistLower = interval.subtract([noteDist, \"m3\"])\n",
    "                intervalInfo = \",<%s,%s>\" % (noteDistUpper.directedName, \n",
    "                    noteDistLower.directedName)\n",
    "                # print \"Upper, lower: %s, %s\" % (noteDistUpper,\n",
    "                #     noteDistLower)\n",
    "                # print \"Upper, lower dnames: %s, %s\" % (\n",
    "                #     noteDistUpper.directedName,\n",
    "                #     noteDistLower.directedName)\n",
    "                # print \"The interval: %s\" % (intervalInfo)\n",
    "                prevNote = nr\n",
    "\n",
    "        # Return. Do lazy evaluation for real-time performance.\n",
    "        grammarTerm = noteInfo + intervalInfo \n",
    "        fullGrammar += (grammarTerm + \" \")\n",
    "\n",
    "    return fullGrammar.rstrip()\n",
    "\n",
    "''' Given a grammar string and chords for a measure, returns measure notes. '''\n",
    "def unparse_grammar(m1_grammar, m1_chords):\n",
    "    m1_elements = stream.Voice()\n",
    "    currOffset = 0.0 # for recalculate last chord.\n",
    "    prevElement = None\n",
    "    for ix, grammarElement in enumerate(m1_grammar.split(' ')):\n",
    "        terms = grammarElement.split(',')\n",
    "        currOffset += float(terms[1]) # works just fine\n",
    "\n",
    "        # Case 1: it's a rest. Just append\n",
    "        if terms[0] == 'R':\n",
    "            rNote = note.Rest(quarterLength = float(terms[1]))\n",
    "            m1_elements.insert(currOffset, rNote)\n",
    "            continue\n",
    "\n",
    "        # Get the last chord first so you can find chord note, scale note, etc.\n",
    "        try: \n",
    "            lastChord = [n for n in m1_chords if n.offset <= currOffset][-1]\n",
    "        except IndexError:\n",
    "            m1_chords[0].offset = 0.0\n",
    "            lastChord = [n for n in m1_chords if n.offset <= currOffset][-1]\n",
    "\n",
    "        # Case: no < > (should just be the first note) so generate from range\n",
    "        # of lowest chord note to highest chord note (if not a chord note, else\n",
    "        # just generate one of the actual chord notes). \n",
    "\n",
    "        # Case #1: if no < > to indicate next note range. Usually this lack of < >\n",
    "        # is for the first note (no precedent), or for rests.\n",
    "        if (len(terms) == 2): # Case 1: if no < >.\n",
    "            insertNote = note.Note() # default is C\n",
    "\n",
    "            # Case C: chord note.\n",
    "            if terms[0] == 'C':\n",
    "                insertNote = __generate_chord_tone(lastChord)\n",
    "\n",
    "            # Case S: scale note.\n",
    "            elif terms[0] == 'S':\n",
    "                insertNote = __generate_scale_tone(lastChord)\n",
    "\n",
    "            # Case A: approach note.\n",
    "            # Handle both A and X notes here for now.\n",
    "            else:\n",
    "                insertNote = __generate_approach_tone(lastChord)\n",
    "\n",
    "            # Update the stream of generated notes\n",
    "            insertNote.quarterLength = float(terms[1])\n",
    "            if insertNote.octave < 4:\n",
    "                insertNote.octave = 4\n",
    "            m1_elements.insert(currOffset, insertNote)\n",
    "            prevElement = insertNote\n",
    "\n",
    "        # Case #2: if < > for the increment. Usually for notes after the first one.\n",
    "        else:\n",
    "            # Get lower, upper intervals and notes.\n",
    "            interval1 = interval.Interval(terms[2].replace(\"<\",''))\n",
    "            interval2 = interval.Interval(terms[3].replace(\">\",''))\n",
    "            if interval1.cents > interval2.cents:\n",
    "                upperInterval, lowerInterval = interval1, interval2\n",
    "            else:\n",
    "                upperInterval, lowerInterval = interval2, interval1\n",
    "            lowPitch = interval.transposePitch(prevElement.pitch, lowerInterval)\n",
    "            highPitch = interval.transposePitch(prevElement.pitch, upperInterval)\n",
    "            numNotes = int(highPitch.ps - lowPitch.ps + 1) # for range(s, e)\n",
    "\n",
    "            # Case C: chord note, must be within increment (terms[2]).\n",
    "            # First, transpose note with lowerInterval to get note that is\n",
    "            # the lower bound. Then iterate over, and find valid notes. Then\n",
    "            # choose randomly from those.\n",
    "            \n",
    "            if terms[0] == 'C':\n",
    "                relevantChordTones = []\n",
    "                for i in range(0, numNotes):\n",
    "                    currNote = note.Note(lowPitch.transpose(i).simplifyEnharmonic())\n",
    "                    if __is_chord_tone(lastChord, currNote):\n",
    "                        relevantChordTones.append(currNote)\n",
    "                if len(relevantChordTones) > 1:\n",
    "                    insertNote = random.choice([i for i in relevantChordTones\n",
    "                        if i.nameWithOctave != prevElement.nameWithOctave])\n",
    "                elif len(relevantChordTones) == 1:\n",
    "                    insertNote = relevantChordTones[0]\n",
    "                else: # if no choices, set to prev element +-1 whole step\n",
    "                    insertNote = prevElement.transpose(random.choice([-2,2]))\n",
    "                if insertNote.octave < 3:\n",
    "                    insertNote.octave = 3\n",
    "                insertNote.quarterLength = float(terms[1])\n",
    "                m1_elements.insert(currOffset, insertNote)\n",
    "\n",
    "            # Case S: scale note, must be within increment.\n",
    "            elif terms[0] == 'S':\n",
    "                relevantScaleTones = []\n",
    "                for i in range(0, numNotes):\n",
    "                    currNote = note.Note(lowPitch.transpose(i).simplifyEnharmonic())\n",
    "                    if __is_scale_tone(lastChord, currNote):\n",
    "                        relevantScaleTones.append(currNote)\n",
    "                if len(relevantScaleTones) > 1:\n",
    "                    insertNote = random.choice([i for i in relevantScaleTones\n",
    "                        if i.nameWithOctave != prevElement.nameWithOctave])\n",
    "                elif len(relevantScaleTones) == 1:\n",
    "                    insertNote = relevantScaleTones[0]\n",
    "                else: # if no choices, set to prev element +-1 whole step\n",
    "                    insertNote = prevElement.transpose(random.choice([-2,2]))\n",
    "                if insertNote.octave < 3:\n",
    "                    insertNote.octave = 3\n",
    "                insertNote.quarterLength = float(terms[1])\n",
    "                m1_elements.insert(currOffset, insertNote)\n",
    "\n",
    "            # Case A: approach tone, must be within increment.\n",
    "            # For now: handle both A and X cases.\n",
    "            else:\n",
    "                relevantApproachTones = []\n",
    "                for i in range(0, numNotes):\n",
    "                    currNote = note.Note(lowPitch.transpose(i).simplifyEnharmonic())\n",
    "                    if __is_approach_tone(lastChord, currNote):\n",
    "                        relevantApproachTones.append(currNote)\n",
    "                if len(relevantApproachTones) > 1:\n",
    "                    insertNote = random.choice([i for i in relevantApproachTones\n",
    "                        if i.nameWithOctave != prevElement.nameWithOctave])\n",
    "                elif len(relevantApproachTones) == 1:\n",
    "                    insertNote = relevantApproachTones[0]\n",
    "                else: # if no choices, set to prev element +-1 whole step\n",
    "                    insertNote = prevElement.transpose(random.choice([-2,2]))\n",
    "                if insertNote.octave < 3:\n",
    "                    insertNote.octave = 3\n",
    "                insertNote.quarterLength = float(terms[1])\n",
    "                m1_elements.insert(currOffset, insertNote)\n",
    "\n",
    "            # update the previous element.\n",
    "            prevElement = insertNote\n",
    "\n",
    "    return m1_elements    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author:     Ji-Sung Kim, Evan Chow\n",
    "Project:    deepjazz\n",
    "Purpose:    Provide pruning and cleanup functions.\n",
    "\n",
    "Code adapted from Evan Chow's jazzml, https://github.com/evancchow/jazzml \n",
    "with express permission.\n",
    "'''\n",
    "from itertools import zip_longest\n",
    "import random\n",
    "\n",
    "from music21 import *\n",
    "\n",
    "#----------------------------HELPER FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Helper function to down num to the nearest multiple of mult. '''\n",
    "def __roundDown(num, mult):\n",
    "    return (float(num) - (float(num) % mult))\n",
    "\n",
    "''' Helper function to round up num to nearest multiple of mult. '''\n",
    "def __roundUp(num, mult):\n",
    "    return __roundDown(num, mult) + mult\n",
    "\n",
    "''' Helper function that, based on if upDown < 0 or upDown >= 0, rounds number \n",
    "    down or up respectively to nearest multiple of mult. '''\n",
    "def __roundUpDown(num, mult, upDown):\n",
    "    if upDown < 0:\n",
    "        return __roundDown(num, mult)\n",
    "    else:\n",
    "        return __roundUp(num, mult)\n",
    "\n",
    "''' Helper function, from recipes, to iterate over list in chunks of n \n",
    "    length. '''\n",
    "def __grouper(iterable, n, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "#----------------------------PUBLIC FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Smooth the measure, ensuring that everything is in standard note lengths \n",
    "    (e.g., 0.125, 0.250, 0.333 ... ). '''\n",
    "def prune_grammar(curr_grammar):\n",
    "    pruned_grammar = curr_grammar.split(' ')\n",
    "\n",
    "    for ix, gram in enumerate(pruned_grammar):\n",
    "        terms = gram.split(',')\n",
    "        terms[1] = str(__roundUpDown(float(terms[1]), 0.250, \n",
    "            random.choice([-1, 1])))\n",
    "        pruned_grammar[ix] = ','.join(terms)\n",
    "    pruned_grammar = ' '.join(pruned_grammar)\n",
    "\n",
    "    return pruned_grammar\n",
    "\n",
    "''' Remove repeated notes, and notes that are too close together. '''\n",
    "def prune_notes(curr_notes):\n",
    "    for n1, n2 in __grouper(curr_notes, n=2):\n",
    "        if n2 == None: # corner case: odd-length list\n",
    "            continue\n",
    "        if isinstance(n1, note.Note) and isinstance(n2, note.Note):\n",
    "            if n1.nameWithOctave == n2.nameWithOctave:\n",
    "                curr_notes.remove(n2)\n",
    "\n",
    "    return curr_notes\n",
    "\n",
    "''' Perform quality assurance on notes '''\n",
    "def clean_up_notes(curr_notes):\n",
    "    removeIxs = []\n",
    "    for ix, m in enumerate(curr_notes):\n",
    "        # QA1: ensure nothing is of 0 quarter note len, if so changes its len\n",
    "        if (m.quarterLength == 0.0):\n",
    "            m.quarterLength = 0.250\n",
    "        # QA2: ensure no two melody notes have same offset, i.e. form a chord.\n",
    "        # Sorted, so same offset would be consecutive notes.\n",
    "        if (ix < (len(curr_notes) - 1)):\n",
    "            if (m.offset == curr_notes[ix + 1].offset and\n",
    "                isinstance(curr_notes[ix + 1], note.Note)):\n",
    "                removeIxs.append((ix + 1))\n",
    "    curr_notes = [i for ix, i in enumerate(curr_notes) if ix not in removeIxs]\n",
    "\n",
    "    return curr_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef load_music_utils():\\n    chord_data, raw_music_data = get_musical_data('data/original_metheny.mid')\\n    music_data, values, values_indices, indices_values = get_corpus_data(raw_music_data)\\n\\n    X, Y = data_processing(music_data, values_indices, Tx = 20, step = 3)\\n    return (X, Y)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Author:     Ji-Sung Kim\n",
    "Project:    deepjazz\n",
    "Purpose:    Parse, cleanup and process data.\n",
    "\n",
    "Code adapted from Evan Chow's jazzml, https://github.com/evancchow/jazzml with\n",
    "express permission.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from music21 import *\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import groupby, zip_longest\n",
    "\n",
    "#from grammar import *\n",
    "\n",
    "#from grammar import parse_melody\n",
    "#from music_utils import *\n",
    "\n",
    "#----------------------------HELPER FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Helper function to parse a MIDI file into its measures and chords '''\n",
    "def __parse_midi(data_fn):\n",
    "    # Parse the MIDI data for separate melody and accompaniment parts.\n",
    "    midi_data = converter.parse(data_fn)\n",
    "    \n",
    "    # Handle different MIDI file structures\n",
    "    if len(midi_data.parts) <= 5:\n",
    "        # If there are not enough parts, use the last part as melody\n",
    "        melody_stream = midi_data.parts[-1]\n",
    "    else:\n",
    "        # Get melody part, compress into single voice.\n",
    "        melody_stream = midi_data[5]     # For Metheny piece, Melody is Part #5.\n",
    "    \n",
    "    # Check if melody_stream has Voice elements\n",
    "    voices = melody_stream.getElementsByClass(stream.Voice)\n",
    "    if len(voices) >= 2:\n",
    "        melody1, melody2 = voices[0], voices[1]\n",
    "        for j in melody2:\n",
    "            melody1.insert(j.offset, j)\n",
    "        melody_voice = melody1\n",
    "    elif len(voices) == 1:\n",
    "        melody_voice = voices[0]\n",
    "    else:\n",
    "        # If no voices found, create a new voice and add all elements\n",
    "        melody_voice = stream.Voice()\n",
    "        for element in melody_stream.flat:\n",
    "            if hasattr(element, 'offset'):\n",
    "                melody_voice.insert(element.offset, element)\n",
    "\n",
    "    for i in melody_voice:\n",
    "        if i.quarterLength == 0.0:\n",
    "            i.quarterLength = 0.25\n",
    "\n",
    "    # Change key signature to adhere to comp_stream (1 sharp, mode = major).\n",
    "    # Also add Electric Guitar. \n",
    "    melody_voice.insert(0, instrument.ElectricGuitar())\n",
    "    melody_voice.insert(0, key.KeySignature(sharps=1))\n",
    "\n",
    "    # The accompaniment parts. Take only the best subset of parts from\n",
    "    # the original data. Maybe add more parts, hand-add valid instruments.\n",
    "    # Should add least add a string part (for sparse solos).\n",
    "    # Verified are good parts: 0, 1, 6, 7 '''\n",
    "    partIndices = [0, 1, 6, 7]\n",
    "    comp_stream = stream.Voice()\n",
    "    \n",
    "    # Filter partIndices to only include existing parts\n",
    "    valid_part_indices = [i for i in partIndices if i < len(midi_data.parts)]\n",
    "    if not valid_part_indices:\n",
    "        # If no valid parts found, use first few parts\n",
    "        valid_part_indices = list(range(min(4, len(midi_data.parts))))\n",
    "    \n",
    "    comp_stream.append([j.flat for i, j in enumerate(midi_data.parts) \n",
    "        if i in valid_part_indices])\n",
    "\n",
    "    # Full stream containing both the melody and the accompaniment. \n",
    "    # All parts are flattened. \n",
    "    full_stream = stream.Voice()\n",
    "    for i in range(len(comp_stream)):\n",
    "        full_stream.append(comp_stream[i])\n",
    "    full_stream.append(melody_voice)\n",
    "\n",
    "    # Extract solo stream, assuming you know the positions ..ByOffset(i, j).\n",
    "    # Note that for different instruments (with stream.flat), you NEED to use\n",
    "    # stream.Part(), not stream.Voice().\n",
    "    # Accompanied solo is in range [478, 548)\n",
    "    solo_stream = stream.Voice()\n",
    "    \n",
    "    # Get the total length of the piece to determine solo range\n",
    "    total_length = max([part.flat.highestOffset for part in full_stream.parts if part.flat.highestOffset > 0], default=600)\n",
    "    solo_start = min(476, total_length * 0.8)  # Start at 80% of the piece or 476, whichever is smaller\n",
    "    solo_end = min(548, total_length)  # End at the end of the piece or 548, whichever is smaller\n",
    "    \n",
    "    for part in full_stream:\n",
    "        curr_part = stream.Part()\n",
    "        curr_part.append(part.getElementsByClass(instrument.Instrument))\n",
    "        curr_part.append(part.getElementsByClass(tempo.MetronomeMark))\n",
    "        curr_part.append(part.getElementsByClass(key.KeySignature))\n",
    "        curr_part.append(part.getElementsByClass(meter.TimeSignature))\n",
    "        \n",
    "        # Extract elements in the solo range\n",
    "        solo_elements = part.getElementsByOffset(solo_start, solo_end, includeEndBoundary=True)\n",
    "        if solo_elements:\n",
    "            curr_part.append(solo_elements)\n",
    "        \n",
    "        cp = curr_part.flat\n",
    "        solo_stream.insert(cp)\n",
    "\n",
    "    # Group by measure so you can classify. \n",
    "    # Note that measure 0 is for the time signature, metronome, etc. which have\n",
    "    # an offset of 0.0.\n",
    "    if len(solo_stream) > 0:\n",
    "        melody_stream = solo_stream[-1]\n",
    "    else:\n",
    "        # If no solo stream, use the melody voice directly\n",
    "        melody_stream = melody_voice\n",
    "    \n",
    "    measures = OrderedDict()\n",
    "    offsetTuples = [(int(n.offset / 4), n) for n in melody_stream if hasattr(n, 'offset')]\n",
    "    measureNum = 0 # for now, don't use real m. nums (119, 120)\n",
    "    for key_x, group in groupby(offsetTuples, lambda x: x[0]):\n",
    "        measures[measureNum] = [n[1] for n in group]\n",
    "        measureNum += 1\n",
    "\n",
    "    # Get the stream of chords.\n",
    "    # offsetTuples_chords: group chords by measure number.\n",
    "    if len(solo_stream) > 0:\n",
    "        chordStream = solo_stream[0]\n",
    "    else:\n",
    "        # If no solo stream, use the first accompaniment part\n",
    "        chordStream = comp_stream[0] if len(comp_stream) > 0 else melody_voice\n",
    "    \n",
    "    chordStream.removeByClass(note.Rest)\n",
    "    chordStream.removeByClass(note.Note)\n",
    "    offsetTuples_chords = [(int(n.offset / 4), n) for n in chordStream if hasattr(n, 'offset')]\n",
    "\n",
    "    # Generate the chord structure. Use just track 1 (piano) since it is\n",
    "    # the only instrument that has chords. \n",
    "    # Group into 4s, just like before. \n",
    "    chords = OrderedDict()\n",
    "    measureNum = 0\n",
    "    for key_x, group in groupby(offsetTuples_chords, lambda x: x[0]):\n",
    "        chords[measureNum] = [n[1] for n in group]\n",
    "        measureNum += 1\n",
    "\n",
    "    # Fix for the below problem.\n",
    "    #   1) Find out why len(measures) != len(chords).\n",
    "    #   ANSWER: resolves at end but melody ends 1/16 before last measure so doesn't\n",
    "    #           actually show up, while the accompaniment's beat 1 right after does.\n",
    "    #           Actually on second thought: melody/comp start on Ab, and resolve to\n",
    "    #           the same key (Ab) so could actually just cut out last measure to loop.\n",
    "    #           Decided: just cut out the last measure. \n",
    "    del chords[len(chords) - 1]\n",
    "    assert len(chords) == len(measures)\n",
    "\n",
    "    return measures, chords\n",
    "\n",
    "''' Helper function to get the grammatical data from given musical data. '''\n",
    "def __get_abstract_grammars(measures, chords):\n",
    "    # extract grammars\n",
    "    abstract_grammars = []\n",
    "    for ix in range(1, len(measures)):\n",
    "        m = stream.Voice()\n",
    "        for i in measures[ix]:\n",
    "            m.insert(i.offset, i)\n",
    "        c = stream.Voice()\n",
    "        for j in chords[ix]:\n",
    "            c.insert(j.offset, j)\n",
    "        parsed = parse_melody(m, c)\n",
    "        abstract_grammars.append(parsed)\n",
    "\n",
    "    return abstract_grammars\n",
    "\n",
    "#----------------------------PUBLIC FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Get musical data from a MIDI file '''\n",
    "def get_musical_data(data_fn):\n",
    "    \n",
    "    measures, chords = __parse_midi(data_fn)\n",
    "    abstract_grammars = __get_abstract_grammars(measures, chords)\n",
    "\n",
    "    return chords, abstract_grammars\n",
    "\n",
    "''' Get corpus data from grammatical data '''\n",
    "def get_corpus_data(abstract_grammars):\n",
    "    corpus = [x for sublist in abstract_grammars for x in sublist.split(' ')]\n",
    "    values = set(corpus)\n",
    "    val_indices = dict((v, i) for i, v in enumerate(values))\n",
    "    indices_val = dict((i, v) for i, v in enumerate(values))\n",
    "\n",
    "    return corpus, values, val_indices, indices_val\n",
    "\n",
    "# Alternative MIDI parsing function that handles different file structures\n",
    "def parse_midi_robust(data_fn):\n",
    "    \"\"\"\n",
    "    Robust MIDI parsing function that can handle different MIDI file structures\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the MIDI data\n",
    "        midi_data = converter.parse(data_fn)\n",
    "        print(f\"MIDI file loaded. Number of parts: {len(midi_data.parts)}\")\n",
    "        \n",
    "        # Find the part with the most notes (likely the melody)\n",
    "        melody_part = None\n",
    "        max_notes = 0\n",
    "        for i, part in enumerate(midi_data.parts):\n",
    "            notes = part.flatten().getElementsByClass(note.Note)\n",
    "            if len(notes) > max_notes:\n",
    "                max_notes = len(notes)\n",
    "                melody_part = part\n",
    "                print(f\"Part {i} has {len(notes)} notes - selected as melody\")\n",
    "        \n",
    "        if melody_part is None:\n",
    "            print(\"No melody part found, using first part\")\n",
    "            melody_part = midi_data.parts[0]\n",
    "        \n",
    "        # Create melody voice\n",
    "        melody_voice = stream.Voice()\n",
    "        for element in melody_part.flatten():\n",
    "            if hasattr(element, 'offset'):\n",
    "                melody_voice.insert(element.offset, element)\n",
    "        \n",
    "        # Add instrument and key signature\n",
    "        melody_voice.insert(0, instrument.ElectricGuitar())\n",
    "        melody_voice.insert(0, key.KeySignature(sharps=1))\n",
    "        \n",
    "        # Create accompaniment stream from other parts\n",
    "        comp_stream = stream.Voice()\n",
    "        for i, part in enumerate(midi_data.parts):\n",
    "            if part != melody_part:  # Exclude melody part\n",
    "                # Fix: iterate over flattened elements instead of appending StreamIterator\n",
    "                for elem in part.flatten():\n",
    "                    comp_stream.append(elem)\n",
    "        \n",
    "        # Create full stream\n",
    "        full_stream = stream.Voice()\n",
    "        full_stream.append(comp_stream)\n",
    "        full_stream.append(melody_voice)\n",
    "        \n",
    "        # Extract solo stream (use the entire melody for simplicity)\n",
    "        solo_stream = stream.Voice()\n",
    "        for part in full_stream:\n",
    "            curr_part = stream.Part()\n",
    "            # Fix: iterate over elements instead of appending StreamIterator\n",
    "            for elem in part.getElementsByClass(instrument.Instrument):\n",
    "                curr_part.append(elem)\n",
    "            for elem in part.getElementsByClass(tempo.MetronomeMark):\n",
    "                curr_part.append(elem)\n",
    "            for elem in part.getElementsByClass(key.KeySignature):\n",
    "                curr_part.append(elem)\n",
    "            for elem in part.getElementsByClass(meter.TimeSignature):\n",
    "                curr_part.append(elem)\n",
    "            # Use flatten() instead of deprecated .flat\n",
    "            for elem in part.flatten():\n",
    "                curr_part.append(elem)\n",
    "            cp = curr_part.flatten()\n",
    "            solo_stream.insert(cp)\n",
    "        \n",
    "        # Group melody by measures\n",
    "        measures = OrderedDict()\n",
    "        offsetTuples = [(int(n.offset / 4), n) for n in melody_voice if hasattr(n, 'offset')]\n",
    "        measureNum = 0\n",
    "        for key_x, group in groupby(offsetTuples, lambda x: x[0]):\n",
    "            measures[measureNum] = [n[1] for n in group]\n",
    "            measureNum += 1\n",
    "        \n",
    "        # Group chords by measures\n",
    "        chords = OrderedDict()\n",
    "        if len(solo_stream) > 0:\n",
    "            chordStream = solo_stream[0]\n",
    "        else:\n",
    "            chordStream = comp_stream[0] if len(comp_stream) > 0 else melody_voice\n",
    "        \n",
    "        chordStream.removeByClass(note.Rest)\n",
    "        chordStream.removeByClass(note.Note)\n",
    "        offsetTuples_chords = [(int(n.offset / 4), n) for n in chordStream if hasattr(n, 'offset')]\n",
    "        \n",
    "        measureNum = 0\n",
    "        for key_x, group in groupby(offsetTuples_chords, lambda x: x[0]):\n",
    "            chords[measureNum] = [n[1] for n in group]\n",
    "            measureNum += 1\n",
    "        \n",
    "        # Ensure same number of measures\n",
    "        min_measures = min(len(measures), len(chords))\n",
    "        if min_measures > 0:\n",
    "            measures = {k: v for k, v in measures.items() if k < min_measures}\n",
    "            chords = {k: v for k, v in chords.items() if k < min_measures}\n",
    "        \n",
    "        print(f\"Parsed {len(measures)} measures and {len(chords)} chord sequences\")\n",
    "        return measures, chords\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in robust MIDI parsing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def get_musical_data_robust(data_fn):\n",
    "    \"\"\"\n",
    "    Get musical data using robust parsing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        measures, chords = parse_midi_robust(data_fn)\n",
    "        if measures is None or chords is None:\n",
    "            print(\"Failed to parse MIDI file\")\n",
    "            return None, None\n",
    "        \n",
    "        abstract_grammars = []\n",
    "        for ix in range(1, len(measures)):  # Skip measure 0\n",
    "            if ix in measures and ix in chords:\n",
    "                m = stream.Voice()\n",
    "                for i in measures[ix]:\n",
    "                    m.insert(i.offset, i)\n",
    "                c = stream.Voice()\n",
    "                for j in chords[ix]:\n",
    "                    c.insert(j.offset, j)\n",
    "                parsed = parse_melody(m, c)\n",
    "                abstract_grammars.append(parsed)\n",
    "        \n",
    "        print(f\"Generated {len(abstract_grammars)} abstract grammars\")\n",
    "        return chords, abstract_grammars\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_musical_data_robust: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "'''\n",
    "def load_music_utils():\n",
    "    chord_data, raw_music_data = get_musical_data('data/original_metheny.mid')\n",
    "    music_data, values, values_indices, indices_values = get_corpus_data(raw_music_data)\n",
    "\n",
    "    X, Y = data_processing(music_data, values_indices, Tx = 20, step = 3)\n",
    "    return (X, Y)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## music_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sys\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "#from grammar import *\n",
    "#from preprocess import *\n",
    "#from qa import *\n",
    "\n",
    "\n",
    "def data_processing(corpus, values_indices, m = 60, Tx = 30):\n",
    "    # cut the corpus into semi-redundant sequences of Tx values\n",
    "    Tx = Tx \n",
    "    N_values = len(set(corpus))\n",
    "    np.random.seed(0)\n",
    "    X = np.zeros((m, Tx, N_values), dtype=np.bool_)\n",
    "    Y = np.zeros((m, Tx, N_values), dtype=np.bool_)\n",
    "    for i in range(m):\n",
    "#         for t in range(1, Tx):\n",
    "        random_idx = np.random.choice(len(corpus) - Tx)\n",
    "        corp_data = corpus[random_idx:(random_idx + Tx)]\n",
    "        for j in range(Tx):\n",
    "            idx = values_indices[corp_data[j]]\n",
    "            if j != 0:\n",
    "                X[i, j, idx] = 1\n",
    "                Y[i, j-1, idx] = 1\n",
    "    \n",
    "    Y = np.swapaxes(Y,0,1)\n",
    "    Y = Y.tolist()\n",
    "    return np.asarray(X), np.asarray(Y), N_values \n",
    "\n",
    "def next_value_processing(model, next_value, x, predict_and_sample, indices_values, abstract_grammars, duration, max_tries = 1000, temperature = 0.5):\n",
    "    \"\"\"\n",
    "    Helper function to fix the first value.\n",
    "    \n",
    "    Arguments:\n",
    "    next_value -- predicted and sampled value, index between 0 and 77\n",
    "    x -- numpy-array, one-hot encoding of next_value\n",
    "    predict_and_sample -- predict function\n",
    "    indices_values -- a python dictionary mapping indices (0-77) into their corresponding unique value (ex: A,0.250,< m2,P-4 >)\n",
    "    abstract_grammars -- list of grammars, on element can be: 'S,0.250,<m2,P-4> C,0.250,<P4,m-2> A,0.250,<P4,m-2>'\n",
    "    duration -- scalar, index of the loop in the parent function\n",
    "    max_tries -- Maximum numbers of time trying to fix the value\n",
    "    \n",
    "    Returns:\n",
    "    next_value -- process predicted value\n",
    "    \"\"\"\n",
    "\n",
    "    # fix first note: must not have < > and not be a rest\n",
    "    if (duration < 0.00001):\n",
    "        tries = 0\n",
    "        while (next_value.split(',')[0] == 'R' or \n",
    "            len(next_value.split(',')) != 2):\n",
    "            # give up after 1000 tries; random from input's first notes\n",
    "            if tries >= max_tries:\n",
    "                #print('Gave up on first note generation after', max_tries, 'tries')\n",
    "                # np.random is exclusive to high\n",
    "                rand = np.random.randint(0, len(abstract_grammars))\n",
    "                next_value = abstract_grammars[rand].split(' ')[0]\n",
    "            else:\n",
    "                next_value = predict_and_sample(model, x, indices_values, temperature)\n",
    "\n",
    "            tries += 1\n",
    "            \n",
    "    return next_value\n",
    "\n",
    "\n",
    "def sequence_to_matrix(sequence, values_indices):\n",
    "    \"\"\"\n",
    "    Convert a sequence (slice of the corpus) into a matrix (numpy) of one-hot vectors corresponding \n",
    "    to indices in values_indices\n",
    "    \n",
    "    Arguments:\n",
    "    sequence -- python list\n",
    "    \n",
    "    Returns:\n",
    "    x -- numpy-array of one-hot vectors \n",
    "    \"\"\"\n",
    "    sequence_len = len(sequence)\n",
    "    x = np.zeros((1, sequence_len, len(values_indices)))\n",
    "    for t, value in enumerate(sequence):\n",
    "        if (not value in values_indices): print(value)\n",
    "        x[0, t, values_indices[value]] = 1.\n",
    "    return x\n",
    "\n",
    "def one_hot(x):\n",
    "    # PyTorch equivalent of the Keras one_hot function\n",
    "    x = torch.argmax(x, dim=-1)\n",
    "    x = F.one_hot(x, num_classes=78).float()\n",
    "    x = x.unsqueeze(1)  # Equivalent to RepeatVector(1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luxia\\AppData\\Local\\Temp\\ipykernel_29004\\954868163.py:180: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  measures, chords = __parse_midi(data_fn)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Voice' object has no attribute 'parts'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mF\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Use the simplified parsing function instead\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m chords, abstract_grammars = get_musical_data(\u001b[33m'\u001b[39m\u001b[33mdata/original_metheny.mid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m abstract_grammars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     10\u001b[39m     corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 180\u001b[39m, in \u001b[36mget_musical_data\u001b[39m\u001b[34m(data_fn)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mget_musical_data\u001b[39m(data_fn):\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     measures, chords = __parse_midi(data_fn)\n\u001b[32m    181\u001b[39m     abstract_grammars = __get_abstract_grammars(measures, chords)\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chords, abstract_grammars\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36m__parse_midi\u001b[39m\u001b[34m(data_fn)\u001b[39m\n\u001b[32m     88\u001b[39m solo_stream = stream.Voice()\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Get the total length of the piece to determine solo range\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m total_length = \u001b[38;5;28mmax\u001b[39m([part.flat.highestOffset \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m full_stream.parts \u001b[38;5;28;01mif\u001b[39;00m part.flat.highestOffset > \u001b[32m0\u001b[39m], default=\u001b[32m600\u001b[39m)\n\u001b[32m     92\u001b[39m solo_start = \u001b[38;5;28mmin\u001b[39m(\u001b[32m476\u001b[39m, total_length * \u001b[32m0.8\u001b[39m)  \u001b[38;5;66;03m# Start at 80% of the piece or 476, whichever is smaller\u001b[39;00m\n\u001b[32m     93\u001b[39m solo_end = \u001b[38;5;28mmin\u001b[39m(\u001b[32m548\u001b[39m, total_length)  \u001b[38;5;66;03m# End at the end of the piece or 548, whichever is smaller\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Voice' object has no attribute 'parts'"
     ]
    }
   ],
   "source": [
    "#from music_utils import * \n",
    "#from preprocess import * \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use the simplified parsing function instead\n",
    "chords, abstract_grammars = get_musical_data('data/original_metheny.mid')\n",
    "\n",
    "if chords is not None and abstract_grammars is not None:\n",
    "    corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
    "    N_tones = len(set(corpus))\n",
    "    n_a = 64\n",
    "    x_initializer = torch.zeros((1, 1, 78))\n",
    "    a_initializer = torch.zeros((1, n_a))\n",
    "    c_initializer = torch.zeros((1, n_a))\n",
    "    print(f\"Successfully loaded data with {N_tones} unique tones\")\n",
    "    \n",
    "    # Make variables available globally for other functions\n",
    "    globals().update({\n",
    "        'chords': chords,\n",
    "        'abstract_grammars': abstract_grammars,\n",
    "        'corpus': corpus,\n",
    "        'tones': tones,\n",
    "        'tones_indices': tones_indices,\n",
    "        'indices_tones': indices_tones,\n",
    "        'N_tones': N_tones,\n",
    "        'n_a': n_a,\n",
    "        'x_initializer': x_initializer,\n",
    "        'a_initializer': a_initializer,\n",
    "        'c_initializer': c_initializer\n",
    "    })\n",
    "else:\n",
    "    print(\"Failed to load musical data. Please check the MIDI file.\")\n",
    "\n",
    "def load_music_utils():\n",
    "    chords, abstract_grammars = get_musical_data_simple('data/original_metheny.mid')\n",
    "    if chords is None or abstract_grammars is None:\n",
    "        print(\"Failed to load musical data\")\n",
    "        return None\n",
    "    \n",
    "    corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
    "    N_tones = len(set(corpus))\n",
    "    X, Y, N_tones = data_processing(corpus, tones_indices, 60, 30)   \n",
    "    return (X, Y, N_tones, indices_tones)\n",
    "\n",
    "\n",
    "def generate_music(inference_model, corpus = corpus, abstract_grammars = abstract_grammars, tones = tones, tones_indices = tones_indices, indices_tones = indices_tones, T_y = 10, max_tries = 1000, diversity = 0.5):\n",
    "    \"\"\"\n",
    "    Generates music using a model trained to learn musical patterns of a jazz soloist. Creates an audio stream\n",
    "    to save the music and play it.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- Keras model Instance, output of djmodel()\n",
    "    corpus -- musical corpus, list of 193 tones as strings (ex: 'C,0.333,<P1,d-5>')\n",
    "    abstract_grammars -- list of grammars, on element can be: 'S,0.250,<m2,P-4> C,0.250,<P4,m-2> A,0.250,<P4,m-2>'\n",
    "    tones -- set of unique tones, ex: 'A,0.250,<M2,d-4>' is one element of the set.\n",
    "    tones_indices -- a python dictionary mapping unique tone (ex: A,0.250,< m2,P-4 >) into their corresponding indices (0-77)\n",
    "    indices_tones -- a python dictionary mapping indices (0-77) into their corresponding unique tone (ex: A,0.250,< m2,P-4 >)\n",
    "    Tx -- integer, number of time-steps used at training time\n",
    "    temperature -- scalar value, defines how conservative/creative the model is when generating music\n",
    "    \n",
    "    Returns:\n",
    "    predicted_tones -- python list containing predicted tones\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up audio stream\n",
    "    out_stream = stream.Stream()\n",
    "    \n",
    "    # Initialize chord variables\n",
    "    curr_offset = 0.0                                     # variable used to write sounds to the Stream.\n",
    "    num_chords = int(len(chords) / 3)                     # number of different set of chords\n",
    "    \n",
    "    print(\"Predicting new values for different set of chords.\")\n",
    "    # Loop over all 18 set of chords. At each iteration generate a sequence of tones\n",
    "    # and use the current chords to convert it into actual sounds \n",
    "    for i in range(1, num_chords):\n",
    "        \n",
    "        # Retrieve current chord from stream\n",
    "        curr_chords = stream.Voice()\n",
    "        \n",
    "        # Loop over the chords of the current set of chords\n",
    "        for j in chords[i]:\n",
    "            # Add chord to the current chords with the adequate offset, no need to understand this\n",
    "            curr_chords.insert((j.offset % 4), j)\n",
    "        \n",
    "        # Generate a sequence of tones using the model\n",
    "        _, indices = predict_and_sample_data_utils(inference_model)\n",
    "        indices = list(indices.squeeze())\n",
    "        pred = [indices_tones[p] for p in indices]\n",
    "        \n",
    "        predicted_tones = 'C,0.25 '\n",
    "        for k in range(len(pred) - 1):\n",
    "            predicted_tones += pred[k] + ' ' \n",
    "        \n",
    "        predicted_tones +=  pred[-1]\n",
    "                \n",
    "        #### POST PROCESSING OF THE PREDICTED TONES ####\n",
    "        # We will consider \"A\" and \"X\" as \"C\" tones. It is a common choice.\n",
    "        predicted_tones = predicted_tones.replace(' A',' C').replace(' X',' C')\n",
    "\n",
    "        # Pruning #1: smoothing measure\n",
    "        predicted_tones = prune_grammar(predicted_tones)\n",
    "        \n",
    "        # Use predicted tones and current chords to generate sounds\n",
    "        sounds = unparse_grammar(predicted_tones, curr_chords)\n",
    "\n",
    "        # Pruning #2: removing repeated and too close together sounds\n",
    "        sounds = prune_notes(sounds)\n",
    "\n",
    "        # Quality assurance: clean up sounds\n",
    "        sounds = clean_up_notes(sounds)\n",
    "\n",
    "        # Print number of tones/notes in sounds\n",
    "        print('Generated %s sounds using the predicted values for the set of chords (\"%s\") and after pruning' % (len([k for k in sounds if isinstance(k, note.Note)]), i))\n",
    "        \n",
    "        # Insert sounds into the output stream\n",
    "        for m in sounds:\n",
    "            out_stream.insert(curr_offset + m.offset, m)\n",
    "        for mc in curr_chords:\n",
    "            out_stream.insert(curr_offset + mc.offset, mc)\n",
    "\n",
    "        curr_offset += 4.0\n",
    "        \n",
    "    # Initialize tempo of the output stream with 130 bit per minute\n",
    "    out_stream.insert(0.0, tempo.MetronomeMark(number=130))\n",
    "\n",
    "    # Save audio stream to fine\n",
    "    mf = midi.translate.streamToMidiFile(out_stream)\n",
    "    mf.open(\"output/my_music.midi\", 'wb')\n",
    "    mf.write()\n",
    "    print(\"Your generated music is saved in output/my_music.midi\")\n",
    "    mf.close()\n",
    "    \n",
    "    # Play the final stream through output (see 'play' lambda function above)\n",
    "    # play = lambda x: midi.realtime.StreamPlayer(x).play()\n",
    "    # play(out_stream)\n",
    "    \n",
    "    return out_stream\n",
    "\n",
    "\n",
    "def predict_and_sample_data_utils(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- PyTorch model instance for inference time\n",
    "    x_initializer -- torch tensor of shape (1, 1, 78), one-hot vector initializing the values generation\n",
    "    a_initializer -- torch tensor of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- torch tensor of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    Ty -- length of the sequence you'd like to generate.\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    inference_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = inference_model(x_initializer, a_initializer, c_initializer)\n",
    "        indices = []\n",
    "        for output in pred:\n",
    "            idx = torch.argmax(output, dim=-1).cpu().numpy()\n",
    "            indices.append(idx)\n",
    "        indices = np.array(indices)\n",
    "        \n",
    "        # Convert to one-hot vectors\n",
    "        results = []\n",
    "        for idx in indices:\n",
    "            one_hot = F.one_hot(torch.tensor(idx), num_classes=78).float().numpy()\n",
    "            results.append(one_hot)\n",
    "        results = np.array(results)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import IPython\n",
    "import sys\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "#from grammar import *\n",
    "#from qa import *\n",
    "#from preprocess import * \n",
    "#from music_utils import *\n",
    "#from data_utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 问题描述\n",
    "\n",
    "你想为朋友的生日创作一段爵士乐，但你不会任何乐器，也不懂作曲。幸运的是，你懂深度学习，可以使用 LSTM 网络来解决这个问题。\n",
    "\n",
    "你将训练一个网络，让它生成新的爵士独奏作品，并且风格类似于已有的表演曲目。\n",
    "\n",
    "<img src=\"images/jazz.jpg\" style=\"width:450;height:300px;\">\n",
    "\n",
    "### 1.1 - 数据集\n",
    "\n",
    "你将用一组爵士乐曲目训练算法。运行下面的单元可以试听训练集中的音频片段：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('./data/30s_seq.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经对音乐数据进行了预处理，将其表示为音乐“数值”。可以非正式地把每个“数值”理解为一个音符，它包含音高和时长。例如，如果你按下某个钢琴键持续 0.5 秒，你就演奏了一个音符。在音乐理论中，一个“数值”实际上比这更复杂——它还包含演奏多个音符同时发声所需的信息。例如，在演奏音乐作品时，你可能同时按下两个钢琴键（同时演奏多个音符产生所谓的“和弦”）。但是在本作业中，我们不需要深入音乐理论的细节。你只需要知道，我们将得到一个数值数据集，并使用 RNN 模型学习生成数值序列。\n",
    "\n",
    "我们的音乐生成系统将使用 78 个独特的数值。运行以下代码可以加载原始音乐数据并将其预处理为数值形式。这可能需要几分钟。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, n_values, indices_values = load_music_utils()\n",
    "print('shape of X:', X.shape)\n",
    "print('number of training examples:', X.shape[0])\n",
    "print('Tx (length of sequence):', X.shape[1])\n",
    "print('total # of unique values:', n_values)\n",
    "print('Shape of Y:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你刚刚加载了以下内容：\n",
    "\n",
    "- `X`：这是一个维度为 (m, $T_x$, 78) 的数组。我们有 m 个训练样本，每个样本是长度为 $T_x = 30$ 的音乐片段。在每个时间步，输入是 78 个可能值之一，用 one-hot 向量表示。例如，`X[i, t, :]` 表示第 i 个样本在时间步 t 的 one-hot 向量。\n",
    "\n",
    "- `Y`：本质上与 `X` 相同，但向左（过去方向）移动了一步。类似于恐龙名字任务，我们希望网络利用之前的值来预测下一个值，因此我们的序列模型会尝试预测 $y^{\\langle t \\rangle}$，给定 $x^{\\langle 1\\rangle}, \\ldots, x^{\\langle t \\rangle}$。不过，`Y` 的数据被重新排列为维度 $(T_y, m, 78)$，其中 $T_y = T_x$。这种格式更方便后续输入到 LSTM。\n",
    "\n",
    "- `n_values`：数据集中唯一值的数量。这里应该是 78。\n",
    "\n",
    "- `indices_values`：Python 字典，将 0-77 映射到对应的音乐值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - 我们模型概述\n",
    "\n",
    "下面是我们将使用的模型架构。这与之前笔记中使用的恐龙名字模型类似，不同之处在于这里我们将使用 Keras 来实现。架构如下：\n",
    "\n",
    "<img src=\"images/music_generation.png\" style=\"width:600;height:400px;\">\n",
    "\n",
    "我们将使用来自较长音乐片段的随机 30 个值的片段来训练模型。因此，我们不再像之前为恐龙名字设置 $x^{\\langle 1 \\rangle} = \\vec{0}$ 来表示序列开始，因为这些音频片段大多数都是从音乐中间某个位置截取的。我们将每个片段设置为相同长度 $T_x = 30$，以便更方便地进行向量化处理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 构建模型\n",
    "\n",
    "在本部分，你将构建并训练一个能够学习音乐模式的模型。为此，你需要构建一个模型，其输入 `X` 的形状为 $(m, T_x, 78)$，输出 `Y` 的形状为 $(T_y, m, 78)$。我们将使用一个隐藏状态维度为 64 的 LSTM。令 `n_a = 64`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是如何在 Keras 中创建具有多个输入和输出的模型。如果你要构建一个 RNN，并且在测试时整个输入序列 $x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\ldots, x^{\\langle T_x \\rangle}$ 已知，例如输入是单词而输出是一个标签，那么 Keras 提供了简单的内置函数来构建模型。然而，对于序列生成任务，在测试时我们并不知道所有 $x^{\\langle t\\rangle}$ 的值；相反，我们会使用 $x^{\\langle t\\rangle} = y^{\\langle t-1 \\rangle}$ 一次生成一个值。因此代码会稍微复杂一些，你需要自己实现一个 for 循环来迭代不同的时间步。\n",
    "\n",
    "函数 `djmodel()` 将使用 for 循环调用 LSTM 层 $T_x$ 次，重要的是所有 $T_x$ 个副本应共享相同的权重。也就是说，不应每次都重新初始化权重——$T_x$ 个时间步应该共享权重。实现 Keras 中可共享权重层的关键步骤如下：\n",
    "1. 定义层对象（我们将使用全局变量来实现）。\n",
    "2. 在前向传播时调用这些对象。\n",
    "\n",
    "我们已将所需的层对象定义为全局变量。请运行下一格来创建它们。你可以查阅 Keras 文档以确保理解这些层：[Reshape()](https://keras.io/layers/core/#reshape), [LSTM()](https://keras.io/layers/recurrent/#lstm), [Dense()](https://keras.io/layers/core/#dense)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch equivalent layers - these will be part of our model class\n",
    "# reshapor equivalent: x.view(1, 78) or x.unsqueeze(0)\n",
    "# LSTM_cell equivalent: nn.LSTM(n_values, n_a, batch_first=True)\n",
    "# densor equivalent: nn.Linear(n_a, n_values) with softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，`reshapor`、`LSTM_cell` 和 `densor` 都是层对象，你可以用它们来实现 `djmodel()`。为了将一个 Keras 张量对象 X 传递通过某个层，可以使用 `layer_object(X)`（如果该层需要多个输入，则使用 `layer_object([X,Y])`）。例如，`reshapor(X)` 会将 X 传入之前定义的 `Reshape((1,78))` 层进行前向传播。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：实现 `djmodel()`。你需要执行以下两个步骤：\n",
    "\n",
    "1. 创建一个空列表 `outputs` 来保存每个时间步 LSTM 单元的输出。\n",
    "2. 对 $t \\in 1, \\ldots, T_x$ 进行循环：\n",
    "\n",
    "   A. 从 X 中选择第 t 个时间步的向量。这个选择的形状应该是 (78,)。你可以通过创建一个自定义的 [Lambda](https://keras.io/layers/core/#lambda) 层来实现：\n",
    "```python\n",
    "x = Lambda(lambda x: X[:,t,:])(X)\n",
    "\n",
    "``` \n",
    "查看 Keras 文档以理解其作用。它创建了一个“临时”或“匿名”函数（Lambda 函数），用于提取对应的 one-hot 向量，并将该函数封装成 Keras 的 `Layer` 对象来应用于 X。\n",
    "\n",
    "B. 将 x 重塑为 (1,78)。你可以使用下面定义的 `reshapor()` 层来实现。\n",
    "\n",
    "C. 将 x 传入 LSTM_cell 的一个时间步。记得用上一步的隐藏状态 $a$ 和细胞状态 $c$ 初始化 LSTM_cell，格式如下：\n",
    "```python\n",
    "a, _, c = LSTM_cell(input_x, initial_state=[previous hidden state, previous cell state])\n",
    "\n",
    "```\n",
    "\n",
    "    D. 将 LSTM 的输出激活值通过一个 dense + softmax 层传递，使用 `densor` 实现。\n",
    "    \n",
    "    E. 将预测的值追加到 \"outputs\" 列表中。\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch LSTM Model for Jazz Generation\n",
    "class JazzLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch LSTM model for jazz music generation\n",
    "    \n",
    "    Arguments:\n",
    "    n_values -- number of unique values in the music data (vocabulary size)\n",
    "    n_a -- the number of activations used in our model (hidden size)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_values, n_a):\n",
    "        super(JazzLSTM, self).__init__()\n",
    "        self.n_values = n_values\n",
    "        self.n_a = n_a\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(n_values, n_a, batch_first=True)\n",
    "        \n",
    "        # Dense layer for output\n",
    "        self.dense = nn.Linear(n_a, n_values)\n",
    "        \n",
    "    def forward(self, x, a0=None, c0=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM\n",
    "        \n",
    "        Arguments:\n",
    "        x -- input tensor of shape (batch_size, seq_len, n_values)\n",
    "        a0 -- initial hidden state (batch_size, n_a)\n",
    "        c0 -- initial cell state (batch_size, n_a)\n",
    "        \n",
    "        Returns:\n",
    "        outputs -- list of outputs for each time step\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden and cell states if not provided\n",
    "        if a0 is None:\n",
    "            a0 = torch.zeros(1, batch_size, self.n_a, device=x.device)\n",
    "        else:\n",
    "            a0 = a0.unsqueeze(0)  # Add sequence dimension\n",
    "            \n",
    "        if c0 is None:\n",
    "            c0 = torch.zeros(1, batch_size, self.n_a, device=x.device)\n",
    "        else:\n",
    "            c0 = c0.unsqueeze(0)  # Add sequence dimension\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        # Process each time step\n",
    "        for t in range(x.size(1)):\n",
    "            # Get the t-th time step vector\n",
    "            x_t = x[:, t, :].unsqueeze(1)  # Shape: (batch_size, 1, n_values)\n",
    "            \n",
    "            # Pass through LSTM\n",
    "            lstm_out, (a0, c0) = self.lstm(x_t, (a0, c0))\n",
    "            \n",
    "            # Apply dense layer and softmax\n",
    "            output = F.softmax(self.dense(lstm_out.squeeze(1)), dim=-1)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return outputs, a0.squeeze(0), c0.squeeze(0)\n",
    "\n",
    "def djmodel(Tx, n_a, n_values):\n",
    "    \"\"\"\n",
    "    Create PyTorch model instance\n",
    "    \n",
    "    Arguments:\n",
    "    Tx -- length of the sequence in a corpus\n",
    "    n_a -- the number of activations used in our model\n",
    "    n_values -- number of unique values in the music data \n",
    "    \n",
    "    Returns:\n",
    "    model -- a PyTorch model instance\n",
    "    \"\"\"\n",
    "    return JazzLSTM(n_values, n_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的代码单元来定义你的模型。我们将使用 `Tx=30`、`n_a=64`（LSTM 激活的维度）以及 `n_values=78`。此单元可能需要几秒钟才能运行完成。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = djmodel(Tx = 30 , n_a = 64, n_values = 78)\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Model created and moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你需要编译你的模型以进行训练。我们将使用 Adam 优化器和分类交叉熵（categorical cross-entropy）损失函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Model compiled with Adam optimizer and CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们将 LSTM 的初始隐藏状态 `a0` 和初始细胞状态 `c0` 初始化为零。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 60\n",
    "a0 = torch.zeros((m, n_a), device=device)\n",
    "c0 = torch.zeros((m, n_a), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们开始训练模型！在训练之前，我们将 `Y` 转换为列表格式，因为损失函数要求 `Y` 以这种格式提供（每个时间步一个列表项）。因此，`list(Y)` 是一个包含 30 个元素的列表，每个元素的形状为 (60, 78)。我们将训练 100 个 epoch，这将花费几分钟时间。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(X).to(device)\n",
    "Y_tensor = torch.FloatTensor(Y).to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs, _, _ = model(X_tensor, a0, c0)\n",
    "    \n",
    "    # Calculate loss for each time step\n",
    "    total_loss = 0\n",
    "    for t in range(len(outputs)):\n",
    "        # Get target for this time step (Y[t] is shape (m, n_values))\n",
    "        target = torch.argmax(Y_tensor[t], dim=1)  # Convert one-hot to indices\n",
    "        loss = criterion(outputs[t], target)\n",
    "        total_loss += loss\n",
    "    \n",
    "    # Average loss across time steps\n",
    "    total_loss = total_loss / len(outputs)\n",
    "    \n",
    "    # Backward pass\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss.item():.4f}')\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你应该会看到模型的损失在下降。现在模型已经训练完成，让我们进入最后一部分，来实现推理算法，并生成一些音乐！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 生成音乐\n",
    "\n",
    "现在你已经有了一个训练好的模型，它学会了爵士独奏者的演奏模式。让我们使用这个模型来合成新的音乐。\n",
    "\n",
    "#### 3.1 - 预测与采样\n",
    "\n",
    "<img src=\"images/music_gen.png\" style=\"width:600;height:400px;\">\n",
    "\n",
    "在每一步采样中，你将使用来自 LSTM 前一状态的激活 `a` 和细胞状态 `c`，向前传播一步，得到新的输出激活和细胞状态。新的激活 `a` 可以像之前一样，通过 `densor` 生成输出。\n",
    "\n",
    "为了开始模型生成，我们将初始化 `x0`，以及 LSTM 的激活和细胞状态 `a0` 和 `c0` 为零。\n",
    "\n",
    "\n",
    "**练习**：实现下面的函数，用于采样一段音乐序列。以下是在生成 $T_y$ 个输出音符的 for 循环中需要实现的关键步骤：\n",
    "\n",
    "- Step 2.A：使用 `LSTM_Cell`，输入前一步的 `c` 和 `a` 来生成当前步骤的 `c` 和 `a`。\n",
    "- Step 2.B：使用之前定义的 `densor` 对 `a` 做 softmax，得到当前步骤的输出。\n",
    "- Step 2.C：将刚生成的输出保存，通过追加到 `outputs` 列表中。\n",
    "- Step 2.D：将 x 设置为输出 `out` 的 one-hot 版本（预测值），以便传递给下一步 LSTM。我们已经提供了这行代码，使用了 [Lambda](https://keras.io/layers/core/#lambda) 函数：\n",
    "```python\n",
    "x = Lambda(one_hot)(out)\n",
    "\n",
    "```\n",
    "[小技术说明：与根据 `out` 中的概率随机采样不同，这行代码实际上在每一步使用 argmax 选择最可能的音符。]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Inference Model for Music Generation\n",
    "class MusicInferenceModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch inference model for generating music sequences\n",
    "    \n",
    "    Arguments:\n",
    "    trained_model -- the trained JazzLSTM model\n",
    "    n_values -- integer, number of unique values\n",
    "    n_a -- number of units in the LSTM\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trained_model, n_values=78, n_a=64, Ty=100):\n",
    "        super(MusicInferenceModel, self).__init__()\n",
    "        self.trained_model = trained_model\n",
    "        self.n_values = n_values\n",
    "        self.n_a = n_a\n",
    "        self.Ty = Ty\n",
    "        \n",
    "    def forward(self, x0, a0, c0):\n",
    "        \"\"\"\n",
    "        Generate a sequence of music values\n",
    "        \n",
    "        Arguments:\n",
    "        x0 -- initial input tensor of shape (1, 1, n_values)\n",
    "        a0 -- initial hidden state (1, n_a)\n",
    "        c0 -- initial cell state (1, n_a)\n",
    "        \n",
    "        Returns:\n",
    "        outputs -- list of generated outputs\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        x = x0\n",
    "        a = a0.unsqueeze(0)  # Add sequence dimension for LSTM\n",
    "        c = c0.unsqueeze(0)  # Add sequence dimension for LSTM\n",
    "        \n",
    "        for t in range(self.Ty):\n",
    "            # Perform one step of LSTM\n",
    "            lstm_out, (a, c) = self.trained_model.lstm(x, (a, c))\n",
    "            \n",
    "            # Apply dense layer with softmax\n",
    "            out = F.softmax(self.trained_model.dense(lstm_out.squeeze(1)), dim=-1)\n",
    "            outputs.append(out)\n",
    "            \n",
    "            # Select the next value (using argmax for deterministic generation)\n",
    "            x = torch.argmax(out, dim=-1)\n",
    "            x = F.one_hot(x, num_classes=self.n_values).float().unsqueeze(1)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "def music_inference_model(trained_model, n_values=78, n_a=64, Ty=100):\n",
    "    \"\"\"\n",
    "    Create PyTorch inference model instance\n",
    "    \n",
    "    Arguments:\n",
    "    trained_model -- the trained JazzLSTM model\n",
    "    n_values -- integer, number of unique values\n",
    "    n_a -- number of units in the LSTM\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- PyTorch model instance\n",
    "    \"\"\"\n",
    "    return MusicInferenceModel(trained_model, n_values, n_a, Ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的单元格以定义推理模型。该模型被硬编码为生成 50 个音符值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = music_inference_model(model, n_values = 78, n_a = 64, Ty = 50)\n",
    "inference_model = inference_model.to(device)\n",
    "print(\"Inference model created and moved to device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，这会创建用于初始化输入向量 `x` 以及 LSTM 状态变量 `a` 和 `c` 的全零向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = torch.zeros((1, 1, 78), device=device)\n",
    "a_initializer = torch.zeros((1, n_a), device=device)\n",
    "c_initializer = torch.zeros((1, n_a), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：实现 `predict_and_sample()` 函数。该函数接收多个参数，包括输入 `[x_initializer, a_initializer, c_initializer]`。为了预测对应于这些输入的输出，你需要完成以下 3 个步骤：\n",
    "\n",
    "1. 使用你的推理模型（inference model）对输入进行预测。输出 `pred` 应该是一个长度为 50 的列表，每个元素都是一个形状为 ($T_y$, n_values) 的 numpy 数组。\n",
    "2. 将 `pred` 转换为 $T_y$ 个索引组成的 numpy 数组。每个索引通过对 `pred` 列表中的元素取 `argmax` 计算得到。[提示](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html)。\n",
    "3. 将索引转换为它们的 one-hot 向量表示。[提示](https://keras.io/utils/#to_categorical)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch predict_and_sample function\n",
    "def predict_and_sample(inference_model, x_initializer=None, a_initializer=None, c_initializer=None):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- PyTorch model instance for inference time\n",
    "    x_initializer -- torch tensor of shape (1, 1, 78), one-hot vector initializing the values generation\n",
    "    a_initializer -- torch tensor of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- torch tensor of shape (1, n_a), initializing the cell state of the LSTM_cell\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set default values if not provided\n",
    "    if x_initializer is None:\n",
    "        x_initializer = torch.zeros((1, 1, 78), device=device)\n",
    "    if a_initializer is None:\n",
    "        a_initializer = torch.zeros((1, 64), device=device)\n",
    "    if c_initializer is None:\n",
    "        c_initializer = torch.zeros((1, 64), device=device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    inference_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Step 1: Use your inference model to predict an output sequence\n",
    "        pred = inference_model(x_initializer, a_initializer, c_initializer)\n",
    "        \n",
    "        # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "        indices = []\n",
    "        for output in pred:\n",
    "            idx = torch.argmax(output, dim=-1).cpu().numpy()\n",
    "            indices.append(idx)\n",
    "        indices = np.array(indices)\n",
    "        \n",
    "        # Step 3: Convert indices to one-hot vectors\n",
    "        results = []\n",
    "        for idx in indices:\n",
    "            one_hot = F.one_hot(torch.tensor(idx), num_classes=78).float().numpy()\n",
    "            results.append(one_hot)\n",
    "        results = np.array(results)\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)\n",
    "print(\"np.argmax(results[12]) =\", np.argmax(results[12]))\n",
    "print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
    "print(\"list(indices[12:18]) =\", list(indices[12:18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**：由于 Keras 的结果并不是完全可预测的，因此你的结果可能有所不同。不过，如果你按照上述说明使用 `model.fit()` 对 LSTM_cell 训练了整整 100 个 epoch，你很可能会看到一个索引序列，并非所有元素都相同。此外，你应当会观察到：\n",
    "\n",
    "- `np.argmax(results[12])` 是 `list(indices[12:18])` 的第一个元素\n",
    "- `np.argmax(results[17])` 是 `list(indices[12:18])` 的最后一个元素\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **np.argmax(results[12])** =\n",
    "        </td>\n",
    "        <td>\n",
    "            1\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **np.argmax(results[17])** =\n",
    "        </td>\n",
    "        <td>\n",
    "            42\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **list(indices[12:18])** =\n",
    "        </td>\n",
    "        <td>\n",
    "            [array([1]), array([42]), array([54]), array([17]), array([1]), array([42])]\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - 生成音乐\n",
    "\n",
    "现在，你已经可以生成音乐了。你的 RNN 会生成一个值的序列。下面的代码会先调用你的 `predict_and_sample()` 函数生成这些值。生成的值随后会被后处理为音乐和弦（意味着可以同时播放多个值或音符）。\n",
    "\n",
    "大多数计算机音乐算法都会进行某种后处理，因为如果没有这些处理，很难生成听起来悦耳的音乐。后处理会做一些操作，例如清理生成的音频，确保同一个声音不会重复太多次，相邻的两个音符在音高上不会相差太远，等等。有人可能会认为这些后处理步骤很多都是技巧性的；此外，许多音乐生成文献也强调手工设计后处理器，而且输出的质量很大程度上依赖于后处理的质量，而不仅仅依赖于 RNN 的质量。但是这些后处理确实对最终效果影响很大，因此我们在实现中也会使用它。\n",
    "\n",
    "让我们生成一些音乐吧！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的代码单元来生成音乐，并将其记录到 `out_stream` 中。这可能需要几分钟时间。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_stream = generate_music(inference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要聆听你生成的音乐，请点击 **File -> Open...**，然后进入 `output/` 文件夹，下载 `my_music.midi` 文件。你可以在电脑上使用支持 MIDI 文件的播放器播放，或者使用免费的在线“MIDI 转 MP3”工具将其转换为 MP3 格式。\n",
    "\n",
    "作为参考，这里也提供了一个我们使用该算法生成的 30 秒音频片段。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('./data/30s_trained_model.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恭喜！\n",
    "\n",
    "你已经完成了本笔记本的学习。\n",
    "\n",
    "<font color=\"blue\">\n",
    "你应该记住的要点如下：\n",
    "- 序列模型可以用来生成音乐值，这些值随后可以通过后处理生成 MIDI 音乐。\n",
    "- 相当类似的模型既可以用来生成恐龙名字，也可以用来生成音乐，主要区别在于输入数据的不同。\n",
    "- 在 Keras 中，序列生成涉及定义共享权重的层，然后将这些层在不同的时间步 $1, \\ldots, T_x$ 中重复使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你完成了本次作业并成功生成了一段爵士独奏！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参考文献**\n",
    "\n",
    "本笔记本中介绍的思想主要来源于以下三篇计算音乐相关的论文。同时，本实现也参考并借鉴了 Ji-Sung Kim 的 GitHub 仓库中的许多组件。\n",
    "\n",
    "- Ji-Sung Kim, 2016, [deepjazz](https://github.com/jisungk/deepjazz)\n",
    "- Jon Gillick, Kevin Tang 和 Robert Keller, 2009. [Learning Jazz Grammars](http://ai.stanford.edu/~kdtang/papers/smc09-jazzgrammar.pdf)\n",
    "- Robert Keller 和 David Morrison, 2007, [A Grammatical Approach to Automatic Improvisation](http://smc07.uoa.gr/SMC07%20Proceedings/SMC07%20Paper%2055.pdf)\n",
    "- François Pachet, 1999, [Surprising Harmonies](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.7473&rep=rep1&type=pdf)\n",
    "\n",
    "我们也感谢 François Germain 提供的宝贵反馈。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MIDI parsing\n",
    "print(\"Testing MIDI file structure...\")\n",
    "\n",
    "try:\n",
    "    midi_file = 'data/original_metheny.mid'\n",
    "    \n",
    "    # Parse the MIDI file\n",
    "    midi_data = converter.parse(midi_file)\n",
    "    print(f\"MIDI file loaded successfully. Number of parts: {len(midi_data.parts)}\")\n",
    "    \n",
    "    # Print information about each part\n",
    "    for i, part in enumerate(midi_data.parts):\n",
    "        print(f\"Part {i}: {len(part.flat)} elements, highest offset: {part.flat.highestOffset}\")\n",
    "        \n",
    "        # Check for voices in this part\n",
    "        voices = part.getElementsByClass(stream.Voice)\n",
    "        print(f\"  Voices found: {len(voices)}\")\n",
    "        \n",
    "        # Check for notes and chords\n",
    "        notes = part.flat.getElementsByClass(note.Note)\n",
    "        chords = part.flat.getElementsByClass(chord.Chord)\n",
    "        print(f\"  Notes: {len(notes)}, Chords: {len(chords)}\")\n",
    "    \n",
    "    print(\"MIDI structure analysis completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during MIDI parsing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the robust parsing\n",
    "print(\"Testing robust MIDI parsing...\")\n",
    "\n",
    "measures, chords = parse_midi_robust('data/original_metheny.mid')\n",
    "if measures is not None:\n",
    "    print(\"✓ Robust parsing successful!\")\n",
    "else:\n",
    "    print(\"✗ Robust parsing failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the updated function\n",
    "print(\"Testing get_musical_data_robust...\")\n",
    "\n",
    "chords, abstract_grammars = get_musical_data_robust('data/original_metheny.mid')\n",
    "if chords is not None and abstract_grammars is not None:\n",
    "    print(\"✓ Successfully loaded musical data!\")\n",
    "    print(f\"  Number of chord sequences: {len(chords)}\")\n",
    "    print(f\"  Number of abstract grammars: {len(abstract_grammars)}\")\n",
    "else:\n",
    "    print(\"✗ Failed to load musical data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete data loading pipeline\n",
    "print(\"Testing complete data loading pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Load musical data\n",
    "    chords, abstract_grammars = get_musical_data_robust('data/original_metheny.mid')\n",
    "    \n",
    "    if chords is not None and abstract_grammars is not None:\n",
    "        print(\"✓ Musical data loaded successfully\")\n",
    "        \n",
    "        # Get corpus data\n",
    "        corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
    "        print(f\"✓ Corpus data processed: {len(corpus)} tokens, {len(tones)} unique tones\")\n",
    "        \n",
    "        # Test data processing\n",
    "        X, Y, N_tones = data_processing(corpus, tones_indices, 10, 10)  # Small test\n",
    "        print(f\"✓ Data processing successful: X shape {X.shape}, Y shape {Y.shape}\")\n",
    "        \n",
    "        print(\"\\n🎵 All data loading tests passed! Ready to train the model.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"✗ Failed to load musical data\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the robust functions are properly defined\n",
    "print(\"Verifying function definitions...\")\n",
    "\n",
    "# Check if functions are defined\n",
    "if 'parse_midi_robust' in globals():\n",
    "    print(\"✓ parse_midi_robust is defined\")\n",
    "else:\n",
    "    print(\"✗ parse_midi_robust is NOT defined\")\n",
    "\n",
    "if 'get_musical_data_robust' in globals():\n",
    "    print(\"✓ get_musical_data_robust is defined\")\n",
    "else:\n",
    "    print(\"✗ get_musical_data_robust is NOT defined\")\n",
    "\n",
    "if 'parse_melody' in globals():\n",
    "    print(\"✓ parse_melody is defined\")\n",
    "else:\n",
    "    print(\"✗ parse_melody is NOT defined\")\n",
    "\n",
    "print(\"\\nAll functions should now be properly defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重要说明：运行顺序\n",
    "\n",
    "为了确保所有函数都正确定义，请按照以下顺序运行单元格：\n",
    "\n",
    "1. **首先运行** `## grammar` 单元格（定义音乐语法函数）\n",
    "2. **然后运行** `## qa` 单元格（定义质量保证函数）  \n",
    "3. **接着运行** `## preprocess` 单元格（定义MIDI解析函数，包括新的robust函数）\n",
    "4. **最后运行** `## music_utils` 和 `## data_utils` 单元格\n",
    "\n",
    "这样就能确保所有依赖的函数都已经定义，避免 `NameError` 错误。\n",
    "\n",
    "如果遇到任何错误，请重新按顺序运行这些单元格。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fixed MIDI parsing\n",
    "print(\"Testing fixed MIDI parsing...\")\n",
    "\n",
    "try:\n",
    "    # Test with a simple approach first\n",
    "    midi_data = converter.parse('data/original_metheny.mid')\n",
    "    print(f\"✓ MIDI file loaded successfully. Number of parts: {len(midi_data.parts)}\")\n",
    "    \n",
    "    # Find the part with most notes\n",
    "    max_notes = 0\n",
    "    melody_part_idx = 0\n",
    "    for i, part in enumerate(midi_data.parts):\n",
    "        notes = part.flatten().getElementsByClass(note.Note)\n",
    "        if len(notes) > max_notes:\n",
    "            max_notes = len(notes)\n",
    "            melody_part_idx = i\n",
    "    \n",
    "    print(f\"✓ Selected part {melody_part_idx} as melody with {max_notes} notes\")\n",
    "    \n",
    "    # Test the robust parsing\n",
    "    measures, chords = parse_midi_robust('data/original_metheny.mid')\n",
    "    if measures is not None and chords is not None:\n",
    "        print(\"✓ Robust parsing successful!\")\n",
    "        print(f\"  Measures: {len(measures)}, Chords: {len(chords)}\")\n",
    "    else:\n",
    "        print(\"✗ Robust parsing failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified MIDI parsing function\n",
    "def parse_midi_simple(data_fn):\n",
    "    \"\"\"\n",
    "    Simplified MIDI parsing function that avoids complex Stream operations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the MIDI data\n",
    "        midi_data = converter.parse(data_fn)\n",
    "        print(f\"MIDI file loaded. Number of parts: {len(midi_data.parts)}\")\n",
    "        \n",
    "        # Find the part with the most notes (likely the melody)\n",
    "        melody_part = None\n",
    "        max_notes = 0\n",
    "        for i, part in enumerate(midi_data.parts):\n",
    "            notes = part.flatten().getElementsByClass(note.Note)\n",
    "            if len(notes) > max_notes:\n",
    "                max_notes = len(notes)\n",
    "                melody_part = part\n",
    "                print(f\"Part {i} has {len(notes)} notes - selected as melody\")\n",
    "        \n",
    "        if melody_part is None:\n",
    "            print(\"No melody part found, using first part\")\n",
    "            melody_part = midi_data.parts[0]\n",
    "        \n",
    "        # Create melody voice - simplified approach\n",
    "        melody_voice = stream.Voice()\n",
    "        for element in melody_part.flatten():\n",
    "            if hasattr(element, 'offset') and isinstance(element, (note.Note, note.Rest)):\n",
    "                melody_voice.insert(element.offset, element)\n",
    "        \n",
    "        # Add basic metadata\n",
    "        melody_voice.insert(0, instrument.ElectricGuitar())\n",
    "        melody_voice.insert(0, key.KeySignature(sharps=1))\n",
    "        \n",
    "        # Create simple accompaniment stream\n",
    "        comp_stream = stream.Voice()\n",
    "        for i, part in enumerate(midi_data.parts):\n",
    "            if part != melody_part:  # Exclude melody part\n",
    "                for element in part.flatten():\n",
    "                    if isinstance(element, chord.Chord):\n",
    "                        comp_stream.insert(element.offset, element)\n",
    "        \n",
    "        # Group melody by measures - simplified\n",
    "        measures = OrderedDict()\n",
    "        all_notes = list(melody_voice.flatten().getElementsByClass([note.Note, note.Rest]))\n",
    "        if all_notes:\n",
    "            offsetTuples = [(int(n.offset / 4), n) for n in all_notes]\n",
    "            measureNum = 0\n",
    "            for key_x, group in groupby(offsetTuples, lambda x: x[0]):\n",
    "                measures[measureNum] = [n[1] for n in group]\n",
    "                measureNum += 1\n",
    "        \n",
    "        # Group chords by measures - simplified\n",
    "        chords = OrderedDict()\n",
    "        all_chords = list(comp_stream.flatten().getElementsByClass(chord.Chord))\n",
    "        if all_chords:\n",
    "            offsetTuples_chords = [(int(n.offset / 4), n) for n in all_chords]\n",
    "            measureNum = 0\n",
    "            for key_x, group in groupby(offsetTuples_chords, lambda x: x[0]):\n",
    "                chords[measureNum] = [n[1] for n in group]\n",
    "                measureNum += 1\n",
    "        \n",
    "        # Ensure we have some data\n",
    "        if len(measures) == 0:\n",
    "            print(\"No measures found, creating dummy data\")\n",
    "            measures[0] = [note.Note('C4', quarterLength=1.0)]\n",
    "            chords[0] = [chord.Chord(['C4', 'E4', 'G4'])]\n",
    "        \n",
    "        print(f\"Parsed {len(measures)} measures and {len(chords)} chord sequences\")\n",
    "        return measures, chords\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in simplified MIDI parsing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def get_musical_data_simple(data_fn):\n",
    "    \"\"\"\n",
    "    Get musical data using simplified parsing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        measures, chords = parse_midi_simple(data_fn)\n",
    "        if measures is None or chords is None:\n",
    "            print(\"Failed to parse MIDI file\")\n",
    "            return None, None\n",
    "        \n",
    "        abstract_grammars = []\n",
    "        # Use available measures\n",
    "        max_measures = max(len(measures), len(chords))\n",
    "        for ix in range(1, max_measures):  # Skip measure 0\n",
    "            if ix in measures and ix in chords:\n",
    "                m = stream.Voice()\n",
    "                for i in measures[ix]:\n",
    "                    m.insert(i.offset, i)\n",
    "                c = stream.Voice()\n",
    "                for j in chords[ix]:\n",
    "                    c.insert(j.offset, j)\n",
    "                parsed = parse_melody(m, c)\n",
    "                abstract_grammars.append(parsed)\n",
    "        \n",
    "        print(f\"Generated {len(abstract_grammars)} abstract grammars\")\n",
    "        return chords, abstract_grammars\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_musical_data_simple: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Test the simplified parsing\n",
    "print(\"Testing simplified MIDI parsing...\")\n",
    "measures, chords = parse_midi_simple('data/original_metheny.mid')\n",
    "if measures is not None:\n",
    "    print(\"✓ Simplified parsing successful!\")\n",
    "else:\n",
    "    print(\"✗ Simplified parsing failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete data loading pipeline with simplified parsing\n",
    "print(\"Testing complete data loading pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Test simplified parsing\n",
    "    chords, abstract_grammars = get_musical_data_simple('data/original_metheny.mid')\n",
    "    \n",
    "    if chords is not None and abstract_grammars is not None:\n",
    "        print(\"✓ Musical data loaded successfully\")\n",
    "        \n",
    "        # Get corpus data\n",
    "        corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
    "        print(f\"✓ Corpus data processed: {len(corpus)} tokens, {len(tones)} unique tones\")\n",
    "        \n",
    "        # Test data processing\n",
    "        X, Y, N_tones = data_processing(corpus, tones_indices, 10, 10)  # Small test\n",
    "        print(f\"✓ Data processing successful: X shape {X.shape}, Y shape {Y.shape}\")\n",
    "        \n",
    "        print(\"\\n🎵 All data loading tests passed! Ready to train the model.\")\n",
    "        \n",
    "        # Make variables available globally\n",
    "        globals().update({\n",
    "            'chords': chords,\n",
    "            'abstract_grammars': abstract_grammars,\n",
    "            'corpus': corpus,\n",
    "            'tones': tones,\n",
    "            'tones_indices': tones_indices,\n",
    "            'indices_tones': indices_tones,\n",
    "            'N_tones': N_tones,\n",
    "            'X': X,\n",
    "            'Y': Y\n",
    "        })\n",
    "        \n",
    "    else:\n",
    "        print(\"✗ Failed to load musical data\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "EG0F7",
   "launcher_item_id": "cxJXc"
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
