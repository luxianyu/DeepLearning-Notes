{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 用 LSTM 网络即兴演奏爵士独奏 (PyTorch版本)\n",
        "\n",
        "欢迎来到本周的最后一个编程作业！在这个 notebook 中，你将实现一个使用 LSTM 生成音乐的模型。完成作业后，你甚至可以听到自己生成的音乐。\n",
        "\n",
        "**你将学习：**\n",
        "- 将 LSTM 应用于音乐生成。\n",
        "- 使用深度学习生成你自己的爵士音乐。\n",
        "- 使用PyTorch框架实现深度学习模型。\n",
        "\n",
        "请运行以下单元以加载本次作业所需的所有包。这可能需要几分钟时间。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import IPython\n",
        "import sys\n",
        "from music21 import *\n",
        "from grammar import *\n",
        "from qa import *\n",
        "from preprocess import * \n",
        "from music_utils import *\n",
        "from data_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'使用设备: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 - 问题描述\n",
        "\n",
        "你想为朋友的生日创作一段爵士乐，但你不会任何乐器，也不懂作曲。幸运的是，你懂深度学习，可以使用 LSTM 网络来解决这个问题。\n",
        "\n",
        "你将训练一个网络，让它生成新的爵士独奏作品，并且风格类似于已有的表演曲目。\n",
        "\n",
        "<img src=\"images/jazz.jpg\" style=\"width:450;height:300px;\">\n",
        "\n",
        "### 1.1 - 数据集\n",
        "\n",
        "你将用一组爵士乐曲目训练算法。运行下面的单元可以试听训练集中的音频片段：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IPython.display.Audio('./data/30s_seq.mp3')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们已经对音乐数据进行了预处理，将其表示为音乐\"数值\"。可以非正式地把每个\"数值\"理解为一个音符，它包含音高和时长。例如，如果你按下某个钢琴键持续 0.5 秒，你就演奏了一个音符。在音乐理论中，一个\"数值\"实际上比这更复杂——它还包含演奏多个音符同时发声所需的信息。例如，在演奏音乐作品时，你可能同时按下两个钢琴键（同时演奏多个音符产生所谓的\"和弦\"）。但是在本作业中，我们不需要深入音乐理论的细节。你只需要知道，我们将得到一个数值数据集，并使用 RNN 模型学习生成数值序列。\n",
        "\n",
        "我们的音乐生成系统将使用 78 个独特的数值。运行以下代码可以加载原始音乐数据并将其预处理为数值形式。这可能需要几分钟。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X, Y, n_values, indices_values = load_music_utils()\n",
        "print('shape of X:', X.shape)\n",
        "print('number of training examples:', X.shape[0])\n",
        "print('Tx (length of sequence):', X.shape[1])\n",
        "print('total # of unique values:', n_values)\n",
        "print('Shape of Y:', len(Y), Y[0].shape if len(Y) > 0 else 'Empty')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "你刚刚加载了以下内容：\n",
        "\n",
        "- `X`：这是一个维度为 (m, $T_x$, 78) 的数组。我们有 m 个训练样本，每个样本是长度为 $T_x = 30$ 的音乐片段。在每个时间步，输入是 78 个可能值之一，用 one-hot 向量表示。例如，`X[i, t, :]` 表示第 i 个样本在时间步 t 的 one-hot 向量。\n",
        "\n",
        "- `Y`：本质上与 `X` 相同，但向左（过去方向）移动了一步。类似于恐龙名字任务，我们希望网络利用之前的值来预测下一个值，因此我们的序列模型会尝试预测 $y^{\\langle t \\rangle}$，给定 $x^{\\langle 1\\\\rangle}, \\\\ldots, x^{\\langle t \\\\rangle}$。不过，`Y` 的数据被重新排列为维度 $(T_y, m, 78)$，其中 $T_y = T_x$。这种格式更方便后续输入到 LSTM。\n",
        "\n",
        "- `n_values`：数据集中唯一值的数量。这里应该是 78。\n",
        "\n",
        "- `indices_values`：Python 字典，将 0-77 映射到对应的音乐值。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 - 我们模型概述\n",
        "\n",
        "下面是我们将使用的模型架构。这与之前笔记中使用的恐龙名字模型类似，不同之处在于这里我们将使用 PyTorch 来实现。架构如下：\n",
        "\n",
        "<img src=\"images/music_generation.png\" style=\"width:600;height:400px;\">\n",
        "\n",
        "我们将使用来自较长音乐片段的随机 30 个值的片段来训练模型。因此，我们不再像之前为恐龙名字设置 $x^{\\langle 1 \\\\rangle} = \\\\vec{0}$ 来表示序列开始，因为这些音频片段大多数都是从音乐中间某个位置截取的。我们将每个片段设置为相同长度 $T_x = 30$，以便更方便地进行向量化处理。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 - 构建模型\n",
        "\n",
        "在本部分，你将构建并训练一个能够学习音乐模式的模型。为此，你需要构建一个模型，其输入 `X` 的形状为 $(m, T_x, 78)$，输出 `Y` 的形状为 $(T_y, m, 78)$。我们将使用一个隐藏状态维度为 64 的 LSTM。令 `n_a = 64`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_a = 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在我们定义PyTorch版本的LSTM模型：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入我们的PyTorch模型\n",
        "from jazz_lstm_model import JazzLSTM, MusicInferenceModel, train_model, evaluate_model\n",
        "\n",
        "# 创建模型\n",
        "model = JazzLSTM(input_size=78, hidden_size=n_a, output_size=78).to(device)\n",
        "print(f\"模型参数数量: {sum(p.numel() for p in model.parameters())}\")\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在我们需要准备数据并创建数据加载器：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 将数据转换为PyTorch张量\n",
        "X_tensor = torch.FloatTensor(X).to(device)\n",
        "# Y需要重新组织为(batch_size, seq_len, vocab_size)格式\n",
        "Y_tensor = torch.FloatTensor(np.array(Y)).to(device)\n",
        "Y_tensor = Y_tensor.permute(1, 0, 2)  # 从(Ty, m, vocab)转换为(m, Ty, vocab)\n",
        "\n",
        "print(f'X_tensor shape: {X_tensor.shape}')\n",
        "print(f'Y_tensor shape: {Y_tensor.shape}')\n",
        "\n",
        "# 创建数据集和数据加载器\n",
        "dataset = TensorDataset(X_tensor, Y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=60, shuffle=True)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在开始训练模型：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练循环\n",
        "num_epochs = 100\n",
        "losses = train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs, device)\n",
        "\n",
        "print('训练完成！')\n",
        "\n",
        "# 绘制损失曲线\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 - 生成音乐\n",
        "\n",
        "现在你已经有了一个训练好的模型，它学会了爵士独奏者的演奏模式。让我们使用这个模型来合成新的音乐。\n",
        "\n",
        "### 3.1 - 预测与采样\n",
        "\n",
        "<img src=\"images/music_gen.png\" style=\"width:600;height:400px;\">\n",
        "\n",
        "在每一步采样中，你将使用来自 LSTM 前一状态的激活 `a` 和细胞状态 `c`，向前传播一步，得到新的输出激活和细胞状态。新的激活 `a` 可以像之前一样，通过 `dense` 层生成输出。\n",
        "\n",
        "为了开始模型生成，我们将初始化 `x0`，以及 LSTM 的激活和细胞状态 `a0` 和 `c0` 为零。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建推理模型\n",
        "inference_model = MusicInferenceModel(model, device)\n",
        "\n",
        "# 初始化输入\n",
        "x_initializer = np.zeros((1, 1, 78))\n",
        "\n",
        "# 生成音乐序列\n",
        "results, indices = inference_model.generate_sequence(x_initializer, Ty=50)\n",
        "\n",
        "print(\"np.argmax(results[12]) =\", np.argmax(results[12]))\n",
        "print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
        "print(\"list(indices[12:18]) =\", list(indices[12:18]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 - 生成音乐\n",
        "\n",
        "现在，你已经可以生成音乐了。你的 RNN 会生成一个值的序列。下面的代码会先调用你的生成函数生成这些值。生成的值随后会被后处理为音乐和弦（意味着可以同时播放多个值或音符）。\n",
        "\n",
        "大多数计算机音乐算法都会进行某种后处理，因为如果没有这些处理，很难生成听起来悦耳的音乐。后处理会做一些操作，例如清理生成的音频，确保同一个声音不会重复太多次，相邻的两个音符在音高上不会相差太远，等等。有人可能会认为这些后处理步骤很多都是技巧性的；此外，许多音乐生成文献也强调手工设计后处理器，而且输出的质量很大程度上依赖于后处理的质量，而不仅仅依赖于 RNN 的质量。但是这些后处理确实对最终效果影响很大，因此我们在实现中也会使用它。\n",
        "\n",
        "让我们生成一些音乐吧！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用PyTorch版本生成音乐\n",
        "def generate_music_pytorch(inference_model, corpus, abstract_grammars, tones, tones_indices, indices_tones, T_y=10, max_tries=1000, diversity=0.5):\n",
        "    \"\"\"\n",
        "    使用PyTorch模型生成音乐\n",
        "    \n",
        "    Arguments:\n",
        "    inference_model -- PyTorch推理模型实例\n",
        "    corpus -- 音乐语料库\n",
        "    abstract_grammars -- 抽象语法列表\n",
        "    tones -- 音调集合\n",
        "    tones_indices -- 音调到索引的映射\n",
        "    indices_tones -- 索引到音调的映射\n",
        "    T_y -- 每个和弦集合生成的音调数量\n",
        "    \n",
        "    Returns:\n",
        "    out_stream -- 生成的音乐流\n",
        "    \"\"\"\n",
        "    \n",
        "    # 设置音频流\n",
        "    out_stream = stream.Stream()\n",
        "    \n",
        "    # 初始化和弦变量\n",
        "    curr_offset = 0.0\n",
        "    num_chords = int(len(corpus) / 3)\n",
        "    \n",
        "    print(\"Predicting new values for different set of chords.\")\n",
        "    \n",
        "    # 遍历所有和弦集合\n",
        "    for i in range(1, num_chords):\n",
        "        # 获取当前和弦\n",
        "        curr_chords = stream.Voice()\n",
        "        \n",
        "        # 遍历当前和弦集合中的和弦\n",
        "        for j in corpus[i]:\n",
        "            curr_chords.insert((j.offset % 4), j)\n",
        "        \n",
        "        # 使用模型生成音调序列\n",
        "        _, indices = inference_model.generate_sequence(np.zeros((1, 1, 78)), Ty=T_y)\n",
        "        indices = list(indices)\n",
        "        pred = [indices_tones[p] for p in indices]\n",
        "        \n",
        "        predicted_tones = 'C,0.25 '\n",
        "        for k in range(len(pred) - 1):\n",
        "            predicted_tones += pred[k] + ' '\n",
        "        \n",
        "        predicted_tones += pred[-1]\n",
        "        \n",
        "        # 后处理预测的音调\n",
        "        predicted_tones = predicted_tones.replace(' A',' C').replace(' X',' C')\n",
        "        \n",
        "        # 修剪1：平滑处理\n",
        "        predicted_tones = prune_grammar(predicted_tones)\n",
        "        \n",
        "        # 使用预测音调和当前和弦生成声音\n",
        "        sounds = unparse_grammar(predicted_tones, curr_chords)\n",
        "        \n",
        "        # 修剪2：移除重复和过于接近的声音\n",
        "        sounds = prune_notes(sounds)\n",
        "        \n",
        "        # 质量保证：清理声音\n",
        "        sounds = clean_up_notes(sounds)\n",
        "        \n",
        "        # 打印生成的声音数量\n",
        "        print(f'Generated {len([k for k in sounds if isinstance(k, note.Note)])} sounds using the predicted values for the set of chords (\\\"{i}\\\") and after pruning')\n",
        "        \n",
        "        # 将声音插入输出流\n",
        "        for m in sounds:\n",
        "            out_stream.insert(curr_offset + m.offset, m)\n",
        "        for mc in curr_chords:\n",
        "            out_stream.insert(curr_offset + mc.offset, mc)\n",
        "        \n",
        "        curr_offset += 4.0\n",
        "    \n",
        "    # 初始化输出流的节拍为每分钟130拍\n",
        "    out_stream.insert(0.0, tempo.MetronomeMark(number=130))\n",
        "    \n",
        "    # 保存音频流到文件\n",
        "    mf = midi.translate.streamToMidiFile(out_stream)\n",
        "    mf.open(\"output/my_music_pytorch.midi\", 'wb')\n",
        "    mf.write()\n",
        "    print(\"Your generated music is saved in output/my_music_pytorch.midi\")\n",
        "    mf.close()\n",
        "    \n",
        "    return out_stream\n",
        "\n",
        "# 加载音乐数据\n",
        "chords, abstract_grammars = get_musical_data('data/original_metheny.mid')\n",
        "corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
        "\n",
        "# 生成音乐\n",
        "out_stream = generate_music_pytorch(inference_model, chords, abstract_grammars, tones, tones_indices, indices_tones, T_y=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要聆听你生成的音乐，请点击 **File -> Open...**，然后进入 `output/` 文件夹，下载 `my_music_pytorch.midi` 文件。你可以在电脑上使用支持 MIDI 文件的播放器播放，或者使用免费的在线\"MIDI 转 MP3\"工具将其转换为 MP3 格式。\n",
        "\n",
        "作为参考，这里也提供了一个我们使用该算法生成的 30 秒音频片段。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IPython.display.Audio('./data/30s_trained_model.mp3')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 恭喜！\n",
        "\n",
        "你已经完成了本笔记本的学习。\n",
        "\n",
        "<font color=\"blue\">\n",
        "你应该记住的要点如下：\n",
        "- 序列模型可以用来生成音乐值，这些值随后可以通过后处理生成 MIDI 音乐。\n",
        "- 相当类似的模型既可以用来生成恐龙名字，也可以用来生成音乐，主要区别在于输入数据的不同。\n",
        "- 在 PyTorch 中，序列生成涉及定义LSTM层，然后在循环中逐步生成序列。\n",
        "- PyTorch提供了更灵活的张量操作和自动微分功能，使得模型实现更加直观。\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "恭喜你完成了本次作业并成功生成了一段爵士独奏！\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
