{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用 LSTM 网络即兴演奏爵士独奏\n",
    "\n",
    "欢迎来到本周的最后一个编程作业！在这个 notebook 中，你将实现一个使用 LSTM 生成音乐的模型。完成作业后，你甚至可以听到自己生成的音乐。\n",
    "\n",
    "**你将学习：**\n",
    "- 将 LSTM 应用于音乐生成。\n",
    "- 使用深度学习生成你自己的爵士音乐。\n",
    "\n",
    "请运行以下单元以加载本次作业所需的所有包。这可能需要几分钟时间。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author:     Ji-Sung Kim, Evan Chow\n",
    "Project:    jazzml / (used in) deepjazz\n",
    "Purpose:    Extract, manipulate, process musical grammar\n",
    "\n",
    "Directly taken then cleaned up from Evan Chow's jazzml, \n",
    "https://github.com/evancchow/jazzml,with permission.\n",
    "'''\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from itertools import groupby\n",
    "from music21 import *\n",
    "import copy, random, pdb\n",
    "\n",
    "#from preprocess import *\n",
    "\n",
    "''' Helper function to determine if a note is a scale tone. '''\n",
    "def __is_scale_tone(chord, note):\n",
    "    # Method: generate all scales that have the chord notes th check if note is\n",
    "    # in names\n",
    "\n",
    "    # Derive major or minor scales (minor if 'other') based on the quality\n",
    "    # of the chord.\n",
    "    scaleType = scale.DorianScale() # i.e. minor pentatonic\n",
    "    if chord.quality == 'major':\n",
    "        scaleType = scale.MajorScale()\n",
    "    # Can change later to deriveAll() for flexibility. If so then use list\n",
    "    # comprehension of form [x for a in b for x in a].\n",
    "    scales = scaleType.derive(chord) # use deriveAll() later for flexibility\n",
    "    allPitches = list(set([pitch for pitch in scales.getPitches()]))\n",
    "    allNoteNames = [i.name for i in allPitches] # octaves don't matter\n",
    "\n",
    "    # Get note name. Return true if in the list of note names.\n",
    "    noteName = note.name\n",
    "    return (noteName in allNoteNames)\n",
    "\n",
    "''' Helper function to determine if a note is an approach tone. '''\n",
    "def __is_approach_tone(chord, note):\n",
    "    # Method: see if note is +/- 1 a chord tone.\n",
    "\n",
    "    for chordPitch in chord.pitches:\n",
    "        stepUp = chordPitch.transpose(1)\n",
    "        stepDown = chordPitch.transpose(-1)\n",
    "        if (note.name == stepDown.name or \n",
    "            note.name == stepDown.getEnharmonic().name or\n",
    "            note.name == stepUp.name or\n",
    "            note.name == stepUp.getEnharmonic().name):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "''' Helper function to determine if a note is a chord tone. '''\n",
    "def __is_chord_tone(lastChord, note):\n",
    "    return (note.name in (p.name for p in lastChord.pitches))\n",
    "\n",
    "''' Helper function to generate a chord tone. '''\n",
    "def __generate_chord_tone(lastChord):\n",
    "    lastChordNoteNames = [p.nameWithOctave for p in lastChord.pitches]\n",
    "    return note.Note(random.choice(lastChordNoteNames))\n",
    "\n",
    "''' Helper function to generate a scale tone. '''\n",
    "def __generate_scale_tone(lastChord):\n",
    "    # Derive major or minor scales (minor if 'other') based on the quality\n",
    "    # of the lastChord.\n",
    "    scaleType = scale.WeightedHexatonicBlues() # minor pentatonic\n",
    "    if lastChord.quality == 'major':\n",
    "        scaleType = scale.MajorScale()\n",
    "    # Can change later to deriveAll() for flexibility. If so then use list\n",
    "    # comprehension of form [x for a in b for x in a].\n",
    "    scales = scaleType.derive(lastChord) # use deriveAll() later for flexibility\n",
    "    allPitches = list(set([pitch for pitch in scales.getPitches()]))\n",
    "    allNoteNames = [i.name for i in allPitches] # octaves don't matter\n",
    "\n",
    "    # Return a note (no octave here) in a scale that matches the lastChord.\n",
    "    sNoteName = random.choice(allNoteNames)\n",
    "    lastChordSort = lastChord.sortAscending()\n",
    "    sNoteOctave = random.choice([i.octave for i in lastChordSort.pitches])\n",
    "    sNote = note.Note((\"%s%s\" % (sNoteName, sNoteOctave)))\n",
    "    return sNote\n",
    "\n",
    "''' Helper function to generate an approach tone. '''\n",
    "def __generate_approach_tone(lastChord):\n",
    "    sNote = __generate_scale_tone(lastChord)\n",
    "    aNote = sNote.transpose(random.choice([1, -1]))\n",
    "    return aNote\n",
    "\n",
    "''' Helper function to generate a random tone. '''\n",
    "def __generate_arbitrary_tone(lastChord):\n",
    "    return __generate_scale_tone(lastChord) # fix later, make random note.\n",
    "\n",
    "\n",
    "''' Given the notes in a measure ('measure') and the chords in that measure\n",
    "    ('chords'), generate a list of abstract grammatical symbols to represent \n",
    "    that measure as described in GTK's \"Learning Jazz Grammars\" (2009). \n",
    "\n",
    "    Inputs: \n",
    "    1) \"measure\" : a stream.Voice object where each element is a\n",
    "        note.Note or note.Rest object.\n",
    "\n",
    "        >>> m1\n",
    "        <music21.stream.Voice 328482572>\n",
    "        >>> m1[0]\n",
    "        <music21.note.Rest rest>\n",
    "        >>> m1[1]\n",
    "        <music21.note.Note C>\n",
    "\n",
    "        Can have instruments and other elements, removes them here.\n",
    "\n",
    "    2) \"chords\" : a stream.Voice object where each element is a chord.Chord.\n",
    "\n",
    "        >>> c1\n",
    "        <music21.stream.Voice 328497548>\n",
    "        >>> c1[0]\n",
    "        <music21.chord.Chord E-4 G4 C4 B-3 G#2>\n",
    "        >>> c1[1]\n",
    "        <music21.chord.Chord B-3 F4 D4 A3>\n",
    "\n",
    "        Can have instruments and other elements, removes them here. \n",
    "\n",
    "    Outputs:\n",
    "    1) \"fullGrammar\" : a string that holds the abstract grammar for measure.\n",
    "        Format: \n",
    "        (Remember, these are DURATIONS not offsets!)\n",
    "        \"R,0.125\" : a rest element of  (1/32) length, or 1/8 quarter note. \n",
    "        \"C,0.125<M-2,m-6>\" : chord note of (1/32) length, generated\n",
    "                             anywhere from minor 6th down to major 2nd down.\n",
    "                             (interval <a,b> is not ordered). '''\n",
    "\n",
    "def parse_melody(fullMeasureNotes, fullMeasureChords):\n",
    "    # Remove extraneous elements.x\n",
    "    measure = copy.deepcopy(fullMeasureNotes)\n",
    "    chords = copy.deepcopy(fullMeasureChords)\n",
    "    measure.removeByNotOfClass([note.Note, note.Rest])\n",
    "    chords.removeByNotOfClass([chord.Chord])\n",
    "\n",
    "    # Information for the start of the measure.\n",
    "    # 1) measureStartTime: the offset for measure's start, e.g. 476.0.\n",
    "    # 2) measureStartOffset: how long from the measure start to the first element.\n",
    "    measureStartTime = measure[0].offset - (measure[0].offset % 4)\n",
    "    measureStartOffset  = measure[0].offset - measureStartTime\n",
    "\n",
    "    # Iterate over the notes and rests in measure, finding the grammar for each\n",
    "    # note in the measure and adding an abstract grammatical string for it. \n",
    "\n",
    "    fullGrammar = \"\"\n",
    "    prevNote = None # Store previous note. Need for interval.\n",
    "    numNonRests = 0 # Number of non-rest elements. Need for updating prevNote.\n",
    "    for ix, nr in enumerate(measure):\n",
    "        # Get the last chord. If no last chord, then (assuming chords is of length\n",
    "        # >0) shift first chord in chords to the beginning of the measure.\n",
    "        try: \n",
    "            lastChord = [n for n in chords if n.offset <= nr.offset][-1]\n",
    "        except IndexError:\n",
    "            chords[0].offset = measureStartTime\n",
    "            lastChord = [n for n in chords if n.offset <= nr.offset][-1]\n",
    "\n",
    "        # FIRST, get type of note, e.g. R for Rest, C for Chord, etc.\n",
    "        # Dealing with solo notes here. If unexpected chord: still call 'C'.\n",
    "        elementType = ' '\n",
    "        # R: First, check if it's a rest. Clearly a rest --> only one possibility.\n",
    "        if isinstance(nr, note.Rest):\n",
    "            elementType = 'R'\n",
    "        # C: Next, check to see if note pitch is in the last chord.\n",
    "        elif nr.name in lastChord.pitchNames or isinstance(nr, chord.Chord):\n",
    "            elementType = 'C'\n",
    "        # L: (Complement tone) Skip this for now.\n",
    "        # S: Check if it's a scale tone.\n",
    "        elif __is_scale_tone(lastChord, nr):\n",
    "            elementType = 'S'\n",
    "        # A: Check if it's an approach tone, i.e. +-1 halfstep chord tone.\n",
    "        elif __is_approach_tone(lastChord, nr):\n",
    "            elementType = 'A'\n",
    "        # X: Otherwise, it's an arbitrary tone. Generate random note.\n",
    "        else:\n",
    "            elementType = 'X'\n",
    "\n",
    "        # SECOND, get the length for each element. e.g. 8th note = R8, but\n",
    "        # to simplify things you'll use the direct num, e.g. R,0.125\n",
    "        if (ix == (len(measure)-1)):\n",
    "            # formula for a in \"a - b\": start of measure (e.g. 476) + 4\n",
    "            diff = measureStartTime + 4.0 - nr.offset\n",
    "        else:\n",
    "            diff = measure[ix + 1].offset - nr.offset\n",
    "\n",
    "        # Combine into the note info.\n",
    "        noteInfo = \"%s,%.3f\" % (elementType, nr.quarterLength) # back to diff\n",
    "\n",
    "        # THIRD, get the deltas (max range up, max range down) based on where\n",
    "        # the previous note was, +- minor 3. Skip rests (don't affect deltas).\n",
    "        intervalInfo = \"\"\n",
    "        if isinstance(nr, note.Note):\n",
    "            numNonRests += 1\n",
    "            if numNonRests == 1:\n",
    "                prevNote = nr\n",
    "            else:\n",
    "                noteDist = interval.Interval(noteStart=prevNote, noteEnd=nr)\n",
    "                noteDistUpper = interval.add([noteDist, \"m3\"])\n",
    "                noteDistLower = interval.subtract([noteDist, \"m3\"])\n",
    "                intervalInfo = \",<%s,%s>\" % (noteDistUpper.directedName, \n",
    "                    noteDistLower.directedName)\n",
    "                # print \"Upper, lower: %s, %s\" % (noteDistUpper,\n",
    "                #     noteDistLower)\n",
    "                # print \"Upper, lower dnames: %s, %s\" % (\n",
    "                #     noteDistUpper.directedName,\n",
    "                #     noteDistLower.directedName)\n",
    "                # print \"The interval: %s\" % (intervalInfo)\n",
    "                prevNote = nr\n",
    "\n",
    "        # Return. Do lazy evaluation for real-time performance.\n",
    "        grammarTerm = noteInfo + intervalInfo \n",
    "        fullGrammar += (grammarTerm + \" \")\n",
    "\n",
    "    return fullGrammar.rstrip()\n",
    "\n",
    "''' Given a grammar string and chords for a measure, returns measure notes. '''\n",
    "def unparse_grammar(m1_grammar, m1_chords):\n",
    "    m1_elements = stream.Voice()\n",
    "    currOffset = 0.0 # for recalculate last chord.\n",
    "    prevElement = None\n",
    "    for ix, grammarElement in enumerate(m1_grammar.split(' ')):\n",
    "        terms = grammarElement.split(',')\n",
    "        currOffset += float(terms[1]) # works just fine\n",
    "\n",
    "        # Case 1: it's a rest. Just append\n",
    "        if terms[0] == 'R':\n",
    "            rNote = note.Rest(quarterLength = float(terms[1]))\n",
    "            m1_elements.insert(currOffset, rNote)\n",
    "            continue\n",
    "\n",
    "        # Get the last chord first so you can find chord note, scale note, etc.\n",
    "        try: \n",
    "            lastChord = [n for n in m1_chords if n.offset <= currOffset][-1]\n",
    "        except IndexError:\n",
    "            m1_chords[0].offset = 0.0\n",
    "            lastChord = [n for n in m1_chords if n.offset <= currOffset][-1]\n",
    "\n",
    "        # Case: no < > (should just be the first note) so generate from range\n",
    "        # of lowest chord note to highest chord note (if not a chord note, else\n",
    "        # just generate one of the actual chord notes). \n",
    "\n",
    "        # Case #1: if no < > to indicate next note range. Usually this lack of < >\n",
    "        # is for the first note (no precedent), or for rests.\n",
    "        if (len(terms) == 2): # Case 1: if no < >.\n",
    "            insertNote = note.Note() # default is C\n",
    "\n",
    "            # Case C: chord note.\n",
    "            if terms[0] == 'C':\n",
    "                insertNote = __generate_chord_tone(lastChord)\n",
    "\n",
    "            # Case S: scale note.\n",
    "            elif terms[0] == 'S':\n",
    "                insertNote = __generate_scale_tone(lastChord)\n",
    "\n",
    "            # Case A: approach note.\n",
    "            # Handle both A and X notes here for now.\n",
    "            else:\n",
    "                insertNote = __generate_approach_tone(lastChord)\n",
    "\n",
    "            # Update the stream of generated notes\n",
    "            insertNote.quarterLength = float(terms[1])\n",
    "            if insertNote.octave < 4:\n",
    "                insertNote.octave = 4\n",
    "            m1_elements.insert(currOffset, insertNote)\n",
    "            prevElement = insertNote\n",
    "\n",
    "        # Case #2: if < > for the increment. Usually for notes after the first one.\n",
    "        else:\n",
    "            # Get lower, upper intervals and notes.\n",
    "            interval1 = interval.Interval(terms[2].replace(\"<\",''))\n",
    "            interval2 = interval.Interval(terms[3].replace(\">\",''))\n",
    "            if interval1.cents > interval2.cents:\n",
    "                upperInterval, lowerInterval = interval1, interval2\n",
    "            else:\n",
    "                upperInterval, lowerInterval = interval2, interval1\n",
    "            lowPitch = interval.transposePitch(prevElement.pitch, lowerInterval)\n",
    "            highPitch = interval.transposePitch(prevElement.pitch, upperInterval)\n",
    "            numNotes = int(highPitch.ps - lowPitch.ps + 1) # for range(s, e)\n",
    "\n",
    "            # Case C: chord note, must be within increment (terms[2]).\n",
    "            # First, transpose note with lowerInterval to get note that is\n",
    "            # the lower bound. Then iterate over, and find valid notes. Then\n",
    "            # choose randomly from those.\n",
    "            \n",
    "            if terms[0] == 'C':\n",
    "                relevantChordTones = []\n",
    "                for i in range(0, numNotes):\n",
    "                    currNote = note.Note(lowPitch.transpose(i).simplifyEnharmonic())\n",
    "                    if __is_chord_tone(lastChord, currNote):\n",
    "                        relevantChordTones.append(currNote)\n",
    "                if len(relevantChordTones) > 1:\n",
    "                    insertNote = random.choice([i for i in relevantChordTones\n",
    "                        if i.nameWithOctave != prevElement.nameWithOctave])\n",
    "                elif len(relevantChordTones) == 1:\n",
    "                    insertNote = relevantChordTones[0]\n",
    "                else: # if no choices, set to prev element +-1 whole step\n",
    "                    insertNote = prevElement.transpose(random.choice([-2,2]))\n",
    "                if insertNote.octave < 3:\n",
    "                    insertNote.octave = 3\n",
    "                insertNote.quarterLength = float(terms[1])\n",
    "                m1_elements.insert(currOffset, insertNote)\n",
    "\n",
    "            # Case S: scale note, must be within increment.\n",
    "            elif terms[0] == 'S':\n",
    "                relevantScaleTones = []\n",
    "                for i in range(0, numNotes):\n",
    "                    currNote = note.Note(lowPitch.transpose(i).simplifyEnharmonic())\n",
    "                    if __is_scale_tone(lastChord, currNote):\n",
    "                        relevantScaleTones.append(currNote)\n",
    "                if len(relevantScaleTones) > 1:\n",
    "                    insertNote = random.choice([i for i in relevantScaleTones\n",
    "                        if i.nameWithOctave != prevElement.nameWithOctave])\n",
    "                elif len(relevantScaleTones) == 1:\n",
    "                    insertNote = relevantScaleTones[0]\n",
    "                else: # if no choices, set to prev element +-1 whole step\n",
    "                    insertNote = prevElement.transpose(random.choice([-2,2]))\n",
    "                if insertNote.octave < 3:\n",
    "                    insertNote.octave = 3\n",
    "                insertNote.quarterLength = float(terms[1])\n",
    "                m1_elements.insert(currOffset, insertNote)\n",
    "\n",
    "            # Case A: approach tone, must be within increment.\n",
    "            # For now: handle both A and X cases.\n",
    "            else:\n",
    "                relevantApproachTones = []\n",
    "                for i in range(0, numNotes):\n",
    "                    currNote = note.Note(lowPitch.transpose(i).simplifyEnharmonic())\n",
    "                    if __is_approach_tone(lastChord, currNote):\n",
    "                        relevantApproachTones.append(currNote)\n",
    "                if len(relevantApproachTones) > 1:\n",
    "                    insertNote = random.choice([i for i in relevantApproachTones\n",
    "                        if i.nameWithOctave != prevElement.nameWithOctave])\n",
    "                elif len(relevantApproachTones) == 1:\n",
    "                    insertNote = relevantApproachTones[0]\n",
    "                else: # if no choices, set to prev element +-1 whole step\n",
    "                    insertNote = prevElement.transpose(random.choice([-2,2]))\n",
    "                if insertNote.octave < 3:\n",
    "                    insertNote.octave = 3\n",
    "                insertNote.quarterLength = float(terms[1])\n",
    "                m1_elements.insert(currOffset, insertNote)\n",
    "\n",
    "            # update the previous element.\n",
    "            prevElement = insertNote\n",
    "\n",
    "    return m1_elements    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author:     Ji-Sung Kim, Evan Chow\n",
    "Project:    deepjazz\n",
    "Purpose:    Provide pruning and cleanup functions.\n",
    "\n",
    "Code adapted from Evan Chow's jazzml, https://github.com/evancchow/jazzml \n",
    "with express permission.\n",
    "'''\n",
    "from itertools import zip_longest\n",
    "import random\n",
    "\n",
    "from music21 import *\n",
    "\n",
    "#----------------------------HELPER FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Helper function to down num to the nearest multiple of mult. '''\n",
    "def __roundDown(num, mult):\n",
    "    return (float(num) - (float(num) % mult))\n",
    "\n",
    "''' Helper function to round up num to nearest multiple of mult. '''\n",
    "def __roundUp(num, mult):\n",
    "    return __roundDown(num, mult) + mult\n",
    "\n",
    "''' Helper function that, based on if upDown < 0 or upDown >= 0, rounds number \n",
    "    down or up respectively to nearest multiple of mult. '''\n",
    "def __roundUpDown(num, mult, upDown):\n",
    "    if upDown < 0:\n",
    "        return __roundDown(num, mult)\n",
    "    else:\n",
    "        return __roundUp(num, mult)\n",
    "\n",
    "''' Helper function, from recipes, to iterate over list in chunks of n \n",
    "    length. '''\n",
    "def __grouper(iterable, n, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "#----------------------------PUBLIC FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Smooth the measure, ensuring that everything is in standard note lengths \n",
    "    (e.g., 0.125, 0.250, 0.333 ... ). '''\n",
    "def prune_grammar(curr_grammar):\n",
    "    pruned_grammar = curr_grammar.split(' ')\n",
    "\n",
    "    for ix, gram in enumerate(pruned_grammar):\n",
    "        terms = gram.split(',')\n",
    "        terms[1] = str(__roundUpDown(float(terms[1]), 0.250, \n",
    "            random.choice([-1, 1])))\n",
    "        pruned_grammar[ix] = ','.join(terms)\n",
    "    pruned_grammar = ' '.join(pruned_grammar)\n",
    "\n",
    "    return pruned_grammar\n",
    "\n",
    "''' Remove repeated notes, and notes that are too close together. '''\n",
    "def prune_notes(curr_notes):\n",
    "    for n1, n2 in __grouper(curr_notes, n=2):\n",
    "        if n2 == None: # corner case: odd-length list\n",
    "            continue\n",
    "        if isinstance(n1, note.Note) and isinstance(n2, note.Note):\n",
    "            if n1.nameWithOctave == n2.nameWithOctave:\n",
    "                curr_notes.remove(n2)\n",
    "\n",
    "    return curr_notes\n",
    "\n",
    "''' Perform quality assurance on notes '''\n",
    "def clean_up_notes(curr_notes):\n",
    "    removeIxs = []\n",
    "    for ix, m in enumerate(curr_notes):\n",
    "        # QA1: ensure nothing is of 0 quarter note len, if so changes its len\n",
    "        if (m.quarterLength == 0.0):\n",
    "            m.quarterLength = 0.250\n",
    "        # QA2: ensure no two melody notes have same offset, i.e. form a chord.\n",
    "        # Sorted, so same offset would be consecutive notes.\n",
    "        if (ix < (len(curr_notes) - 1)):\n",
    "            if (m.offset == curr_notes[ix + 1].offset and\n",
    "                isinstance(curr_notes[ix + 1], note.Note)):\n",
    "                removeIxs.append((ix + 1))\n",
    "    curr_notes = [i for ix, i in enumerate(curr_notes) if ix not in removeIxs]\n",
    "\n",
    "    return curr_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef load_music_utils():\\n    chord_data, raw_music_data = get_musical_data('data/original_metheny.mid')\\n    music_data, values, values_indices, indices_values = get_corpus_data(raw_music_data)\\n\\n    X, Y = data_processing(music_data, values_indices, Tx = 20, step = 3)\\n    return (X, Y)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Author:     Ji-Sung Kim\n",
    "Project:    deepjazz\n",
    "Purpose:    Parse, cleanup and process data.\n",
    "\n",
    "Code adapted from Evan Chow's jazzml, https://github.com/evancchow/jazzml with\n",
    "express permission.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from music21 import *\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import groupby, zip_longest\n",
    "\n",
    "#from grammar import *\n",
    "\n",
    "#from grammar import parse_melody\n",
    "#from music_utils import *\n",
    "\n",
    "#----------------------------HELPER FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Helper function to parse a MIDI file into its measures and chords '''\n",
    "def __parse_midi(data_fn):\n",
    "    # Parse the MIDI data for separate melody and accompaniment parts.\n",
    "    midi_data = converter.parse(data_fn)\n",
    "    # Get melody part, compress into single voice.\n",
    "    melody_stream = midi_data[5]     # For Metheny piece, Melody is Part #5.\n",
    "    melody1, melody2 = melody_stream.getElementsByClass(stream.Voice)\n",
    "    for j in melody2:\n",
    "        melody1.insert(j.offset, j)\n",
    "    melody_voice = melody1\n",
    "\n",
    "    for i in melody_voice:\n",
    "        if i.quarterLength == 0.0:\n",
    "            i.quarterLength = 0.25\n",
    "\n",
    "    # Change key signature to adhere to comp_stream (1 sharp, mode = major).\n",
    "    # Also add Electric Guitar. \n",
    "    melody_voice.insert(0, instrument.ElectricGuitar())\n",
    "    melody_voice.insert(0, key.KeySignature(sharps=1))\n",
    "\n",
    "    # The accompaniment parts. Take only the best subset of parts from\n",
    "    # the original data. Maybe add more parts, hand-add valid instruments.\n",
    "    # Should add least add a string part (for sparse solos).\n",
    "    # Verified are good parts: 0, 1, 6, 7 '''\n",
    "    partIndices = [0, 1, 6, 7]\n",
    "    comp_stream = stream.Voice()\n",
    "    comp_stream.append([j.flat for i, j in enumerate(midi_data) \n",
    "        if i in partIndices])\n",
    "\n",
    "    # Full stream containing both the melody and the accompaniment. \n",
    "    # All parts are flattened. \n",
    "    full_stream = stream.Voice()\n",
    "    for i in range(len(comp_stream)):\n",
    "        full_stream.append(comp_stream[i])\n",
    "    full_stream.append(melody_voice)\n",
    "\n",
    "    # Extract solo stream, assuming you know the positions ..ByOffset(i, j).\n",
    "    # Note that for different instruments (with stream.flat), you NEED to use\n",
    "    # stream.Part(), not stream.Voice().\n",
    "    # Accompanied solo is in range [478, 548)\n",
    "    solo_stream = stream.Voice()\n",
    "    for part in full_stream:\n",
    "        curr_part = stream.Part()\n",
    "        curr_part.append(part.getElementsByClass(instrument.Instrument))\n",
    "        curr_part.append(part.getElementsByClass(tempo.MetronomeMark))\n",
    "        curr_part.append(part.getElementsByClass(key.KeySignature))\n",
    "        curr_part.append(part.getElementsByClass(meter.TimeSignature))\n",
    "        curr_part.append(part.getElementsByOffset(476, 548, \n",
    "                                                  includeEndBoundary=True))\n",
    "        cp = curr_part.flat\n",
    "        solo_stream.insert(cp)\n",
    "\n",
    "    # Group by measure so you can classify. \n",
    "    # Note that measure 0 is for the time signature, metronome, etc. which have\n",
    "    # an offset of 0.0.\n",
    "    melody_stream = solo_stream[-1]\n",
    "    measures = OrderedDict()\n",
    "    offsetTuples = [(int(n.offset / 4), n) for n in melody_stream]\n",
    "    measureNum = 0 # for now, don't use real m. nums (119, 120)\n",
    "    for key_x, group in groupby(offsetTuples, lambda x: x[0]):\n",
    "        measures[measureNum] = [n[1] for n in group]\n",
    "        measureNum += 1\n",
    "\n",
    "    # Get the stream of chords.\n",
    "    # offsetTuples_chords: group chords by measure number.\n",
    "    chordStream = solo_stream[0]\n",
    "    chordStream.removeByClass(note.Rest)\n",
    "    chordStream.removeByClass(note.Note)\n",
    "    offsetTuples_chords = [(int(n.offset / 4), n) for n in chordStream]\n",
    "\n",
    "    # Generate the chord structure. Use just track 1 (piano) since it is\n",
    "    # the only instrument that has chords. \n",
    "    # Group into 4s, just like before. \n",
    "    chords = OrderedDict()\n",
    "    measureNum = 0\n",
    "    for key_x, group in groupby(offsetTuples_chords, lambda x: x[0]):\n",
    "        chords[measureNum] = [n[1] for n in group]\n",
    "        measureNum += 1\n",
    "\n",
    "    # Fix for the below problem.\n",
    "    #   1) Find out why len(measures) != len(chords).\n",
    "    #   ANSWER: resolves at end but melody ends 1/16 before last measure so doesn't\n",
    "    #           actually show up, while the accompaniment's beat 1 right after does.\n",
    "    #           Actually on second thought: melody/comp start on Ab, and resolve to\n",
    "    #           the same key (Ab) so could actually just cut out last measure to loop.\n",
    "    #           Decided: just cut out the last measure. \n",
    "    del chords[len(chords) - 1]\n",
    "    assert len(chords) == len(measures)\n",
    "\n",
    "    return measures, chords\n",
    "\n",
    "''' Helper function to get the grammatical data from given musical data. '''\n",
    "def __get_abstract_grammars(measures, chords):\n",
    "    # extract grammars\n",
    "    abstract_grammars = []\n",
    "    for ix in range(1, len(measures)):\n",
    "        m = stream.Voice()\n",
    "        for i in measures[ix]:\n",
    "            m.insert(i.offset, i)\n",
    "        c = stream.Voice()\n",
    "        for j in chords[ix]:\n",
    "            c.insert(j.offset, j)\n",
    "        parsed = parse_melody(m, c)\n",
    "        abstract_grammars.append(parsed)\n",
    "\n",
    "    return abstract_grammars\n",
    "\n",
    "#----------------------------PUBLIC FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Get musical data from a MIDI file '''\n",
    "def get_musical_data(data_fn):\n",
    "    \n",
    "    measures, chords = __parse_midi(data_fn)\n",
    "    abstract_grammars = __get_abstract_grammars(measures, chords)\n",
    "\n",
    "    return chords, abstract_grammars\n",
    "\n",
    "''' Get corpus data from grammatical data '''\n",
    "def get_corpus_data(abstract_grammars):\n",
    "    corpus = [x for sublist in abstract_grammars for x in sublist.split(' ')]\n",
    "    values = set(corpus)\n",
    "    val_indices = dict((v, i) for i, v in enumerate(values))\n",
    "    indices_val = dict((i, v) for i, v in enumerate(values))\n",
    "\n",
    "    return corpus, values, val_indices, indices_val\n",
    "\n",
    "'''\n",
    "def load_music_utils():\n",
    "    chord_data, raw_music_data = get_musical_data('data/original_metheny.mid')\n",
    "    music_data, values, values_indices, indices_values = get_corpus_data(raw_music_data)\n",
    "\n",
    "    X, Y = data_processing(music_data, values_indices, Tx = 20, step = 3)\n",
    "    return (X, Y)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## music_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.layers import RepeatVector\n",
    "import sys\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "#from grammar import *\n",
    "#from preprocess import *\n",
    "#from qa import *\n",
    "\n",
    "\n",
    "def data_processing(corpus, values_indices, m = 60, Tx = 30):\n",
    "    # cut the corpus into semi-redundant sequences of Tx values\n",
    "    Tx = Tx \n",
    "    N_values = len(set(corpus))\n",
    "    np.random.seed(0)\n",
    "    X = np.zeros((m, Tx, N_values), dtype=np.bool)\n",
    "    Y = np.zeros((m, Tx, N_values), dtype=np.bool)\n",
    "    for i in range(m):\n",
    "#         for t in range(1, Tx):\n",
    "        random_idx = np.random.choice(len(corpus) - Tx)\n",
    "        corp_data = corpus[random_idx:(random_idx + Tx)]\n",
    "        for j in range(Tx):\n",
    "            idx = values_indices[corp_data[j]]\n",
    "            if j != 0:\n",
    "                X[i, j, idx] = 1\n",
    "                Y[i, j-1, idx] = 1\n",
    "    \n",
    "    Y = np.swapaxes(Y,0,1)\n",
    "    Y = Y.tolist()\n",
    "    return np.asarray(X), np.asarray(Y), N_values \n",
    "\n",
    "def next_value_processing(model, next_value, x, predict_and_sample, indices_values, abstract_grammars, duration, max_tries = 1000, temperature = 0.5):\n",
    "    \"\"\"\n",
    "    Helper function to fix the first value.\n",
    "    \n",
    "    Arguments:\n",
    "    next_value -- predicted and sampled value, index between 0 and 77\n",
    "    x -- numpy-array, one-hot encoding of next_value\n",
    "    predict_and_sample -- predict function\n",
    "    indices_values -- a python dictionary mapping indices (0-77) into their corresponding unique value (ex: A,0.250,< m2,P-4 >)\n",
    "    abstract_grammars -- list of grammars, on element can be: 'S,0.250,<m2,P-4> C,0.250,<P4,m-2> A,0.250,<P4,m-2>'\n",
    "    duration -- scalar, index of the loop in the parent function\n",
    "    max_tries -- Maximum numbers of time trying to fix the value\n",
    "    \n",
    "    Returns:\n",
    "    next_value -- process predicted value\n",
    "    \"\"\"\n",
    "\n",
    "    # fix first note: must not have < > and not be a rest\n",
    "    if (duration < 0.00001):\n",
    "        tries = 0\n",
    "        while (next_value.split(',')[0] == 'R' or \n",
    "            len(next_value.split(',')) != 2):\n",
    "            # give up after 1000 tries; random from input's first notes\n",
    "            if tries >= max_tries:\n",
    "                #print('Gave up on first note generation after', max_tries, 'tries')\n",
    "                # np.random is exclusive to high\n",
    "                rand = np.random.randint(0, len(abstract_grammars))\n",
    "                next_value = abstract_grammars[rand].split(' ')[0]\n",
    "            else:\n",
    "                next_value = predict_and_sample(model, x, indices_values, temperature)\n",
    "\n",
    "            tries += 1\n",
    "            \n",
    "    return next_value\n",
    "\n",
    "\n",
    "def sequence_to_matrix(sequence, values_indices):\n",
    "    \"\"\"\n",
    "    Convert a sequence (slice of the corpus) into a matrix (numpy) of one-hot vectors corresponding \n",
    "    to indices in values_indices\n",
    "    \n",
    "    Arguments:\n",
    "    sequence -- python list\n",
    "    \n",
    "    Returns:\n",
    "    x -- numpy-array of one-hot vectors \n",
    "    \"\"\"\n",
    "    sequence_len = len(sequence)\n",
    "    x = np.zeros((1, sequence_len, len(values_indices)))\n",
    "    for t, value in enumerate(sequence):\n",
    "        if (not value in values_indices): print(value)\n",
    "        x[0, t, values_indices[value]] = 1.\n",
    "    return x\n",
    "\n",
    "def one_hot(x):\n",
    "    x = K.argmax(x)\n",
    "    x = tf.one_hot(x, 78) \n",
    "    x = RepeatVector(1)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from music_utils import * \n",
    "#from preprocess import * \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "chords, abstract_grammars = get_musical_data('data/original_metheny.mid')\n",
    "corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
    "N_tones = len(set(corpus))\n",
    "n_a = 64\n",
    "x_initializer = np.zeros((1, 1, 78))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))\n",
    "\n",
    "def load_music_utils():\n",
    "    chords, abstract_grammars = get_musical_data('data/original_metheny.mid')\n",
    "    corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
    "    N_tones = len(set(corpus))\n",
    "    X, Y, N_tones = data_processing(corpus, tones_indices, 60, 30)   \n",
    "    return (X, Y, N_tones, indices_tones)\n",
    "\n",
    "\n",
    "def generate_music(inference_model, corpus = corpus, abstract_grammars = abstract_grammars, tones = tones, tones_indices = tones_indices, indices_tones = indices_tones, T_y = 10, max_tries = 1000, diversity = 0.5):\n",
    "    \"\"\"\n",
    "    Generates music using a model trained to learn musical patterns of a jazz soloist. Creates an audio stream\n",
    "    to save the music and play it.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- Keras model Instance, output of djmodel()\n",
    "    corpus -- musical corpus, list of 193 tones as strings (ex: 'C,0.333,<P1,d-5>')\n",
    "    abstract_grammars -- list of grammars, on element can be: 'S,0.250,<m2,P-4> C,0.250,<P4,m-2> A,0.250,<P4,m-2>'\n",
    "    tones -- set of unique tones, ex: 'A,0.250,<M2,d-4>' is one element of the set.\n",
    "    tones_indices -- a python dictionary mapping unique tone (ex: A,0.250,< m2,P-4 >) into their corresponding indices (0-77)\n",
    "    indices_tones -- a python dictionary mapping indices (0-77) into their corresponding unique tone (ex: A,0.250,< m2,P-4 >)\n",
    "    Tx -- integer, number of time-steps used at training time\n",
    "    temperature -- scalar value, defines how conservative/creative the model is when generating music\n",
    "    \n",
    "    Returns:\n",
    "    predicted_tones -- python list containing predicted tones\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up audio stream\n",
    "    out_stream = stream.Stream()\n",
    "    \n",
    "    # Initialize chord variables\n",
    "    curr_offset = 0.0                                     # variable used to write sounds to the Stream.\n",
    "    num_chords = int(len(chords) / 3)                     # number of different set of chords\n",
    "    \n",
    "    print(\"Predicting new values for different set of chords.\")\n",
    "    # Loop over all 18 set of chords. At each iteration generate a sequence of tones\n",
    "    # and use the current chords to convert it into actual sounds \n",
    "    for i in range(1, num_chords):\n",
    "        \n",
    "        # Retrieve current chord from stream\n",
    "        curr_chords = stream.Voice()\n",
    "        \n",
    "        # Loop over the chords of the current set of chords\n",
    "        for j in chords[i]:\n",
    "            # Add chord to the current chords with the adequate offset, no need to understand this\n",
    "            curr_chords.insert((j.offset % 4), j)\n",
    "        \n",
    "        # Generate a sequence of tones using the model\n",
    "        _, indices = predict_and_sample(inference_model)\n",
    "        indices = list(indices.squeeze())\n",
    "        pred = [indices_tones[p] for p in indices]\n",
    "        \n",
    "        predicted_tones = 'C,0.25 '\n",
    "        for k in range(len(pred) - 1):\n",
    "            predicted_tones += pred[k] + ' ' \n",
    "        \n",
    "        predicted_tones +=  pred[-1]\n",
    "                \n",
    "        #### POST PROCESSING OF THE PREDICTED TONES ####\n",
    "        # We will consider \"A\" and \"X\" as \"C\" tones. It is a common choice.\n",
    "        predicted_tones = predicted_tones.replace(' A',' C').replace(' X',' C')\n",
    "\n",
    "        # Pruning #1: smoothing measure\n",
    "        predicted_tones = prune_grammar(predicted_tones)\n",
    "        \n",
    "        # Use predicted tones and current chords to generate sounds\n",
    "        sounds = unparse_grammar(predicted_tones, curr_chords)\n",
    "\n",
    "        # Pruning #2: removing repeated and too close together sounds\n",
    "        sounds = prune_notes(sounds)\n",
    "\n",
    "        # Quality assurance: clean up sounds\n",
    "        sounds = clean_up_notes(sounds)\n",
    "\n",
    "        # Print number of tones/notes in sounds\n",
    "        print('Generated %s sounds using the predicted values for the set of chords (\"%s\") and after pruning' % (len([k for k in sounds if isinstance(k, note.Note)]), i))\n",
    "        \n",
    "        # Insert sounds into the output stream\n",
    "        for m in sounds:\n",
    "            out_stream.insert(curr_offset + m.offset, m)\n",
    "        for mc in curr_chords:\n",
    "            out_stream.insert(curr_offset + mc.offset, mc)\n",
    "\n",
    "        curr_offset += 4.0\n",
    "        \n",
    "    # Initialize tempo of the output stream with 130 bit per minute\n",
    "    out_stream.insert(0.0, tempo.MetronomeMark(number=130))\n",
    "\n",
    "    # Save audio stream to fine\n",
    "    mf = midi.translate.streamToMidiFile(out_stream)\n",
    "    mf.open(\"output/my_music.midi\", 'wb')\n",
    "    mf.write()\n",
    "    print(\"Your generated music is saved in output/my_music.midi\")\n",
    "    mf.close()\n",
    "    \n",
    "    # Play the final stream through output (see 'play' lambda function above)\n",
    "    # play = lambda x: midi.realtime.StreamPlayer(x).play()\n",
    "    # play(out_stream)\n",
    "    \n",
    "    return out_stream\n",
    "\n",
    "\n",
    "def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    Ty -- length of the sequence you'd like to generate.\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])\n",
    "    indices = np.argmax(pred, axis = -1)\n",
    "    results = to_categorical(indices, num_classes=78)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import IPython\n",
    "import sys\n",
    "from music21 import *\n",
    "import numpy as np\n",
    "#from grammar import *\n",
    "#from qa import *\n",
    "#from preprocess import * \n",
    "#from music_utils import *\n",
    "#from data_utils import *\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 问题描述\n",
    "\n",
    "你想为朋友的生日创作一段爵士乐，但你不会任何乐器，也不懂作曲。幸运的是，你懂深度学习，可以使用 LSTM 网络来解决这个问题。\n",
    "\n",
    "你将训练一个网络，让它生成新的爵士独奏作品，并且风格类似于已有的表演曲目。\n",
    "\n",
    "<img src=\"images/jazz.jpg\" style=\"width:450;height:300px;\">\n",
    "\n",
    "### 1.1 - 数据集\n",
    "\n",
    "你将用一组爵士乐曲目训练算法。运行下面的单元可以试听训练集中的音频片段：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('./data/30s_seq.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经对音乐数据进行了预处理，将其表示为音乐“数值”。可以非正式地把每个“数值”理解为一个音符，它包含音高和时长。例如，如果你按下某个钢琴键持续 0.5 秒，你就演奏了一个音符。在音乐理论中，一个“数值”实际上比这更复杂——它还包含演奏多个音符同时发声所需的信息。例如，在演奏音乐作品时，你可能同时按下两个钢琴键（同时演奏多个音符产生所谓的“和弦”）。但是在本作业中，我们不需要深入音乐理论的细节。你只需要知道，我们将得到一个数值数据集，并使用 RNN 模型学习生成数值序列。\n",
    "\n",
    "我们的音乐生成系统将使用 78 个独特的数值。运行以下代码可以加载原始音乐数据并将其预处理为数值形式。这可能需要几分钟。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, n_values, indices_values = load_music_utils()\n",
    "print('shape of X:', X.shape)\n",
    "print('number of training examples:', X.shape[0])\n",
    "print('Tx (length of sequence):', X.shape[1])\n",
    "print('total # of unique values:', n_values)\n",
    "print('Shape of Y:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你刚刚加载了以下内容：\n",
    "\n",
    "- `X`：这是一个维度为 (m, $T_x$, 78) 的数组。我们有 m 个训练样本，每个样本是长度为 $T_x = 30$ 的音乐片段。在每个时间步，输入是 78 个可能值之一，用 one-hot 向量表示。例如，`X[i, t, :]` 表示第 i 个样本在时间步 t 的 one-hot 向量。\n",
    "\n",
    "- `Y`：本质上与 `X` 相同，但向左（过去方向）移动了一步。类似于恐龙名字任务，我们希望网络利用之前的值来预测下一个值，因此我们的序列模型会尝试预测 $y^{\\langle t \\rangle}$，给定 $x^{\\langle 1\\rangle}, \\ldots, x^{\\langle t \\rangle}$。不过，`Y` 的数据被重新排列为维度 $(T_y, m, 78)$，其中 $T_y = T_x$。这种格式更方便后续输入到 LSTM。\n",
    "\n",
    "- `n_values`：数据集中唯一值的数量。这里应该是 78。\n",
    "\n",
    "- `indices_values`：Python 字典，将 0-77 映射到对应的音乐值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - 我们模型概述\n",
    "\n",
    "下面是我们将使用的模型架构。这与之前笔记中使用的恐龙名字模型类似，不同之处在于这里我们将使用 Keras 来实现。架构如下：\n",
    "\n",
    "<img src=\"images/music_generation.png\" style=\"width:600;height:400px;\">\n",
    "\n",
    "我们将使用来自较长音乐片段的随机 30 个值的片段来训练模型。因此，我们不再像之前为恐龙名字设置 $x^{\\langle 1 \\rangle} = \\vec{0}$ 来表示序列开始，因为这些音频片段大多数都是从音乐中间某个位置截取的。我们将每个片段设置为相同长度 $T_x = 30$，以便更方便地进行向量化处理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 构建模型\n",
    "\n",
    "在本部分，你将构建并训练一个能够学习音乐模式的模型。为此，你需要构建一个模型，其输入 `X` 的形状为 $(m, T_x, 78)$，输出 `Y` 的形状为 $(T_y, m, 78)$。我们将使用一个隐藏状态维度为 64 的 LSTM。令 `n_a = 64`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是如何在 Keras 中创建具有多个输入和输出的模型。如果你要构建一个 RNN，并且在测试时整个输入序列 $x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\ldots, x^{\\langle T_x \\rangle}$ 已知，例如输入是单词而输出是一个标签，那么 Keras 提供了简单的内置函数来构建模型。然而，对于序列生成任务，在测试时我们并不知道所有 $x^{\\langle t\\rangle}$ 的值；相反，我们会使用 $x^{\\langle t\\rangle} = y^{\\langle t-1 \\rangle}$ 一次生成一个值。因此代码会稍微复杂一些，你需要自己实现一个 for 循环来迭代不同的时间步。\n",
    "\n",
    "函数 `djmodel()` 将使用 for 循环调用 LSTM 层 $T_x$ 次，重要的是所有 $T_x$ 个副本应共享相同的权重。也就是说，不应每次都重新初始化权重——$T_x$ 个时间步应该共享权重。实现 Keras 中可共享权重层的关键步骤如下：\n",
    "1. 定义层对象（我们将使用全局变量来实现）。\n",
    "2. 在前向传播时调用这些对象。\n",
    "\n",
    "我们已将所需的层对象定义为全局变量。请运行下一格来创建它们。你可以查阅 Keras 文档以确保理解这些层：[Reshape()](https://keras.io/layers/core/#reshape), [LSTM()](https://keras.io/layers/recurrent/#lstm), [Dense()](https://keras.io/layers/core/#dense)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapor = Reshape((1, 78))                        # Used in Step 2.B of djmodel(), below\n",
    "LSTM_cell = LSTM(n_a, return_state = True)         # Used in Step 2.C\n",
    "densor = Dense(n_values, activation='softmax')     # Used in Step 2.D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，`reshapor`、`LSTM_cell` 和 `densor` 都是层对象，你可以用它们来实现 `djmodel()`。为了将一个 Keras 张量对象 X 传递通过某个层，可以使用 `layer_object(X)`（如果该层需要多个输入，则使用 `layer_object([X,Y])`）。例如，`reshapor(X)` 会将 X 传入之前定义的 `Reshape((1,78))` 层进行前向传播。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：实现 `djmodel()`。你需要执行以下两个步骤：\n",
    "\n",
    "1. 创建一个空列表 `outputs` 来保存每个时间步 LSTM 单元的输出。\n",
    "2. 对 $t \\in 1, \\ldots, T_x$ 进行循环：\n",
    "\n",
    "   A. 从 X 中选择第 t 个时间步的向量。这个选择的形状应该是 (78,)。你可以通过创建一个自定义的 [Lambda](https://keras.io/layers/core/#lambda) 层来实现：\n",
    "```python\n",
    "x = Lambda(lambda x: X[:,t,:])(X)\n",
    "\n",
    "``` \n",
    "查看 Keras 文档以理解其作用。它创建了一个“临时”或“匿名”函数（Lambda 函数），用于提取对应的 one-hot 向量，并将该函数封装成 Keras 的 `Layer` 对象来应用于 X。\n",
    "\n",
    "B. 将 x 重塑为 (1,78)。你可以使用下面定义的 `reshapor()` 层来实现。\n",
    "\n",
    "C. 将 x 传入 LSTM_cell 的一个时间步。记得用上一步的隐藏状态 $a$ 和细胞状态 $c$ 初始化 LSTM_cell，格式如下：\n",
    "```python\n",
    "a, _, c = LSTM_cell(input_x, initial_state=[previous hidden state, previous cell state])\n",
    "\n",
    "```\n",
    "\n",
    "    D. 将 LSTM 的输出激活值通过一个 dense + softmax 层传递，使用 `densor` 实现。\n",
    "    \n",
    "    E. 将预测的值追加到 \"outputs\" 列表中。\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: djmodel\n",
    "\n",
    "def djmodel(Tx, n_a, n_values):\n",
    "    \"\"\"\n",
    "    Implement the model\n",
    "    \n",
    "    Arguments:\n",
    "    Tx -- length of the sequence in a corpus\n",
    "    n_a -- the number of activations used in our model\n",
    "    n_values -- number of unique values in the music data \n",
    "    \n",
    "    Returns:\n",
    "    model -- a keras model with the \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    X = Input(shape=(Tx, n_values))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    # Step 1: Create empty list to append the outputs while you iterate (≈1 line)\n",
    "\n",
    "    \n",
    "    # Step 2: Loop\n",
    "\n",
    "    \n",
    "        # Step 2.A: select the \"t\"th time step vector from X. \n",
    "\n",
    "        # Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)\n",
    "\n",
    "        # Step 2.C: Perform one step of the LSTM_cell\n",
    "\n",
    "        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell\n",
    "\n",
    "        # Step 2.E: add the output to \"outputs\"\n",
    "\n",
    "        \n",
    "    # Step 3: Create model instance\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的代码单元来定义你的模型。我们将使用 `Tx=30`、`n_a=64`（LSTM 激活的维度）以及 `n_values=78`。此单元可能需要几秒钟才能运行完成。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = djmodel(Tx = 30 , n_a = 64, n_values = 78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你需要编译你的模型以进行训练。我们将使用 Adam 优化器和分类交叉熵（categorical cross-entropy）损失函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们将 LSTM 的初始隐藏状态 `a0` 和初始细胞状态 `c0` 初始化为零。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 60\n",
    "a0 = np.zeros((m, n_a))\n",
    "c0 = np.zeros((m, n_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们开始训练模型！在训练之前，我们将 `Y` 转换为列表格式，因为损失函数要求 `Y` 以这种格式提供（每个时间步一个列表项）。因此，`list(Y)` 是一个包含 30 个元素的列表，每个元素的形状为 (60, 78)。我们将训练 100 个 epoch，这将花费几分钟时间。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([X, a0, c0], list(Y), epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你应该会看到模型的损失在下降。现在模型已经训练完成，让我们进入最后一部分，来实现推理算法，并生成一些音乐！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 生成音乐\n",
    "\n",
    "现在你已经有了一个训练好的模型，它学会了爵士独奏者的演奏模式。让我们使用这个模型来合成新的音乐。\n",
    "\n",
    "#### 3.1 - 预测与采样\n",
    "\n",
    "<img src=\"images/music_gen.png\" style=\"width:600;height:400px;\">\n",
    "\n",
    "在每一步采样中，你将使用来自 LSTM 前一状态的激活 `a` 和细胞状态 `c`，向前传播一步，得到新的输出激活和细胞状态。新的激活 `a` 可以像之前一样，通过 `densor` 生成输出。\n",
    "\n",
    "为了开始模型生成，我们将初始化 `x0`，以及 LSTM 的激活和细胞状态 `a0` 和 `c0` 为零。\n",
    "\n",
    "\n",
    "**练习**：实现下面的函数，用于采样一段音乐序列。以下是在生成 $T_y$ 个输出音符的 for 循环中需要实现的关键步骤：\n",
    "\n",
    "- Step 2.A：使用 `LSTM_Cell`，输入前一步的 `c` 和 `a` 来生成当前步骤的 `c` 和 `a`。\n",
    "- Step 2.B：使用之前定义的 `densor` 对 `a` 做 softmax，得到当前步骤的输出。\n",
    "- Step 2.C：将刚生成的输出保存，通过追加到 `outputs` 列表中。\n",
    "- Step 2.D：将 x 设置为输出 `out` 的 one-hot 版本（预测值），以便传递给下一步 LSTM。我们已经提供了这行代码，使用了 [Lambda](https://keras.io/layers/core/#lambda) 函数：\n",
    "```python\n",
    "x = Lambda(one_hot)(out)\n",
    "\n",
    "```\n",
    "[小技术说明：与根据 `out` 中的概率随机采样不同，这行代码实际上在每一步使用 argmax 选择最可能的音符。]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: music_inference_model\n",
    "\n",
    "def music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 100):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    n_values -- integer, umber of unique values\n",
    "    n_a -- number of units in the LSTM_cell\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "\n",
    "    \n",
    "        # Step 2.A: Perform one step of LSTM_cell (≈1 line)\n",
    "\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "\n",
    "        \n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, 78) (≈1 line)\n",
    "\n",
    "        \n",
    "        # Step 2.D: Select the next value according to \"out\", and set \"x\" to be the one-hot representation of the\n",
    "        #           selected value, which will be passed as the input to LSTM_cell on the next step. We have provided \n",
    "        #           the line of code you need to do this. \n",
    "\n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的单元格以定义推理模型。该模型被硬编码为生成 50 个音符值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，这会创建用于初始化输入向量 `x` 以及 LSTM 状态变量 `a` 和 `c` 的全零向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, 78))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：实现 `predict_and_sample()` 函数。该函数接收多个参数，包括输入 `[x_initializer, a_initializer, c_initializer]`。为了预测对应于这些输入的输出，你需要完成以下 3 个步骤：\n",
    "\n",
    "1. 使用你的推理模型（inference model）对输入进行预测。输出 `pred` 应该是一个长度为 50 的列表，每个元素都是一个形状为 ($T_y$, n_values) 的 numpy 数组。\n",
    "2. 将 `pred` 转换为 $T_y$ 个索引组成的 numpy 数组。每个索引通过对 `pred` 列表中的元素取 `argmax` 计算得到。[提示](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html)。\n",
    "3. 将索引转换为它们的 one-hot 向量表示。[提示](https://keras.io/utils/#to_categorical)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict_and_sample\n",
    "\n",
    "def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "\n",
    "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "\n",
    "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (1, )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)\n",
    "print(\"np.argmax(results[12]) =\", np.argmax(results[12]))\n",
    "print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
    "print(\"list(indices[12:18]) =\", list(indices[12:18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**：由于 Keras 的结果并不是完全可预测的，因此你的结果可能有所不同。不过，如果你按照上述说明使用 `model.fit()` 对 LSTM_cell 训练了整整 100 个 epoch，你很可能会看到一个索引序列，并非所有元素都相同。此外，你应当会观察到：\n",
    "\n",
    "- `np.argmax(results[12])` 是 `list(indices[12:18])` 的第一个元素\n",
    "- `np.argmax(results[17])` 是 `list(indices[12:18])` 的最后一个元素\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **np.argmax(results[12])** =\n",
    "        </td>\n",
    "        <td>\n",
    "            1\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **np.argmax(results[17])** =\n",
    "        </td>\n",
    "        <td>\n",
    "            42\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **list(indices[12:18])** =\n",
    "        </td>\n",
    "        <td>\n",
    "            [array([1]), array([42]), array([54]), array([17]), array([1]), array([42])]\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - 生成音乐\n",
    "\n",
    "现在，你已经可以生成音乐了。你的 RNN 会生成一个值的序列。下面的代码会先调用你的 `predict_and_sample()` 函数生成这些值。生成的值随后会被后处理为音乐和弦（意味着可以同时播放多个值或音符）。\n",
    "\n",
    "大多数计算机音乐算法都会进行某种后处理，因为如果没有这些处理，很难生成听起来悦耳的音乐。后处理会做一些操作，例如清理生成的音频，确保同一个声音不会重复太多次，相邻的两个音符在音高上不会相差太远，等等。有人可能会认为这些后处理步骤很多都是技巧性的；此外，许多音乐生成文献也强调手工设计后处理器，而且输出的质量很大程度上依赖于后处理的质量，而不仅仅依赖于 RNN 的质量。但是这些后处理确实对最终效果影响很大，因此我们在实现中也会使用它。\n",
    "\n",
    "让我们生成一些音乐吧！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的代码单元来生成音乐，并将其记录到 `out_stream` 中。这可能需要几分钟时间。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_stream = generate_music(inference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要聆听你生成的音乐，请点击 **File -> Open...**，然后进入 `output/` 文件夹，下载 `my_music.midi` 文件。你可以在电脑上使用支持 MIDI 文件的播放器播放，或者使用免费的在线“MIDI 转 MP3”工具将其转换为 MP3 格式。\n",
    "\n",
    "作为参考，这里也提供了一个我们使用该算法生成的 30 秒音频片段。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('./data/30s_trained_model.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恭喜！\n",
    "\n",
    "你已经完成了本笔记本的学习。\n",
    "\n",
    "<font color=\"blue\">\n",
    "你应该记住的要点如下：\n",
    "- 序列模型可以用来生成音乐值，这些值随后可以通过后处理生成 MIDI 音乐。\n",
    "- 相当类似的模型既可以用来生成恐龙名字，也可以用来生成音乐，主要区别在于输入数据的不同。\n",
    "- 在 Keras 中，序列生成涉及定义共享权重的层，然后将这些层在不同的时间步 $1, \\ldots, T_x$ 中重复使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你完成了本次作业并成功生成了一段爵士独奏！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参考文献**\n",
    "\n",
    "本笔记本中介绍的思想主要来源于以下三篇计算音乐相关的论文。同时，本实现也参考并借鉴了 Ji-Sung Kim 的 GitHub 仓库中的许多组件。\n",
    "\n",
    "- Ji-Sung Kim, 2016, [deepjazz](https://github.com/jisungk/deepjazz)\n",
    "- Jon Gillick, Kevin Tang 和 Robert Keller, 2009. [Learning Jazz Grammars](http://ai.stanford.edu/~kdtang/papers/smc09-jazzgrammar.pdf)\n",
    "- Robert Keller 和 David Morrison, 2007, [A Grammatical Approach to Automatic Improvisation](http://smc07.uoa.gr/SMC07%20Proceedings/SMC07%20Paper%2055.pdf)\n",
    "- François Pachet, 1999, [Surprising Harmonies](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.7473&rep=rep1&type=pdf)\n",
    "\n",
    "我们也感谢 François Germain 提供的宝贵反馈。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "EG0F7",
   "launcher_item_id": "cxJXc"
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
