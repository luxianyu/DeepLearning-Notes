{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ - æé¾™ä¹å›­\n",
    "\n",
    "æ¬¢è¿æ¥åˆ°æé¾™å²›ï¼ğŸ¦–  \n",
    "åœ¨6500ä¸‡å¹´å‰ï¼Œæé¾™æ›¾ç»å­˜åœ¨ï¼Œè€Œåœ¨æœ¬æ¬¡è¯¾ç¨‹ä¸­ï¼Œå®ƒä»¬åˆå›æ¥äº†ã€‚  \n",
    "ä½ è´Ÿè´£ä¸€ä¸ªç‰¹æ®Šä»»åŠ¡ï¼šç”Ÿç‰©å­¦é¡¶å°–ç ”ç©¶äººå‘˜æ­£åœ¨åˆ›é€ æ–°çš„æé¾™å“ç§å¹¶å°†å®ƒä»¬å¸¦å›åœ°çƒï¼Œè€Œä½ çš„å·¥ä½œæ˜¯ä¸ºè¿™äº›æé¾™**å‘½å**ã€‚  \n",
    "å¦‚æœæé¾™ä¸å–œæ¬¢å®ƒçš„åå­—ï¼Œå®ƒå¯èƒ½ä¼šå‘ç‹‚ï¼Œæ‰€ä»¥è¯·æ…é‡é€‰æ‹©ï¼\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:250;height:300px;\">\n",
    "</td>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "å¹¸è¿çš„æ˜¯ï¼Œä½ å·²ç»å­¦ä¹ äº†ä¸€äº›æ·±åº¦å­¦ä¹ çŸ¥è¯†ï¼Œå¯ä»¥ç”¨å®ƒæ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚  \n",
    "ä½ çš„åŠ©æ‰‹å·²ç»æ”¶é›†äº†æ‰€æœ‰èƒ½æ‰¾åˆ°çš„æé¾™åå­—ï¼Œå¹¶å°†å®ƒä»¬æ•´ç†åˆ°è¿™ä¸ª [æ•°æ®é›†](dinos.txt) ä¸­ã€‚ï¼ˆå¯ä»¥ç‚¹å‡»é“¾æ¥æŸ¥çœ‹æ•°æ®å†…å®¹ï¼‰  \n",
    "ä¸ºäº†åˆ›é€ æ–°çš„æé¾™åå­—ï¼Œä½ å°†æ„å»ºä¸€ä¸ª**å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ï¼ˆcharacter-level language modelï¼‰**ã€‚  \n",
    "ä½ çš„ç®—æ³•å°†å­¦ä¹ ä¸åŒåå­—çš„æ¨¡å¼ï¼Œå¹¶éšæœºç”Ÿæˆæ–°çš„åå­—ã€‚å¸Œæœ›è¿™ä¸ªç®—æ³•èƒ½å¸®ä½ é¿å…æé¾™çš„æ„¤æ€’ï¼  \n",
    "\n",
    "---\n",
    "\n",
    "é€šè¿‡å®Œæˆæœ¬è¯¾ç¨‹ï¼Œä½ å°†å­¦åˆ°ï¼š\n",
    "\n",
    "- å¦‚ä½•å°†æ–‡æœ¬æ•°æ®å­˜å‚¨å¹¶ç”¨äº RNN å¤„ç†ï¼›\n",
    "- å¦‚ä½•ç”Ÿæˆæ•°æ®ï¼Œé€šè¿‡åœ¨æ¯ä¸ªæ—¶é—´æ­¥é‡‡æ ·é¢„æµ‹ç»“æœå¹¶å°†å…¶ä¼ é€’ç»™ä¸‹ä¸€ä¸ª RNN å•å…ƒï¼›\n",
    "- å¦‚ä½•æ„å»º**å­—ç¬¦çº§æ–‡æœ¬ç”Ÿæˆå¾ªç¯ç¥ç»ç½‘ç»œ**ï¼›\n",
    "- ä¸ºä»€ä¹ˆæ¢¯åº¦è£å‰ªï¼ˆgradient clippingï¼‰å¾ˆé‡è¦ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "æˆ‘ä»¬å°†ä»åŠ è½½æä¾›çš„ `rnn_utils` å‡½æ•°å¼€å§‹ã€‚  \n",
    "ä½ å¯ä»¥ä½¿ç”¨è¯¸å¦‚ `rnn_forward` å’Œ `rnn_backward` ç­‰å‡½æ•°ï¼Œå®ƒä»¬ä¸å‰ä¸€ä¸ªä½œä¸šä¸­ä½ å®ç°çš„åŠŸèƒ½ç›¸åŒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥ NumPy åº“ï¼Œç”¨äºç§‘å­¦è®¡ç®—å’ŒçŸ©é˜µæ“ä½œ\n",
    "import numpy as np  \n",
    "\n",
    "# ä» utils æ¨¡å—å¯¼å…¥æ‰€æœ‰å‡½æ•°å’Œå˜é‡\n",
    "# utils.py æ˜¯ä½ è‡ªå·±å†™çš„å·¥å…·æ¨¡å—ï¼Œå¯èƒ½åŒ…å«æ•°æ®å¤„ç†ã€ç»˜å›¾ã€æŸå¤±å‡½æ•°ç­‰è‡ªå®šä¹‰å‡½æ•°\n",
    "from utils import *  \n",
    "\n",
    "# å¯¼å…¥ Python å†…ç½®çš„ random æ¨¡å—ï¼Œç”¨äºç”Ÿæˆéšæœºæ•°ã€éšæœºé€‰æ‹©ç­‰æ“ä½œ\n",
    "import random  \n",
    "\n",
    "# ä» random æ¨¡å—ä¸­å•ç‹¬å¯¼å…¥ shuffle å‡½æ•°\n",
    "# shuffle å¯ä»¥å°†åˆ—è¡¨ä¸­çš„å…ƒç´ éšæœºæ‰“ä¹±é¡ºåº\n",
    "from random import shuffle  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 1 - é—®é¢˜æè¿°\n",
    "\n",
    "### 1.1 - æ•°æ®é›†ä¸é¢„å¤„ç†\n",
    "\n",
    "è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼ï¼Œè¯»å–æé¾™åå­—æ•°æ®é›†ï¼Œ  \n",
    "åˆ›å»ºä¸€ä¸ªå”¯ä¸€å­—ç¬¦åˆ—è¡¨ï¼ˆä¾‹å¦‚ a-zï¼‰ï¼Œå¹¶è®¡ç®—æ•°æ®é›†å¤§å°ä»¥åŠè¯æ±‡è¡¨å¤§å°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "# æ‰“å¼€æ–‡æœ¬æ–‡ä»¶ 'dinos.txt'ï¼Œä»¥åªè¯»æ¨¡å¼ 'r' æ‰“å¼€ï¼Œå¹¶è¯»å–æ–‡ä»¶å†…å®¹\n",
    "# read() ä¼šå°†æ•´ä¸ªæ–‡ä»¶å†…å®¹è¯»å–æˆä¸€ä¸ªå­—ç¬¦ä¸²\n",
    "data = open('dinos.txt', 'r').read()  \n",
    "\n",
    "# å°†æ–‡æœ¬å…¨éƒ¨è½¬æ¢ä¸ºå°å†™å­—æ¯ï¼Œä¿è¯å¤§å°å†™ä¸€è‡´æ€§\n",
    "# è¿™æ ·å¯ä»¥å‡å°‘æ¨¡å‹éœ€è¦å­¦ä¹ çš„å­—ç¬¦ç§ç±»\n",
    "data = data.lower()  \n",
    "\n",
    "# ä½¿ç”¨ set() è·å–æ–‡æœ¬ä¸­æ‰€æœ‰å”¯ä¸€å­—ç¬¦ï¼Œå†ç”¨ list() è½¬æ¢ä¸ºåˆ—è¡¨\n",
    "# chars åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ æ˜¯æ–‡æœ¬ä¸­å‡ºç°è¿‡çš„ä¸€ä¸ªå­—ç¬¦ï¼ˆæ— é‡å¤ï¼‰\n",
    "chars = list(set(data))  \n",
    "\n",
    "# è®¡ç®—æ–‡æœ¬æ€»å­—ç¬¦æ•°ï¼Œå³æ–‡æœ¬é•¿åº¦\n",
    "data_size = len(data)  \n",
    "\n",
    "# è®¡ç®—æ–‡æœ¬ä¸­å”¯ä¸€å­—ç¬¦çš„æ•°é‡ï¼Œå³å­—å…¸å¤§å°\n",
    "vocab_size = len(chars)  \n",
    "\n",
    "# æ‰“å°æ–‡æœ¬æ€»å­—ç¬¦æ•°å’Œå”¯ä¸€å­—ç¬¦æ•°\n",
    "# %d æ˜¯æ ¼å¼åŒ–å ä½ç¬¦ï¼Œdata_size å’Œ vocab_size å°†åˆ†åˆ«æ›¿æ¢\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™äº›å­—ç¬¦åŒ…æ‹¬ a-zï¼ˆå…± 26 ä¸ªå­—ç¬¦ï¼‰ä»¥åŠæ¢è¡Œç¬¦ `\\n`ï¼Œ  \n",
    "åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œå®ƒçš„ä½œç”¨ç±»ä¼¼äº `<EOS>`ï¼ˆå¥å­ç»“æŸï¼‰æ ‡è®°ï¼Œåªä¸è¿‡è¿™é‡Œè¡¨ç¤º**æé¾™åå­—çš„ç»“æŸ**ï¼Œè€Œä¸æ˜¯å¥å­çš„ç»“æŸã€‚\n",
    "\n",
    "åœ¨ä¸‹é¢çš„å•å…ƒæ ¼ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª Python å­—å…¸ï¼ˆå“ˆå¸Œè¡¨ï¼‰ï¼š\n",
    "- `char_to_ix`ï¼šå°†æ¯ä¸ªå­—ç¬¦æ˜ å°„ä¸ºç´¢å¼• 0-26ï¼›\n",
    "- `ix_to_char`ï¼šå°†æ¯ä¸ªç´¢å¼•æ˜ å°„å›å¯¹åº”å­—ç¬¦ã€‚\n",
    "\n",
    "è¿™æ ·ï¼Œä½ å°±å¯ä»¥æ ¹æ® softmax è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒç¡®å®šæ¯ä¸ªç´¢å¼•å¯¹åº”çš„å­—ç¬¦ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºå­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸\n",
    "# enumerate() ä¼šè¿”å›ä¸€ä¸ªç´¢å¼• i å’Œå­—ç¬¦ ch\n",
    "# sorted(chars) å¯¹å­—ç¬¦åˆ—è¡¨è¿›è¡Œæ’åºï¼Œè¿™æ ·æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´\n",
    "# æœ€ç»ˆå¾—åˆ°å­—å…¸ char_to_ixï¼Œkey ä¸ºå­—ç¬¦ï¼Œvalue ä¸ºå¯¹åº”çš„ç´¢å¼•\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "\n",
    "# åˆ›å»ºç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„å­—å…¸\n",
    "# ä½œç”¨ä¸ä¸Šé¢çš„å­—å…¸ç›¸å\n",
    "# key ä¸ºç´¢å¼• iï¼Œvalue ä¸ºå­—ç¬¦ ch\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "\n",
    "# æ‰“å°ç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„å­—å…¸ï¼Œä¾¿äºæŸ¥çœ‹ç»“æœ\n",
    "# ä¾‹å¦‚ï¼š{0: ' ', 1: 'a', 2: 'b', ...}\n",
    "print(ix_to_char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - æ¨¡å‹æ¦‚è§ˆ\n",
    "\n",
    "ä½ çš„æ¨¡å‹å°†å…·æœ‰ä»¥ä¸‹ç»“æ„ï¼š\n",
    "\n",
    "- åˆå§‹åŒ–å‚æ•°  \n",
    "- è¿è¡Œä¼˜åŒ–å¾ªç¯ï¼š\n",
    "  - å‰å‘ä¼ æ’­è®¡ç®—æŸå¤±å‡½æ•°  \n",
    "  - åå‘ä¼ æ’­è®¡ç®—æŸå¤±å‡½æ•°å…³äºå‚æ•°çš„æ¢¯åº¦  \n",
    "  - å¯¹æ¢¯åº¦è¿›è¡Œè£å‰ªï¼ˆgradient clippingï¼‰ï¼Œä»¥é¿å…æ¢¯åº¦çˆ†ç‚¸  \n",
    "  - ä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°ï¼ˆæ¢¯åº¦ä¸‹é™æ›´æ–°è§„åˆ™ï¼‰  \n",
    "- è¿”å›è®­ç»ƒå¾—åˆ°çš„å‚æ•°\n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> **å›¾ 1**ï¼šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œä¸ä¹‹å‰ â€œBuilding a RNN - Step by Stepâ€ ä¸­æ„å»ºçš„ç½‘ç»œç±»ä¼¼ã€‚ </center></caption>\n",
    "\n",
    "åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼ŒRNN å°è¯•é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œç»™å®šå‰é¢çš„å­—ç¬¦ã€‚  \n",
    "- æ•°æ®é›† $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ æ˜¯è®­ç»ƒé›†ä¸­å­—ç¬¦çš„åˆ—è¡¨ï¼›  \n",
    "- æ•°æ®é›† $Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ æ»¡è¶³ï¼šåœ¨æ¯ä¸ªæ—¶é—´æ­¥ $t$ï¼Œæœ‰ $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - æ¨¡å‹çš„æ„å»ºæ¨¡å—\n",
    "\n",
    "åœ¨æœ¬éƒ¨åˆ†ï¼Œä½ å°†å®ç°æ•´ä¸ªæ¨¡å‹çš„ä¸¤ä¸ªé‡è¦æ¨¡å—ï¼š\n",
    "\n",
    "- **æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰**ï¼šç”¨äºé¿å…æ¢¯åº¦çˆ†ç‚¸  \n",
    "- **é‡‡æ ·ï¼ˆSamplingï¼‰**ï¼šç”¨äºç”Ÿæˆå­—ç¬¦çš„æŠ€æœ¯\n",
    "\n",
    "éšåï¼Œä½ å°†ä½¿ç”¨è¿™ä¸¤ä¸ªå‡½æ•°æ¥æ„å»ºå®Œæ•´çš„æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - åœ¨ä¼˜åŒ–å¾ªç¯ä¸­è£å‰ªæ¢¯åº¦\n",
    "\n",
    "åœ¨æœ¬èŠ‚ä¸­ï¼Œä½ å°†å®ç° `clip` å‡½æ•°ï¼Œå¹¶åœ¨ä¼˜åŒ–å¾ªç¯ä¸­è°ƒç”¨å®ƒã€‚  \n",
    "å›é¡¾ä¸€ä¸‹ï¼Œæ•´ä¸ªå¾ªç¯ç»“æ„é€šå¸¸åŒ…æ‹¬ï¼šå‰å‘ä¼ æ’­ â†’ æŸå¤±è®¡ç®— â†’ åå‘ä¼ æ’­ â†’ å‚æ•°æ›´æ–°ã€‚  \n",
    "åœ¨æ›´æ–°å‚æ•°ä¹‹å‰ï¼Œéœ€è¦è¿›è¡Œ**æ¢¯åº¦è£å‰ª**ï¼Œç¡®ä¿æ¢¯åº¦ä¸ä¼šâ€œçˆ†ç‚¸â€ï¼Œå³å–å€¼è¿‡å¤§ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "åœ¨ç»ƒä¹ ä¸­ï¼Œä½ å°†å®ç° `clip` å‡½æ•°ï¼š\n",
    "- è¾“å…¥ï¼šä¸€ä¸ªæ¢¯åº¦å­—å…¸ `gradients`  \n",
    "- è¾“å‡ºï¼šè£å‰ªåçš„æ¢¯åº¦å­—å…¸  \n",
    "\n",
    "è£å‰ªæ–¹æ³•ï¼š**é€å…ƒç´ è£å‰ª**ï¼ˆelement-wise clippingï¼‰  \n",
    "- ç»™å®šä¸€ä¸ªé˜ˆå€¼ `maxValue`ï¼ˆä¾‹å¦‚ 10ï¼‰  \n",
    "- è‹¥æ¢¯åº¦åˆ†é‡å¤§äº 10ï¼Œåˆ™è®¾ä¸º 10  \n",
    "- è‹¥æ¢¯åº¦åˆ†é‡å°äº -10ï¼Œåˆ™è®¾ä¸º -10  \n",
    "- è‹¥æ¢¯åº¦åˆ†é‡åœ¨ [-10, 10] å†…ï¼Œåˆ™ä¿æŒä¸å˜\n",
    "\n",
    "<img src=\"images/clip.png\" style=\"width:400;height:150px;\">\n",
    "<caption><center> **å›¾ 2**ï¼šæœ‰æ— æ¢¯åº¦è£å‰ªçš„æ¢¯åº¦ä¸‹é™ç¤ºæ„å›¾ï¼Œå½“ç½‘ç»œå‡ºç°è½»å¾®â€œæ¢¯åº¦çˆ†ç‚¸â€é—®é¢˜æ—¶ã€‚ </center></caption>\n",
    "\n",
    "---\n",
    "\n",
    "**ç»ƒä¹ **ï¼šå®ç° `clip` å‡½æ•°ï¼Œè¿”å›è£å‰ªåçš„æ¢¯åº¦å­—å…¸ã€‚  \n",
    "å¯ä»¥å‚è€ƒæ­¤ [æç¤º](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    å¯¹æ¢¯åº¦è¿›è¡Œè£å‰ªï¼ˆClippingï¼‰ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "    gradients -- åŒ…å«å„ä¸ªæ¢¯åº¦çš„å­—å…¸ï¼Œå¸¸è§é”®åŒ…æ‹¬:\n",
    "                 \"dWaa\"  -> éšè—åˆ°éšè—æƒé‡çŸ©é˜µçš„æ¢¯åº¦\n",
    "                 \"dWax\"  -> è¾“å…¥åˆ°éšè—æƒé‡çŸ©é˜µçš„æ¢¯åº¦\n",
    "                 \"dWya\"  -> éšè—åˆ°è¾“å‡ºæƒé‡çŸ©é˜µçš„æ¢¯åº¦\n",
    "                 \"db\"    -> éšè—å±‚åç½®çš„æ¢¯åº¦\n",
    "                 \"dby\"   -> è¾“å‡ºå±‚åç½®çš„æ¢¯åº¦\n",
    "    maxValue -- æœ€å¤§æ¢¯åº¦é˜ˆå€¼ï¼Œæ¢¯åº¦ä¼šè¢«é™åˆ¶åœ¨ [-maxValue, maxValue] èŒƒå›´å†…\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "    gradients -- è£å‰ªåçš„æ¢¯åº¦å­—å…¸\n",
    "    '''\n",
    "\n",
    "    # ä»å­—å…¸ä¸­æå–å„ä¸ªæ¢¯åº¦çŸ©é˜µ\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    # å¯¹æ¯ä¸€ä¸ªæ¢¯åº¦è¿›è¡Œè£å‰ªæ“ä½œ\n",
    "    # np.clip(gradient, min, max, out=gradient) ä¼šå°† gradient ä¸­å¤§äº max çš„å€¼è®¾ä¸º maxï¼Œ\n",
    "    # å°äº min çš„å€¼è®¾ä¸º minï¼Œå¹¶ç›´æ¥ä¿®æ”¹åŸçŸ©é˜µï¼ˆout=gradientï¼‰\n",
    "    for gradient in [dWaa, dWax, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    \n",
    "    # å°†è£å‰ªåçš„æ¢¯åº¦é‡æ–°æ‰“åŒ…æˆå­—å…¸ï¼Œä¾¿äºè¿”å›\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®éšæœºç§å­ï¼Œä¿è¯æ¯æ¬¡ç”Ÿæˆçš„éšæœºæ•°ä¸€è‡´ï¼Œä¾¿äºå®éªŒå¯å¤ç°\n",
    "np.random.seed(3)\n",
    "\n",
    "# éšæœºç”Ÿæˆæ¢¯åº¦çŸ©é˜µå¹¶æ”¾å¤§10å€ï¼Œæ¨¡æ‹Ÿæ¢¯åº¦å¯èƒ½è¿‡å¤§çš„æƒ…å†µ\n",
    "dWax = np.random.randn(5,3) * 10  # è¾“å…¥åˆ°éšè—å±‚æƒé‡çš„æ¢¯åº¦ï¼Œ5è¡Œ3åˆ—\n",
    "dWaa = np.random.randn(5,5) * 10  # éšè—åˆ°éšè—å±‚æƒé‡çš„æ¢¯åº¦ï¼Œ5è¡Œ5åˆ—\n",
    "dWya = np.random.randn(2,5) * 10  # éšè—åˆ°è¾“å‡ºå±‚æƒé‡çš„æ¢¯åº¦ï¼Œ2è¡Œ5åˆ—\n",
    "db = np.random.randn(5,1) * 10    # éšè—å±‚åç½®çš„æ¢¯åº¦ï¼Œ5è¡Œ1åˆ—\n",
    "dby = np.random.randn(2,1) * 10   # è¾“å‡ºå±‚åç½®çš„æ¢¯åº¦ï¼Œ2è¡Œ1åˆ—\n",
    "\n",
    "# å°†æ‰€æœ‰æ¢¯åº¦æ‰“åŒ…æˆå­—å…¸ï¼Œä¾¿äºå‡½æ•°ä¼ å‚\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "\n",
    "# è°ƒç”¨clipå‡½æ•°ï¼Œå¯¹æ¢¯åº¦è¿›è¡Œè£å‰ªï¼Œé˜ˆå€¼è®¾ç½®ä¸º10\n",
    "# è¶…è¿‡10çš„å€¼ä¼šè¢«é™åˆ¶ä¸º10ï¼Œå°äº-10çš„å€¼ä¼šè¢«é™åˆ¶ä¸º-10\n",
    "gradients = clip(gradients, 10)\n",
    "\n",
    "# è¾“å‡ºè£å‰ªåçš„æ¢¯åº¦çŸ©é˜µä¸­æŒ‡å®šå…ƒç´ ï¼Œè§‚å¯Ÿè£å‰ªæ•ˆæœ\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])  # æŸ¥çœ‹dWaaç¬¬2è¡Œç¬¬3åˆ—\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])  # æŸ¥çœ‹dWaxç¬¬4è¡Œç¬¬2åˆ—\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])  # æŸ¥çœ‹dWyaç¬¬2è¡Œç¬¬3åˆ—\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])            # æŸ¥çœ‹dbç¬¬5è¡Œ\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])          # æŸ¥çœ‹dbyç¬¬2è¡Œ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWaa\"][1][2] **\n",
    "    </td>\n",
    "    <td> \n",
    "    10.0\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWax\"][3][1]**\n",
    "    </td>\n",
    "    <td> \n",
    "    -10.0\n",
    "    </td>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWya\"][1][2]**\n",
    "    </td>\n",
    "    <td> \n",
    "0.29713815361\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"db\"][4]**\n",
    "    </td>\n",
    "    <td> \n",
    "[ 10.]\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dby\"][1]**\n",
    "    </td>\n",
    "    <td> \n",
    "[ 8.45833407]\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - é‡‡æ ·ï¼ˆSamplingï¼‰\n",
    "\n",
    "ç°åœ¨å‡è®¾ä½ çš„æ¨¡å‹å·²ç»è®­ç»ƒå®Œæˆï¼Œä½ å¸Œæœ›ç”Ÿæˆæ–°çš„æ–‡æœ¬ï¼ˆå­—ç¬¦ï¼‰ã€‚  \n",
    "ç”Ÿæˆè¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **å›¾ 3**ï¼šå‡è®¾æ¨¡å‹å·²è®­ç»ƒå®Œæ¯•ã€‚æˆ‘ä»¬åœ¨ç¬¬ä¸€æ—¶é—´æ­¥è¾“å…¥ $x^{\\langle 1\\rangle} = \\vec{0}$ï¼Œç½‘ç»œéšåé€æ­¥é‡‡æ ·ç”Ÿæˆå­—ç¬¦ã€‚ </center></caption>\n",
    "\n",
    "---\n",
    "\n",
    "**ç»ƒä¹ **ï¼šå®ç° `sample` å‡½æ•°æ¥ç”Ÿæˆå­—ç¬¦ï¼ŒåŒ…å«ä»¥ä¸‹å››ä¸ªæ­¥éª¤ï¼š\n",
    "\n",
    "#### **æ­¥éª¤ 1**ï¼šåˆå§‹åŒ–è¾“å…¥\n",
    "- å°†ç½‘ç»œçš„ç¬¬ä¸€ä¸ªè¾“å…¥è®¾ä¸ºâ€œè™šæ‹Ÿè¾“å…¥â€ $x^{\\langle 1 \\rangle} = \\vec{0}$ï¼ˆå…¨é›¶å‘é‡ï¼‰  \n",
    "- å°†éšè—çŠ¶æ€åˆå§‹åŒ–ä¸º $a^{\\langle 0 \\rangle} = \\vec{0}$\n",
    "\n",
    "#### **æ­¥éª¤ 2**ï¼šå‰å‘ä¼ æ’­ä¸€æ­¥\n",
    "è®¡ç®— $a^{\\langle 1 \\rangle}$ å’Œé¢„æµ‹ $\\hat{y}^{\\langle 1 \\rangle}$ï¼š\n",
    "\n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t \\rangle } + W_{aa} a^{\\langle t \\rangle } + b) \\tag{1} $$\n",
    "\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2} $$\n",
    "\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle }) \\tag{3} $$\n",
    "\n",
    "> æ³¨æ„ï¼š$\\hat{y}^{\\langle t+1 \\rangle }$ æ˜¯ä¸€ä¸ªæ¦‚ç‡å‘é‡ï¼Œå…ƒç´ åœ¨ 0 åˆ° 1 ä¹‹é—´å¹¶ä¸”å’Œä¸º 1ã€‚  \n",
    "> $\\hat{y}^{\\langle t+1 \\rangle}_i$ è¡¨ç¤ºç´¢å¼•ä¸º \"i\" çš„å­—ç¬¦æ˜¯ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡ã€‚  \n",
    "> æˆ‘ä»¬æä¾›äº† `softmax()` å‡½æ•°å¯ç›´æ¥ä½¿ç”¨ã€‚\n",
    "\n",
    "#### **æ­¥éª¤ 3**ï¼šæŒ‰æ¦‚ç‡é‡‡æ ·\n",
    "- æ ¹æ® $\\hat{y}^{\\langle t+1 \\rangle }$ çš„æ¦‚ç‡åˆ†å¸ƒé€‰æ‹©ä¸‹ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•ã€‚  \n",
    "- ç¤ºä¾‹ï¼š\n",
    "```python\n",
    "np.random.seed(0)\n",
    "p = np.array([0.1, 0.0, 0.7, 0.2])\n",
    "index = np.random.choice([0, 1, 2, 3], p = p.ravel())\n",
    "\n",
    "```\n",
    "è¿™æ„å‘³ç€ä½ å°†æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé€‰æ‹© `index`ï¼š  \n",
    "$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$ã€‚\n",
    "\n",
    "- **æ­¥éª¤ 4**ï¼šåœ¨ `sample()` ä¸­çš„æœ€åä¸€æ­¥æ˜¯æ›´æ–°å˜é‡ `x`ï¼š\n",
    "  - å½“å‰ `x` å­˜å‚¨çš„æ˜¯ $x^{\\langle t \\rangle }$  \n",
    "  - å°†å…¶æ›´æ–°ä¸º $x^{\\langle t + 1 \\rangle }$  \n",
    "  - ä½¿ç”¨ä½ é¢„æµ‹çš„å­—ç¬¦ç”Ÿæˆå¯¹åº”çš„ **one-hot å‘é‡**  \n",
    "  - å†æ¬¡è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¹¶é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°ç”Ÿæˆ `\\n` å­—ç¬¦ï¼Œè¡¨ç¤ºæé¾™åå­—ç”Ÿæˆå®Œæˆã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    æŒ‰ç…§RNNè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·å­—ç¬¦åºåˆ—\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "    parameters -- å­—å…¸ï¼ŒåŒ…å«RNNçš„å‚æ•°Waa, Wax, Wya, by, b\n",
    "    char_to_ix -- å­—å…¸ï¼Œå°†å­—ç¬¦æ˜ å°„ä¸ºç´¢å¼•\n",
    "    seed -- éšæœºç§å­ï¼Œç”¨äºä¿è¯ç»“æœå¯å¤ç°\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "    indices -- ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«ç”Ÿæˆå­—ç¬¦çš„ç´¢å¼•åºåˆ—\n",
    "    \"\"\"\n",
    "    \n",
    "    # ä»å‚æ•°å­—å…¸ä¸­æå–RNNå‚æ•°\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    # è¯æ±‡è¡¨å¤§å°ï¼Œæ ¹æ®è¾“å‡ºå±‚åç½®ç»´åº¦ç¡®å®š\n",
    "    vocab_size = by.shape[0]\n",
    "    \n",
    "    # éšè—çŠ¶æ€å¤§å°ï¼Œæ ¹æ®Waaåˆ—æ•°ç¡®å®š\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # æ­¥éª¤1: åˆå§‹åŒ–è¾“å…¥å­—ç¬¦çš„ç‹¬çƒ­å‘é‡x\n",
    "    x = np.zeros((vocab_size,1))  # shape = (vocab_size, 1)\n",
    "    \n",
    "    # åˆå§‹åŒ–å‰ä¸€éšè—çŠ¶æ€a_prevä¸ºé›¶å‘é‡\n",
    "    a_prev = np.zeros((n_a,1))    # shape = (n_a, 1)\n",
    "    \n",
    "    # ç”¨äºå­˜å‚¨é‡‡æ ·å‡ºçš„å­—ç¬¦ç´¢å¼•\n",
    "    indices = []\n",
    "    \n",
    "    # åˆå§‹åŒ–idxä¸º-1ï¼Œç”¨ä½œå¾ªç¯æ§åˆ¶ï¼Œç›´åˆ°é‡‡æ ·åˆ°æ¢è¡Œç¬¦åœæ­¢\n",
    "    idx = -1\n",
    "    \n",
    "    # åˆå§‹åŒ–è®¡æ•°å™¨ï¼Œç”¨äºé™åˆ¶ç”Ÿæˆçš„æœ€å¤§å­—ç¬¦æ•°\n",
    "    counter = 0\n",
    "    \n",
    "    # æ¢è¡Œç¬¦åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•\n",
    "    newline_character = char_to_ix[\"\\n\"]\n",
    "    \n",
    "    # å¾ªç¯ç›´åˆ°é‡‡æ ·åˆ°æ¢è¡Œç¬¦æˆ–ç”Ÿæˆå­—ç¬¦æ•°è¾¾åˆ°50\n",
    "    while (idx != newline_character and counter < 50):\n",
    "        \n",
    "        # æ­¥éª¤2ï¼šå‰å‘ä¼ æ’­è®¡ç®—ä¸‹ä¸€ä¸ªéšè—çŠ¶æ€å’Œè¾“å‡º\n",
    "        # a = tanh(Wax * x + Waa * a_prev + b)\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        \n",
    "        # z = Wya * a + byï¼Œç”¨äºè¾“å‡ºå±‚\n",
    "        z = np.dot(Wya, a) + by\n",
    "        \n",
    "        # y = softmax(z)ï¼Œå¾—åˆ°ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å®éªŒå¯å¤ç°\n",
    "        np.random.seed(counter + seed)\n",
    "        \n",
    "        # æ­¥éª¤3ï¼šæ ¹æ®æ¦‚ç‡åˆ†å¸ƒyéšæœºé‡‡æ ·å­—ç¬¦ç´¢å¼•\n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "        \n",
    "        # å°†é‡‡æ ·ç´¢å¼•æ·»åŠ åˆ°indicesåˆ—è¡¨ä¸­\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # æ­¥éª¤4: æ„é€ ä¸‹ä¸€æ—¶é—´æ­¥è¾“å…¥å­—ç¬¦çš„ç‹¬çƒ­å‘é‡\n",
    "        x = np.zeros((vocab_size,1))  # å…ˆæ¸…é›¶\n",
    "        x[idx] = 1                    # å°†é‡‡æ ·åˆ°çš„å­—ç¬¦ä½ç½®ç½®ä¸º1\n",
    "        \n",
    "        # æ›´æ–°éšè—çŠ¶æ€ä¸ºå½“å‰æ—¶é—´æ­¥çš„a\n",
    "        a_prev = a \n",
    "        \n",
    "        # æ›´æ–°éšæœºç§å­å’Œè®¡æ•°å™¨\n",
    "        seed += 1\n",
    "        counter += 1\n",
    "\n",
    "    # å¦‚æœè¾¾åˆ°50ä¸ªå­—ç¬¦ä»æœªé‡‡æ ·åˆ°æ¢è¡Œç¬¦ï¼Œåˆ™å¼ºåˆ¶æ·»åŠ æ¢è¡Œç¬¦\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    # è¿”å›ç”Ÿæˆå­—ç¬¦çš„ç´¢å¼•åºåˆ—\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices: [18, 2, 26, 0]\n",
      "list of sampled characters: ['r', 'b', 'z', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®éšæœºç§å­ï¼Œä½¿ç»“æœå¯å¤ç°\n",
    "np.random.seed(2)\n",
    "\n",
    "# å®šä¹‰ç”Ÿæˆçš„å­—ç¬¦æ•°é‡nå’Œéšè—å±‚ç¥ç»å…ƒæ•°é‡n_a\n",
    "n, n_a = 20, 100  # n: è¾“å‡ºåºåˆ—é•¿åº¦(æ­¤å¤„æœªä¸¥æ ¼ç”¨åˆ°)ï¼Œn_a: éšè—çŠ¶æ€ç»´åº¦\n",
    "\n",
    "# åˆå§‹åŒ–éšè—çŠ¶æ€a0ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºå€¼\n",
    "a0 = np.random.randn(n_a, 1)  # shape = (n_a, 1)\n",
    "\n",
    "# åˆå§‹åŒ–ç¬¬ä¸€ä¸ªè¾“å…¥å­—ç¬¦ç´¢å¼•ï¼Œè¿™é‡Œé€‰æ‹©i0 = 1ï¼Œè¡¨ç¤ºè¯æ±‡è¡¨ä¸­çš„ç¬¬äºŒä¸ªå­—ç¬¦\n",
    "i0 = 1  # ç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯ ix_to_char[i0]\n",
    "\n",
    "# åˆå§‹åŒ–æƒé‡çŸ©é˜µ\n",
    "# Wax: è¾“å…¥åˆ°éšè—çŠ¶æ€çš„æƒé‡ï¼Œshape = (n_a, vocab_size)\n",
    "# Waa: ä¸Šä¸€éšè—çŠ¶æ€åˆ°å½“å‰éšè—çŠ¶æ€çš„æƒé‡ï¼Œshape = (n_a, n_a)\n",
    "# Wya: éšè—çŠ¶æ€åˆ°è¾“å‡ºçš„æƒé‡ï¼Œshape = (vocab_size, n_a)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "\n",
    "# åˆå§‹åŒ–åç½®å‘é‡\n",
    "# b: éšè—å±‚åç½®ï¼Œshape = (n_a, 1)\n",
    "# by: è¾“å‡ºå±‚åç½®ï¼Œshape = (vocab_size, 1)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "\n",
    "# å°†æ‰€æœ‰å‚æ•°å­˜å…¥å­—å…¸ä¸­ï¼Œä¾¿äºä¼ é€’ç»™RNNé‡‡æ ·å‡½æ•°\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "# ä½¿ç”¨sampleå‡½æ•°ç”Ÿæˆå­—ç¬¦ç´¢å¼•åºåˆ—\n",
    "# å‚æ•°:\n",
    "# - parameters: RNNå‚æ•°å­—å…¸\n",
    "# - char_to_ix: å­—ç¬¦åˆ°ç´¢å¼•æ˜ å°„\n",
    "# - 0: éšæœºç§å­\n",
    "indexes = sample(parameters, char_to_ix, 0)\n",
    "\n",
    "# æ‰“å°é‡‡æ ·ç»“æœ\n",
    "print(\"Sampling:\")\n",
    "\n",
    "# æ‰“å°é‡‡æ ·åˆ°çš„ç´¢å¼•åˆ—è¡¨\n",
    "print(\"list of sampled indices:\", indexes)\n",
    "\n",
    "# å°†ç´¢å¼•æ˜ å°„å›å­—ç¬¦ï¼Œæ‰“å°é‡‡æ ·åˆ°çš„å­—ç¬¦åºåˆ—\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indexes])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "<table>\n",
    "<tr>\n",
    "    <td> \n",
    "    **list of sampled indices:**\n",
    "    </td>\n",
    "    <td> \n",
    "    [18, 2, 26, 0]\n",
    "    </td>\n",
    "    </tr><tr>\n",
    "    <td> \n",
    "    **list of sampled characters:**\n",
    "    </td>\n",
    "    <td> \n",
    "    ['r', 'b', 'z', '\\n']\n",
    "    </td>\n",
    "    \n",
    "        \n",
    "    \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - æ„å»ºè¯­è¨€æ¨¡å‹\n",
    "\n",
    "ç°åœ¨æ˜¯æ—¶å€™æ„å»ºå­—ç¬¦çº§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬ç”Ÿæˆäº†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 - æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰\n",
    "\n",
    "åœ¨æœ¬èŠ‚ä¸­ï¼Œä½ å°†å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œæ‰§è¡Œ**ä¸€æ­¥éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰**ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¿›è¡Œæ¢¯åº¦è£å‰ªã€‚  \n",
    "è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½ å°†é€ä¸ªæ ·æœ¬è¿›è¡Œä¼˜åŒ–ï¼Œå› æ­¤ä¼˜åŒ–ç®—æ³•ä¸ºéšæœºæ¢¯åº¦ä¸‹é™ã€‚  \n",
    "\n",
    "å›é¡¾ä¸€ä¸‹ RNN çš„å¸¸ç”¨ä¼˜åŒ–å¾ªç¯æ­¥éª¤ï¼š\n",
    "\n",
    "1. å‰å‘ä¼ æ’­é€šè¿‡ RNN è®¡ç®—æŸå¤±\n",
    "2. åå‘ä¼ æ’­è®¡ç®—æŸå¤±ç›¸å¯¹äºå‚æ•°çš„æ¢¯åº¦ï¼ˆBPTTï¼‰\n",
    "3. å¦‚æœ‰å¿…è¦ï¼Œå¯¹æ¢¯åº¦è¿›è¡Œè£å‰ª\n",
    "4. ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°\n",
    "\n",
    "**ç»ƒä¹ **ï¼šå®ç°ä¸Šè¿°ä¼˜åŒ–è¿‡ç¨‹ï¼ˆä¸€æ¬¡éšæœºæ¢¯åº¦ä¸‹é™ï¼‰ã€‚\n",
    "\n",
    "æˆ‘ä»¬æä¾›äº†ä»¥ä¸‹å‡½æ•°ï¼š\n",
    "\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\" \n",
    "    æ‰§è¡Œ RNN å‰å‘ä¼ æ’­å¹¶è®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚\n",
    "    è¿”å›æŸå¤±å€¼ä»¥åŠå­˜å‚¨ç”¨äºåå‘ä¼ æ’­çš„ç¼“å­˜ã€‚\n",
    "    \"\"\"\n",
    "    ....\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" \n",
    "    æ‰§è¡Œåå‘ä¼ æ’­ï¼ˆæ—¶é—´å±•å¼€ï¼‰è®¡ç®—æŸå¤±ç›¸å¯¹äºå‚æ•°çš„æ¢¯åº¦ã€‚\n",
    "    åŒæ—¶è¿”å›æ‰€æœ‰éšè—çŠ¶æ€ã€‚\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\" \n",
    "    ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°ã€‚\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return parameters\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–æ­¥éª¤ï¼ˆå³ä¸€è½®å‰å‘ä¼ æ’­ + åå‘ä¼ æ’­ + å‚æ•°æ›´æ–°ï¼‰ã€‚\n",
    "\n",
    "    å‚æ•°è¯´æ˜:\n",
    "    X -- è¾“å…¥åºåˆ—ï¼ˆå­—ç¬¦ç´¢å¼•åˆ—è¡¨ï¼‰ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯è¯æ±‡è¡¨ä¸­æŸä¸ªå­—ç¬¦çš„ç¼–å·ã€‚\n",
    "    Y -- ç›®æ ‡åºåˆ—ï¼ˆä¸Xç›¸åŒï¼Œä½†å‘å·¦å¹³ç§»ä¸€ä¸ªå­—ç¬¦ï¼Œç”¨äºç›‘ç£è®­ç»ƒï¼‰ã€‚\n",
    "    a_prev -- ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œshape = (n_a, 1)\n",
    "    parameters -- åŒ…å«RNNå‚æ•°çš„å­—å…¸:\n",
    "        Wax -- è¾“å…¥åˆ°éšè—å±‚çš„æƒé‡çŸ©é˜µï¼Œshape = (n_a, n_x)\n",
    "        Waa -- éšè—å±‚åˆ°éšè—å±‚çš„æƒé‡çŸ©é˜µï¼Œshape = (n_a, n_a)\n",
    "        Wya -- éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡çŸ©é˜µï¼Œshape = (n_y, n_a)\n",
    "        b -- éšè—å±‚åç½®é¡¹ï¼Œshape = (n_a, 1)\n",
    "        by -- è¾“å‡ºå±‚åç½®é¡¹ï¼Œshape = (n_y, 1)\n",
    "    learning_rate -- å­¦ä¹ ç‡ï¼ˆæ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•¿ï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "    loss -- å½“å‰æ—¶é—´æ­¥çš„æŸå¤±å€¼ï¼ˆäº¤å‰ç†µï¼‰\n",
    "    gradients -- æ¢¯åº¦å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š\n",
    "        dWax, dWaa, dWya, db, dby\n",
    "    a[len(X)-1] -- æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ (å³ä¸‹ä¸€æ¬¡è¿­ä»£çš„ a_prev)\n",
    "    \"\"\"\n",
    "\n",
    "    # ====== â‘  å‰å‘ä¼ æ’­ ======\n",
    "    # è¾“å…¥ X åºåˆ—ï¼Œè®¡ç®—æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€å’Œè¾“å‡ºé¢„æµ‹ï¼Œå¹¶è¿”å›æŸå¤±å€¼\n",
    "    # cache ä¸­ä¿å­˜ä¸­é—´ç»“æœï¼ˆéšè—çŠ¶æ€ã€æ¿€æ´»å€¼ç­‰ï¼‰ï¼Œä¾›åå‘ä¼ æ’­ä½¿ç”¨\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    \n",
    "    # ====== â‘¡ åå‘ä¼ æ’­ ======\n",
    "    # æ ¹æ®å‰å‘ä¼ æ’­ç¼“å­˜çš„ä¸­é—´å˜é‡è®¡ç®—å‚æ•°çš„æ¢¯åº¦\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    \n",
    "    # ====== â‘¢ æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰ ======\n",
    "    # ä¸ºé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œå°†æ‰€æœ‰æ¢¯åº¦é™åˆ¶åœ¨ [-5, 5] èŒƒå›´å†…\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    \n",
    "    # ====== â‘£ å‚æ•°æ›´æ–° ======\n",
    "    # ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æ¨¡å‹å‚æ•°ï¼š\n",
    "    # W := W - learning_rate * dW\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    \n",
    "    # ====== â‘¤ è¿”å›ç»“æœ ======\n",
    "    # è¿”å›æŸå¤±å€¼ã€æ¢¯åº¦å­—å…¸ã€æœ€åä¸€ä¸ªéšè—çŠ¶æ€\n",
    "    return loss, gradients, a[len(X)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 126.50397572165369\n",
      "gradients[\"dWaa\"][1][2] = 0.1947093153471991\n",
      "np.argmax(gradients[\"dWax\"]) = 93\n",
      "gradients[\"dWya\"][1][2] = -0.007773876032003623\n",
      "gradients[\"db\"][4] = [-0.06809825]\n",
      "gradients[\"dby\"][1] = [0.01538192]\n",
      "a_last[4] = [-1.]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# æµ‹è¯• optimize() å‡½æ•°çš„è¿è¡Œæ•ˆæœ\n",
    "# ==============================\n",
    "\n",
    "# å›ºå®šéšæœºæ•°ç§å­ï¼Œä¿è¯æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´ï¼Œä¾¿äºè°ƒè¯•å’Œå¯¹æ¯”\n",
    "np.random.seed(1)\n",
    "\n",
    "# å®šä¹‰è¯æ±‡è¡¨å¤§å°ï¼ˆvocab_size = 27ï¼‰å’Œéšè—å±‚å•å…ƒæ•°é‡ï¼ˆn_a = 100ï¼‰\n",
    "# è¯æ±‡è¡¨å¤§å° = 26ä¸ªè‹±æ–‡å­—æ¯ + '\\n'ï¼ˆæ¢è¡Œç¬¦ï¼‰\n",
    "vocab_size, n_a = 27, 100\n",
    "\n",
    "# åˆå§‹åŒ–ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ a_prevï¼Œshape = (n_a, 1)\n",
    "# è¡¨ç¤ºRNNåœ¨å½“å‰æ ·æœ¬è¾“å…¥å‰çš„å†…éƒ¨è®°å¿†çŠ¶æ€\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "\n",
    "\n",
    "# éšæœºåˆå§‹åŒ–RNNå‚æ•°ï¼š\n",
    "# Wax: è¾“å…¥åˆ°éšè—å±‚çš„æƒé‡çŸ©é˜µï¼Œshape = (n_a, vocab_size)\n",
    "Wax = np.random.randn(n_a, vocab_size)\n",
    "\n",
    "# Waa: éšè—å±‚åˆ°éšè—å±‚çš„æƒé‡çŸ©é˜µï¼Œshape = (n_a, n_a)\n",
    "Waa = np.random.randn(n_a, n_a)\n",
    "\n",
    "# Wya: éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡çŸ©é˜µï¼Œshape = (vocab_size, n_a)\n",
    "Wya = np.random.randn(vocab_size, n_a)\n",
    "\n",
    "# b: éšè—å±‚çš„åç½®é¡¹ï¼Œshape = (n_a, 1)\n",
    "b = np.random.randn(n_a, 1)\n",
    "\n",
    "# by: è¾“å‡ºå±‚çš„åç½®é¡¹ï¼Œshape = (vocab_size, 1)\n",
    "by = np.random.randn(vocab_size, 1)\n",
    "\n",
    "\n",
    "# å°†æ‰€æœ‰å‚æ•°å­˜å…¥ä¸€ä¸ªå­—å…¸ï¼Œæ–¹ä¾¿åœ¨å‡½æ•°ä¸­ç»Ÿä¸€ä¼ é€’\n",
    "parameters = {\n",
    "    \"Wax\": Wax,\n",
    "    \"Waa\": Waa,\n",
    "    \"Wya\": Wya,\n",
    "    \"b\": b,\n",
    "    \"by\": by\n",
    "}\n",
    "\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªæ ·æœ¬çš„è¾“å…¥åºåˆ— X å’Œè¾“å‡ºåºåˆ— Y\n",
    "# X æ˜¯ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼Œæ¯ä¸ªæ•´æ•°ä»£è¡¨ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•\n",
    "X = [12, 3, 5, 11, 22, 3]  # è¾“å…¥å­—ç¬¦ç´¢å¼•åºåˆ—\n",
    "Y = [4, 14, 11, 22, 25, 26]  # è¾“å‡ºç›®æ ‡å­—ç¬¦ç´¢å¼•åºåˆ—ï¼ˆå³Xå‘å·¦å¹³ç§»ä¸€ä½ï¼‰\n",
    "\n",
    "\n",
    "# è°ƒç”¨ optimize() æ‰§è¡Œä¸€æ¬¡å‰å‘ + åå‘ + å‚æ•°æ›´æ–°\n",
    "# learning_rate = 0.01 è¡¨ç¤ºå­¦ä¹ ç‡ä¸º 0.01\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "\n",
    "\n",
    "# è¾“å‡ºå½“å‰æ ·æœ¬çš„æŸå¤±å€¼\n",
    "print(\"Loss =\", loss)\n",
    "\n",
    "\n",
    "# ä»¥ä¸‹è¾“å‡ºç”¨äºæ¢¯åº¦æ£€æŸ¥ï¼ˆéªŒè¯åå‘ä¼ æ’­è®¡ç®—æ˜¯å¦æ­£ç¡®ï¼‰\n",
    "# æ‰“å° Waa æ¢¯åº¦çŸ©é˜µä¸­ç¬¬ [1][2] å…ƒç´ çš„å€¼\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "\n",
    "# æ‰¾å‡º dWax çŸ©é˜µä¸­æ•°å€¼æœ€å¤§çš„å…ƒç´ æ‰€åœ¨ä½ç½®ï¼ˆç”¨äºæ£€æŸ¥æ•°å€¼åˆ†å¸ƒï¼‰\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "\n",
    "# æ‰“å° dWya çŸ©é˜µä¸­ç¬¬ [1][2] å…ƒç´ çš„å€¼\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "\n",
    "# æ‰“å° db å‘é‡çš„ç¬¬ 4 ä¸ªå…ƒç´ ï¼ˆç´¢å¼•ä¸º4ï¼‰\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "\n",
    "# æ‰“å° dby å‘é‡çš„ç¬¬ 1 ä¸ªå…ƒç´ ï¼ˆç´¢å¼•ä¸º1ï¼‰\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "\n",
    "# æ‰“å°æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ a_last çš„ç¬¬ 4 ä¸ªå…ƒç´ ï¼ˆç´¢å¼•ä¸º4ï¼‰\n",
    "print(\"a_last[4] =\", a_last[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "\n",
    "<table>\n",
    "\n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **Loss **\n",
    "    </td>\n",
    "    <td> \n",
    "    126.503975722\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWaa\"][1][2]**\n",
    "    </td>\n",
    "    <td> \n",
    "    0.194709315347\n",
    "    </td>\n",
    "<tr>\n",
    "    <td> \n",
    "    **np.argmax(gradients[\"dWax\"])**\n",
    "    </td>\n",
    "    <td> 93\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dWya\"][1][2]**\n",
    "    </td>\n",
    "    <td> -0.007773876032\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"db\"][4]**\n",
    "    </td>\n",
    "    <td> [-0.06809825]\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **gradients[\"dby\"][1]**\n",
    "    </td>\n",
    "    <td>[ 0.01538192]\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **a_last[4]**\n",
    "    </td>\n",
    "    <td> [-1.]\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3.2 - æ¨¡å‹è®­ç»ƒï¼ˆTraining the modelï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»™å®šæé¾™åå­—æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†æ•°æ®é›†çš„æ¯ä¸€è¡Œï¼ˆä¸€ä¸ªåå­—ï¼‰ä½œä¸ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬ã€‚  \n",
    "åœ¨éšæœºæ¢¯åº¦ä¸‹é™çš„æ¯ 100 æ­¥ï¼Œä½ å°†éšæœºé‡‡æ · 10 ä¸ªåå­—æ¥æŸ¥çœ‹ç®—æ³•æ•ˆæœã€‚  \n",
    "è®°å¾—æ‰“ä¹±æ•°æ®é›†é¡ºåºï¼Œä½¿éšæœºæ¢¯åº¦ä¸‹é™å¯ä»¥ä»¥éšæœºé¡ºåºè®¿é—®æ ·æœ¬ã€‚\n",
    "\n",
    "**ç»ƒä¹ **ï¼šæŒ‰ç…§è¯´æ˜å®ç° `model()` å‡½æ•°ã€‚  \n",
    "å½“ `examples[index]` åŒ…å«ä¸€ä¸ªæé¾™åå­—ï¼ˆå­—ç¬¦ä¸²ï¼‰æ—¶ï¼Œä½ å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼åˆ›å»ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬ `(X, Y)`ï¼š\n",
    "\n",
    "```python\n",
    "index = j % len(examples)\n",
    "X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "\n",
    "```\n",
    "æ³¨æ„è¿™é‡Œä½¿ç”¨ï¼š`index = j % len(examples)`ï¼Œå…¶ä¸­ `j = 1....num_iterations`ï¼Œ  \n",
    "ç¡®ä¿ `examples[index]` æ€»æ˜¯æœ‰æ•ˆï¼ˆ`index` å°äº `len(examples)`ï¼‰ã€‚  \n",
    "\n",
    "`X` çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä¸º `None`ï¼Œåœ¨ `rnn_forward()` ä¸­ä¼šè¢«è§£é‡Šä¸º $x^{\\langle 0 \\rangle} = \\vec{0}$ã€‚  \n",
    "æ­¤å¤–ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ `Y` ä¸ `X` ç›¸åŒï¼Œä½†æ•´ä½“å·¦ç§»ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå¹¶åœ¨æœ«å°¾æ·»åŠ  `\"\\n\"`ï¼Œè¡¨ç¤ºæé¾™åå­—ç»“æŸã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# è®­ç»ƒæé¾™åå­—ç”Ÿæˆæ¨¡å‹çš„ä¸»å‡½æ•°\n",
    "# ==============================\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    è®­ç»ƒæ¨¡å‹å¹¶ç”Ÿæˆæé¾™åå­—ã€‚\n",
    "    \n",
    "    å‚æ•°è¯´æ˜:\n",
    "    data -- æ–‡æœ¬è¯­æ–™åº“ï¼ˆæ‰€æœ‰æé¾™åå­—ï¼‰\n",
    "    ix_to_char -- ä»ç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„å­—å…¸\n",
    "    char_to_ix -- ä»å­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸\n",
    "    num_iterations -- è®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤35000ï¼‰\n",
    "    n_a -- RNNéšè—å±‚ç¥ç»å…ƒæ•°é‡ï¼ˆé»˜è®¤50ï¼‰\n",
    "    dino_names -- æ¯2000æ¬¡è¿­ä»£åé‡‡æ ·ç”Ÿæˆçš„åå­—æ•°é‡\n",
    "    vocab_size -- è¯æ±‡è¡¨å¤§å°ï¼ˆå³å”¯ä¸€å­—ç¬¦æ•°ï¼‰\n",
    "    \n",
    "    è¿”å›å€¼:\n",
    "    parameters -- è®­ç»ƒå®Œæˆåçš„å‚æ•°å­—å…¸ï¼ˆWaaã€Waxã€Wyaã€bã€byï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    # è¾“å…¥å’Œè¾“å‡ºå±‚çš„å¤§å°å‡ç­‰äºè¯æ±‡è¡¨å¤§å°\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # ---------- 1. åˆå§‹åŒ–å‚æ•° ----------\n",
    "    # Waa, Wax, Wya, b, by\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # ---------- 2. åˆå§‹åŒ–å¹³æ»‘æŸå¤± ----------\n",
    "    # ç›®çš„ï¼šè®©æŸå¤±å€¼éšæ—¶é—´å¹³æ»‘å˜åŒ–ï¼Œé¿å…å‰§çƒˆæ³¢åŠ¨\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # ---------- 3. è¯»å–å¹¶é¢„å¤„ç†è®­ç»ƒæ•°æ® ----------\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()  # æ¯è¡Œä¸€ä¸ªæé¾™åå­—\n",
    "    # è½¬å°å†™å¹¶å»æ‰æ¢è¡Œç¬¦\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # éšæœºæ‰“ä¹±è®­ç»ƒæ ·æœ¬é¡ºåºï¼Œé¿å…æ¨¡å‹è®°ä½é¡ºåºæ¨¡å¼\n",
    "    shuffle(examples)\n",
    "    \n",
    "    # ---------- 4. åˆå§‹åŒ–éšè—çŠ¶æ€ ----------\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # ---------- 5. è®­ç»ƒä¸»å¾ªç¯ ----------\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        # é€‰æ‹©å½“å‰è®­ç»ƒæ ·æœ¬ï¼ˆå¾ªç¯ä½¿ç”¨ï¼‰\n",
    "        index = j % len(examples)\n",
    "        # è¾“å…¥åºåˆ— Xï¼ŒåŠ ä¸Š None ä½œä¸ºèµ·å§‹æ ‡è®°\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]]\n",
    "        # è¾“å‡ºåºåˆ— Y = X å‘å·¦å¹³ç§»ä¸€ä½ + æ¢è¡Œç¬¦ç»“å°¾\n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        # ---------- 6. æ‰§è¡Œå•æ­¥ä¼˜åŒ– ----------\n",
    "        # å‰å‘ä¼ æ’­ -> åå‘ä¼ æ’­ -> æ¢¯åº¦ä¿®å‰ª -> å‚æ•°æ›´æ–°\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        # ---------- 7. å¹³æ»‘æŸå¤± ----------\n",
    "        # é˜²æ­¢æŸå¤±æ›²çº¿è¿‡äºå‰§çƒˆæ³¢åŠ¨\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # ---------- 8. æ¯2000æ­¥æ‰“å°ä¸€æ¬¡æ ·æœ¬ ----------\n",
    "        if j % 2000 == 0:\n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                # è°ƒç”¨sample()å‡½æ•°æ ¹æ®å½“å‰æ¨¡å‹ç”Ÿæˆæ–°åå­—\n",
    "                sampled_indexes = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indexes, ix_to_char)\n",
    "                seed += 1\n",
    "            \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿è¡Œä¸‹ä¸€ä¸ªä»£ç å•å…ƒï¼Œä½ ä¼šè§‚å¯Ÿåˆ°æ¨¡å‹åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶è¾“å‡ºéšæœºå­—ç¬¦ã€‚  \n",
    "ç»è¿‡å‡ åƒæ¬¡è¿­ä»£åï¼Œæ¨¡å‹åº”è¯¥èƒ½å¤Ÿç”Ÿæˆçœ‹èµ·æ¥æ¯”è¾ƒåˆç†çš„æé¾™åå­—ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.097228\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjjjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjjjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjjjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjjjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 28.039557\n",
      "\n",
      "Mivusalicoravhosocpineshansavieanaltailapgairatolo\n",
      "Inca\n",
      "Iwtos\n",
      "Macaislhaclppthanwjedycerta\n",
      "Xusbbidoravgoslariontoanthylganaltailapgairatolori\n",
      "Ca\n",
      "Tos\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.780069\n",
      "\n",
      "Ngyusleomoraxatruf\n",
      "Ingbakptia\n",
      "Kytrojicholyiros\n",
      "Ngaagosaurus\n",
      "Xusndomosaurus\n",
      "Da\n",
      "Toraonos\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.739194\n",
      "\n",
      "Nhytrodom\n",
      "Klecalptia\n",
      "Lyuspendor\n",
      "Ngabersaurus\n",
      "Xusrbilosaurus\n",
      "Ee\n",
      "Trodolosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.050955\n",
      "\n",
      "Onxxtonnaseluptotasaurus\n",
      "Llecalosaurus\n",
      "Lyvspendopevdrotarbintochurynganenuomechiamofus\n",
      "Olbaltopadrus\n",
      "Xutolonosaurus\n",
      "Eeakptia\n",
      "Trohmeosaurus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.646209\n",
      "\n",
      "Onxusichisaurus\n",
      "Jlacanosaurus\n",
      "Kxusodon\n",
      "Olaagrosaurus\n",
      "Yusraonosaurus\n",
      "Edalosaurus\n",
      "Trodonisaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.290685\n",
      "\n",
      "Onyusodon\n",
      "Licabasphacitesaurus\n",
      "Lyuspendosaurus\n",
      "Ohaaesticaptosaurus\n",
      "Yuspendosaurus\n",
      "Edalosaurus\n",
      "Trodongopunosoconchus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.221103\n",
      "\n",
      "Pixsrolia\n",
      "Miedanosaurus\n",
      "Myusononosaurus\n",
      "Pecaisimachus\n",
      "Yusolonkonus\n",
      "Gaagosaurus\n",
      "Troingosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.009373\n",
      "\n",
      "Onyuskangricterosaurus\n",
      "Kicabasaurus\n",
      "Lvotophesalrosaurus\n",
      "Olabarrekarus\n",
      "Yusolomoravisaurus\n",
      "Edalosaurus\n",
      "Trodonosaurus\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.943757\n",
      "\n",
      "Onyxusaurus\n",
      "Jidabestelasthuhenus\n",
      "Kyusteoosaurus\n",
      "Okaberria\n",
      "Yuspendosaurus\n",
      "Edakosaurus\n",
      "Tromelosaurus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.781539\n",
      "\n",
      "Onyxtondes\n",
      "Kikacerotegiunosaurus\n",
      "Lustridinosterotarhhosaorusthiangosaurus\n",
      "Onaaislgacosaurus\n",
      "Yussanosaurus\n",
      "Eiahosaurus\n",
      "Trohimosaurus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.824504\n",
      "\n",
      "Miwtosaurus\n",
      "Jidaakosaurus\n",
      "Kusqtepanonus\n",
      "Mbabarrag\n",
      "Yurodon\n",
      "Dabarpaeantesaurus\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 22.570809\n",
      "\n",
      "Mexusochishaurosaurus\n",
      "Inecalosaurus\n",
      "Iustrephorcurosaurus\n",
      "Mecaerosaurus\n",
      "Yustholosaurus\n",
      "Dabcosaurus\n",
      "Trocheosaurus\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 22.581994\n",
      "\n",
      "Mivusibhiseiterotaus\n",
      "Ingabersaurus\n",
      "Jyuurisaurus\n",
      "Mdabers\n",
      "Yuuphomosaurus\n",
      "Eiagrus\n",
      "Trpengosaurus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.475691\n",
      "\n",
      "Mewurogia\n",
      "Jibaadron\n",
      "Kwrosaurus\n",
      "Maabarola\n",
      "Yuppangidau\n",
      "Da\n",
      "Trocenator\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.537106\n",
      "\n",
      "Ngxushangorex\n",
      "Kicacernia\n",
      "Lvoshangorhylus\n",
      "Necanosaurus\n",
      "Yurodongolus\n",
      "Eialosaurus\n",
      "Trodondogtanosaurus\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 22.277907\n",
      "\n",
      "Nivptomia\n",
      "Kika\n",
      "Lytrodon\n",
      "Necanosaurus\n",
      "Yusolodon\n",
      "Elagosaurus\n",
      "Vorchenontesaurus\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 22.372706\n",
      "\n",
      "Mbyusjamamaiugrlpania\n",
      "Keia\n",
      "Kustrinasaurus\n",
      "Macakosaurus\n",
      "Yuroclenmavasaurus\n",
      "Edaeosaurus\n",
      "Uslanchiateritasaurus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## ç»“è®º\n",
    "\n",
    "ä½ å¯ä»¥çœ‹åˆ°ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œä½ çš„ç®—æ³•å¼€å§‹ç”Ÿæˆçœ‹èµ·æ¥åˆç†çš„æé¾™åå­—ã€‚  \n",
    "ä¸€å¼€å§‹ï¼Œå®ƒç”Ÿæˆçš„æ˜¯éšæœºå­—ç¬¦ï¼Œä½†åˆ°è®­ç»ƒç»“æŸæ—¶ï¼Œä½ å¯ä»¥çœ‹åˆ°ä¸€äº›æœ‰è¶£çš„æé¾™åå­—ç»“å°¾ã€‚  \n",
    "ä½ å¯ä»¥å°è¯•è¿è¡Œæ›´å¤šè¿­ä»£ï¼Œå¹¶è°ƒæ•´è¶…å‚æ•°ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½å¾—åˆ°æ›´å¥½çš„ç»“æœã€‚  \n",
    "æˆ‘ä»¬çš„å®ç°ç”Ÿæˆäº†ä¸€äº›éå¸¸é…·çš„åå­—ï¼Œå¦‚ `maconucon`ã€`marloralus` å’Œ `macingsersaurus`ã€‚  \n",
    "ä½ çš„æ¨¡å‹ä¹Ÿå¾ˆå¯èƒ½å­¦ä¼šäº†æé¾™åå­—é€šå¸¸ä»¥ `saurus`ã€`don`ã€`aura`ã€`tor` ç­‰ç»“å°¾ã€‚\n",
    "\n",
    "å¦‚æœä½ çš„æ¨¡å‹ç”Ÿæˆäº†ä¸€äº›ä¸å¤ªé…·çš„åå­—ï¼Œä¹Ÿä¸è¦å®Œå…¨è´£æ€ªæ¨¡å‹â€”â€”å¹¶ä¸æ˜¯æ‰€æœ‰çœŸå®çš„æé¾™åå­—éƒ½å¬èµ·æ¥å¾ˆé…·ã€‚ï¼ˆä¾‹å¦‚ï¼Œ`dromaeosauroides` å°±æ˜¯çœŸå®çš„æé¾™åå­—ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒé›†ä¸­ã€‚ï¼‰  \n",
    "è¿™ä¸ªæ¨¡å‹è‡³å°‘å¯ä»¥æä¾›ä¸€ç»„å€™é€‰åå­—ï¼Œä»ä¸­æŒ‘é€‰æœ€é…·çš„ï¼\n",
    "\n",
    "è¿™ä¸ªä½œä¸šä½¿ç”¨çš„æ•°æ®é›†ç›¸å¯¹è¾ƒå°ï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ CPU ä¸Šå¿«é€Ÿè®­ç»ƒ RNNã€‚  \n",
    "è®­ç»ƒè‹±æ–‡è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦æ›´å¤§çš„æ•°æ®é›†å’Œæ›´å¤šçš„è®¡ç®—èµ„æºï¼Œå¹¶ä¸”å¯èƒ½éœ€è¦åœ¨ GPU ä¸Šè¿è¡Œæ•°å°æ—¶ã€‚  \n",
    "æˆ‘ä»¬å¯¹æé¾™åå­—æ¨¡å‹è®­ç»ƒäº†ä¸€æ®µæ—¶é—´ï¼Œåˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬æœ€å–œæ¬¢çš„åå­—æ˜¯ï¼šå¼ºå¤§ã€æ— æ•Œä¸”å‡¶çŒ›çš„ Mangosaurusï¼\n",
    "\n",
    "<img src=\"images/mangosaurus.jpeg\" style=\"width:250;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - åƒèå£«æ¯”äºšä¸€æ ·å†™ä½œ\n",
    "\n",
    "\n",
    "ä¸€ä¸ªç±»ä¼¼çš„ï¼ˆä½†æ›´å¤æ‚ï¼‰ä»»åŠ¡æ˜¯ç”Ÿæˆèå£«æ¯”äºšé£æ ¼çš„è¯—æ­Œã€‚  \n",
    "ä¸æé¾™åå­—ä¸åŒï¼Œä½ å¯ä»¥ä½¿ç”¨èå£«æ¯”äºšè¯—æ­Œé›†åˆä½œä¸ºæ•°æ®é›†ã€‚  \n",
    "åˆ©ç”¨ LSTM å•å…ƒï¼Œä½ å¯ä»¥å­¦ä¹ è·¨è¶Šæ–‡æœ¬ä¸­è®¸å¤šå­—ç¬¦çš„é•¿æœŸä¾èµ–å…³ç³»â€”â€”ä¾‹å¦‚ï¼Œåºåˆ—ä¸­æŸä¸ªä½ç½®å‡ºç°çš„å­—ç¬¦å¯ä»¥å½±å“å¾ˆä¹…ä¹‹ååº”è¯¥å‡ºç°çš„å¦ä¸€ä¸ªå­—ç¬¦ã€‚  \n",
    "è¿™äº›é•¿æœŸä¾èµ–åœ¨æé¾™åå­—ä¸­ä¸å¤ªé‡è¦ï¼Œå› ä¸ºåå­—è¾ƒçŸ­ã€‚\n",
    "\n",
    "<img src=\"images/shakespeare.jpg\" style=\"width:500;height:400px;\">\n",
    "<caption><center> è®©æˆ‘ä»¬æˆä¸ºè¯—äººå§ï¼ </center></caption>\n",
    "\n",
    "æˆ‘ä»¬å·²ç»ä½¿ç”¨Pytorchå®ç°äº†ä¸€ä¸ªèå£«æ¯”äºšè¯—æ­Œç”Ÿæˆå™¨ã€‚  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯èå£«æ¯”äºšè¯—æ­Œé›†ï¼ˆåä¸º [*\"The Sonnets\"*](shakespeare.txt)ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½èå£«æ¯”äºšæ–‡æœ¬ä¸­...\n",
      "å­—ç¬¦æ€»æ•°: 38\n",
      "åˆ›å»ºè®­ç»ƒé›†...\n",
      "è®­ç»ƒæ ·æœ¬æ•°é‡: 31412\n",
      "å‘é‡åŒ–è®­ç»ƒé›†...\n",
      "å¼€å§‹è®­ç»ƒ...\n",
      "Epoch [1/30] Loss: 2.973895\n",
      "Epoch [2/30] Loss: 2.532324\n",
      "Epoch [3/30] Loss: 2.324776\n",
      "Epoch [4/30] Loss: 2.215475\n",
      "Epoch [5/30] Loss: 2.133931\n",
      "ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ã€‚\n",
      "\n",
      "ğŸŒ¹ ç”Ÿæˆçš„èå£«æ¯”äºšè¯—æ­Œï¼š\n",
      "\n",
      "a beautiful girl ldedsesaau sec n,nnopty cere paatule it the tay pfirgi l\n",
      "cure,\n",
      "whyt hon my whime batheerandif.\n",
      "\n",
      "oref sorerot uy  alloth wiriln yaven the as mer\n",
      "wht.\n",
      "\n",
      "\n",
      "ou holss angeine pow,\n",
      "tha ku dallog tor fritht an\n",
      "\n",
      "\n",
      "Epoch [6/30] Loss: 2.055479\n",
      "Epoch [7/30] Loss: 1.990296\n",
      "Epoch [8/30] Loss: 1.927208\n",
      "Epoch [9/30] Loss: 1.872323\n",
      "Epoch [10/30] Loss: 1.819941\n",
      "ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ã€‚\n",
      "\n",
      "ğŸŒ¹ ç”Ÿæˆçš„èå£«æ¯”äºšè¯—æ­Œï¼š\n",
      "\n",
      "a beautiful girl reapuderetnuo,diydbi\n",
      "licinedabe,\n",
      "tan unhis) chine our will dore resteint.\n",
      "\n",
      "\n",
      "to\n",
      "thin for the or ails aul frimmisu nove's,\n",
      "or sealiofino:\n",
      "ore bring i aurstoulling:\n",
      "oud ingon my sllfis) but my tinten eye\n",
      "\n",
      "\n",
      "Epoch [11/30] Loss: 1.761615\n",
      "Epoch [12/30] Loss: 1.702048\n",
      "Epoch [13/30] Loss: 1.638866\n",
      "Epoch [14/30] Loss: 1.563454\n",
      "Epoch [15/30] Loss: 1.482293\n",
      "ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ã€‚\n",
      "\n",
      "ğŸŒ¹ ç”Ÿæˆçš„èå£«æ¯”äºšè¯—æ­Œï¼š\n",
      "\n",
      "a beautiful girl aendesm\n",
      "ss-ocneneiplrwangely churser paid tfeire,\n",
      "whene aithe indesscent aid for gratn:\n",
      "beanine fan steir busterick-apstoon more:\n",
      "the hand your s our hos with de aurt by,\n",
      "bun whin of manse ive mone, s\n",
      "\n",
      "\n",
      "Epoch [16/30] Loss: 1.384519\n",
      "Epoch [17/30] Loss: 1.277916\n",
      "Epoch [18/30] Loss: 1.155510\n",
      "Epoch [19/30] Loss: 1.019365\n",
      "Epoch [20/30] Loss: 0.886456\n",
      "ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ã€‚\n",
      "\n",
      "ğŸŒ¹ ç”Ÿæˆçš„èå£«æ¯”äºšè¯—æ­Œï¼š\n",
      "\n",
      "a beautiful girl ynegc,t,eyteey nnlnreis,\n",
      "and lowe by nored me mendean bnow, not rens;\n",
      "then in thes fore not in then afbe no grzenk.\n",
      "to my swaet isthence in him thun pexguck\n",
      "\n",
      "wering thy sly being lanss be on your swie\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# ğŸŒ¹ èå£«æ¯”äºšæ–‡æœ¬ç”Ÿæˆå™¨ï¼ˆPyTorchï¼‰\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ===============================\n",
    "# ç¬¬1æ­¥ï¼šè¯»å–èå£«æ¯”äºšæ–‡æœ¬æ•°æ®\n",
    "# ===============================\n",
    "print(\"åŠ è½½èå£«æ¯”äºšæ–‡æœ¬ä¸­...\")\n",
    "\n",
    "# ä½ å¯ä»¥æ›¿æ¢æˆä»»ä½•è‹±æ–‡è¯­æ–™æ–‡ä»¶ï¼Œä¾‹å¦‚ \"shakespeare.txt\"\n",
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "# å–å‡ºæ–‡æœ¬ä¸­æ‰€æœ‰çš„å”¯ä¸€å­—ç¬¦\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# åˆ›å»ºå­—ç¬¦ä¸ç´¢å¼•çš„åŒå‘æ˜ å°„å­—å…¸\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"å­—ç¬¦æ€»æ•°: {vocab_size}\")\n",
    "\n",
    "# ===============================\n",
    "# ç¬¬2æ­¥ï¼šåˆ›å»ºè®­ç»ƒæ•°æ®é›†\n",
    "# ===============================\n",
    "print(\"åˆ›å»ºè®­ç»ƒé›†...\")\n",
    "\n",
    "Tx = 40  # åºåˆ—é•¿åº¦ï¼ˆæ¯ä¸ªæ ·æœ¬çš„è¾“å…¥å­—ç¬¦æ•°ï¼‰\n",
    "stride = 3  # æ¯éš”å¤šå°‘å­—ç¬¦é‡‡æ ·ä¸€æ¬¡\n",
    "X, Y = [], []\n",
    "\n",
    "# ä»æ–‡æœ¬ä¸­æå–è¾“å…¥åºåˆ— X å’Œä¸‹ä¸€ä¸ªå­—ç¬¦ Y\n",
    "for i in range(0, len(text) - Tx, stride):\n",
    "    X.append([char_to_ix[ch] for ch in text[i: i + Tx]])\n",
    "    Y.append(char_to_ix[text[i + Tx]])\n",
    "\n",
    "print(f\"è®­ç»ƒæ ·æœ¬æ•°é‡: {len(X)}\")\n",
    "\n",
    "# ===============================\n",
    "# ç¬¬3æ­¥ï¼šå‘é‡åŒ–è®­ç»ƒé›†\n",
    "# ===============================\n",
    "print(\"å‘é‡åŒ–è®­ç»ƒé›†...\")\n",
    "\n",
    "X_oh = np.zeros((len(X), Tx, vocab_size), dtype=np.float32)\n",
    "Y_oh = np.zeros((len(X), vocab_size), dtype=np.float32)\n",
    "\n",
    "for i, seq in enumerate(X):\n",
    "    for t, ch_idx in enumerate(seq):\n",
    "        X_oh[i, t, ch_idx] = 1\n",
    "    Y_oh[i, Y[i]] = 1\n",
    "\n",
    "# è½¬æ¢ä¸º PyTorch å¼ é‡\n",
    "X_tensor = torch.tensor(X_oh)\n",
    "Y_tensor = torch.tensor(Y_oh)\n",
    "\n",
    "# åˆ›å»º DataLoader\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# ===============================\n",
    "# ç¬¬4æ­¥ï¼šå®šä¹‰æ¨¡å‹\n",
    "# ===============================\n",
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, n_x, n_a=512):\n",
    "        super(ShakespeareModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_x, hidden_size=n_a, batch_first=True)\n",
    "        self.fc = nn.Linear(n_a, n_x)\n",
    "        # âš ï¸ ä¸æ·»åŠ  softmaxï¼Œè¿™æ · CrossEntropyLoss æ‰èƒ½æ­£ç¡®è®¡ç®—\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # å–åºåˆ—æœ€åä¸€æ­¥è¾“å‡º\n",
    "        return out  # è¾“å‡º logitsï¼ˆæœªå½’ä¸€åŒ–ï¼‰\n",
    "\n",
    "# ===============================\n",
    "# ç¬¬5æ­¥ï¼šè®­ç»ƒè®¾ç½®\n",
    "# ===============================\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "\n",
    "model = ShakespeareModel(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # è‡ªåŠ¨åŒ…å« softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "num_epochs = 30  # è®­ç»ƒè½®æ•°ï¼Œå¯è°ƒå¤§ä»¥æå‡ç”Ÿæˆè´¨é‡\n",
    "\n",
    "# ===============================\n",
    "# ç¬¬6æ­¥ï¼šè®­ç»ƒå¾ªç¯\n",
    "# ===============================\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Loss: {total_loss / len(loader):.6f}\")\n",
    "\n",
    "    # æ¯5è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹å¹¶ç”Ÿæˆè¯—æ­Œ\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), \"shakespeare_model.pth\")\n",
    "        print(\"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ã€‚\")\n",
    "\n",
    "        # ===============================\n",
    "        # ç¬¬7æ­¥ï¼šæ–‡æœ¬ç”Ÿæˆå‡½æ•°\n",
    "        # ===============================\n",
    "        def generate_text(seed, length=200):\n",
    "            generated = seed\n",
    "            for _ in range(length):\n",
    "                # æ„å»ºè¾“å…¥\n",
    "                x_pred = np.zeros((1, Tx, vocab_size))\n",
    "                for t, ch in enumerate(seed[-Tx:]):\n",
    "                    if ch in char_to_ix:\n",
    "                        x_pred[0, t, char_to_ix[ch]] = 1.\n",
    "\n",
    "                x_pred = torch.tensor(x_pred, dtype=torch.float32)\n",
    "\n",
    "                # é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦\n",
    "                with torch.no_grad():\n",
    "                    preds = torch.softmax(model(x_pred), dim=-1).cpu().numpy().ravel()\n",
    "\n",
    "                # æ ¹æ®é¢„æµ‹æ¦‚ç‡é‡‡æ ·\n",
    "                next_idx = np.random.choice(range(vocab_size), p=preds)\n",
    "                next_char = ix_to_char[next_idx]\n",
    "\n",
    "                generated += next_char\n",
    "                seed += next_char\n",
    "            return generated\n",
    "\n",
    "        # ç”Ÿæˆä¸€é¦–è¯—\n",
    "        print(\"\\nğŸŒ¹ ç”Ÿæˆçš„èå£«æ¯”äºšè¯—æ­Œï¼š\\n\")\n",
    "        print(generate_text(\"a beautiful girl \"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"è®­ç»ƒå®Œæˆ âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-èå£«æ¯”äºšæ¨¡å‹ä¸ä¹‹å‰ä¸ºæé¾™åå­—æ„å»ºçš„æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚å”¯ä¸€çš„ä¸»è¦åŒºåˆ«æœ‰ï¼š\n",
    "\n",
    "- ä½¿ç”¨ LSTM è€Œä¸æ˜¯åŸºç¡€ RNNï¼Œä»¥æ•æ‰æ›´é•¿çš„ä¾èµ–å…³ç³»  \n",
    "- æ¨¡å‹æ›´æ·±ï¼Œé‡‡ç”¨å †å çš„ LSTMï¼ˆ2 å±‚ï¼‰  \n",
    "- ä½¿ç”¨ Keras è€Œä¸æ˜¯çº¯ Pythonï¼Œä»¥ç®€åŒ–ä»£ç   \n",
    "\n",
    "å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œä¹Ÿå¯ä»¥æŸ¥çœ‹ Keras å›¢é˜Ÿåœ¨ GitHub ä¸Šçš„æ–‡æœ¬ç”Ÿæˆå®ç°ï¼š[https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py)ã€‚\n",
    "\n",
    "æ­å–œä½ å®Œæˆäº†è¿™ä¸ª Notebookï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å‚è€ƒèµ„æ–™**ï¼š\n",
    "\n",
    "- æœ¬ç»ƒä¹ å€Ÿé‰´äº† Andrej Karpathy çš„å®ç°ï¼š[https://gist.github.com/karpathy/d4dee566867f8291f086](https://gist.github.com/karpathy/d4dee566867f8291f086)ã€‚æƒ³äº†è§£æ›´å¤šå…³äºæ–‡æœ¬ç”Ÿæˆçš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŸ¥çœ‹ Karpathy çš„[åšå®¢æ–‡ç« ](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)ã€‚  \n",
    "- å…³äºèå£«æ¯”äºšè¯—æ­Œç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬çš„å®ç°åŸºäº Keras å›¢é˜Ÿçš„ LSTM æ–‡æœ¬ç”Ÿæˆå™¨å®ç°ï¼š[https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
