{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python基础与Numpy\n",
    "\n",
    "本节将通过一些实用练习，带你快速熟悉Python的基本功能以及numpy库的核心功能。即使你之前有一定的Python经验，本章内容也将帮助你夯实基础，掌握必要的工具，为后续学习做好准备。\n",
    "\n",
    "**通过学习本节内容，您将能够：**\n",
    "\n",
    "- 使用numpy的函数和操作进行矩阵与向量计算  \n",
    "- 理解并应用“广播”概念  \n",
    "- 实现代码的向量化以提升效率  \n",
    "- 使用numpy构建常用的基础函数  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 使用numpy构建基础函数 ##\r\n",
    "\r\n",
    "Numpy是Python中进行科学计算的主要库，由一个庞大的社区维护（www.numpy.org 本节中，你将学习几个关键的numpy函数，如 `np.exp`、`np.log` 和 `np.reshap函数。\r\n",
    "\r\n",
    "### 1.1 - sigmoid函数，np.exp() ###\r\n",
    "\r\n",
    "在使用 `np.exp()` 之前，你将先使用 `math.exp()` 来实现sigmoid函数。然后你将了解为什么 `np.exp()` 优于 `math.exp()`。\r\n",
    "\r\n",
    "**练习**：构建一个函数，返回实数 `x` 的sigmoid值。请使用 `math.exp(x)` 来计算指数函数。\r\n",
    "\r\n",
    "**提示**：  \r\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ 有时也被称为逻辑函数（logistic function）。这是一种非线性函数，不仅在机器学习（如逻辑回归）中使用，也常用于深度学习中。\r\n",
    "\r\n",
    "要调用某个特定库中的函数，你可以使用 `库名.函数名()` 的方式。例如 `math.exp()`，请运行下面的代码来查看示例。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入Python标准库中的math模块，用于调用数学函数，如指数函数exp\n",
    "import math\n",
    "\n",
    "# 定义一个名为 basic_sigmoid 的函数，接受一个参数 x（实数标量）\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    计算 x 的 sigmoid 函数值\n",
    "\n",
    "    参数:\n",
    "    x -- 一个标量（实数）\n",
    "\n",
    "    返回:\n",
    "    s -- sigmoid(x) 的计算结果\n",
    "    \"\"\"\n",
    "\n",
    "    # 计算 sigmoid 函数的表达式：1 / (1 + e^(-x))\n",
    "    # 使用 math.exp(-x) 计算 e 的 -x 次方\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    # 返回计算得到的 sigmoid 值\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，在深度学习中我们很少使用 \"math\" 库，因为它的函数输入通常是实数。而深度学习中我们主要处理的是矩阵和向量，这也是为什么 numpy 更加有用的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 深度学习中我们通常使用 numpy 而不是 math，这是其中一个原因示例\u001b[39;00m\n\u001b[0;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# 定义变量 x，为一个列表（向量），包含三个元素 [1, 2, 3]\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m basic_sigmoid(x)\n",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m, in \u001b[0;36mbasic_sigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m计算 x 的 sigmoid 函数值\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03ms -- sigmoid(x) 的计算结果\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 计算 sigmoid 函数的表达式：1 / (1 + e^(-x))\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 使用 math.exp(-x) 计算 e 的 -x 次方\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mx))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 返回计算得到的 sigmoid 值\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "# 深度学习中我们通常使用 numpy 而不是 math，这是其中一个原因示例\n",
    "\n",
    "x = [1, 2, 3]  # 定义变量 x，为一个列表（向量），包含三个元素 [1, 2, 3]\n",
    "\n",
    "basic_sigmoid(x)  \n",
    "# 调用之前定义的 basic_sigmoid 函数，传入向量 x\n",
    "# 由于 basic_sigmoid 函数内部使用 math.exp(-x)，而 math.exp 只接受标量（单个数值）作为参数\n",
    "# 所以这里会报错：TypeError: a float is required\n",
    "# 错误原因是 math.exp 无法对列表类型进行逐元素计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，如果 $x = (x_1, x_2, ..., x_n)$ 是一个行向量，那么 $np.exp(x)$ 会对 x 的每个元素应用指数函数。  \r",
    "因此，输出将是：$np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "# 导入 numpy 库，并使用简写别名 np，方便后续调用\n",
    "\n",
    "# 示例：使用 np.exp 进行指数运算\n",
    "x = np.array([1, 2, 3])  \n",
    "# 定义变量 x，是一个 numpy 数组，包含三个元素 [1, 2, 3]\n",
    "# np.array() 将 Python 列表转为 numpy 的数组对象，可以进行向量化操作\n",
    "\n",
    "print(np.exp(x))  \n",
    "# 调用 np.exp(x)，对数组 x 中的每个元素分别执行指数运算（以 e 为底）\n",
    "# 即计算结果为 [e^1, e^2, e^3]\n",
    "# 输出结果大约为 [ 2.71828183  7.3890561  20.08553692 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，如果 x 是一个向量，那么类似 $s = x + 3$ 或 $s = \\frac{1}{x}$ 这样的 Python 运算将会输出一个与 x 大小相同的向量 s。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# 向量运算示例\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "# 定义变量 x，为一个 numpy 数组，包含三个元素 [1, 2, 3]\n",
    "# numpy 数组支持向量化操作，可以对数组中的每个元素执行相同运算\n",
    "\n",
    "print(x + 3)\n",
    "# 对 numpy 数组 x 的每个元素加上标量 3，形成一个新的数组\n",
    "# 结果为 [4, 5, 6]，每个元素对应原数组元素加 3\n",
    "# print() 函数用于将结果输出到控制台"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每当你需要了解更多关于 numpy 函数的信息时，建议查阅[官方文档](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html)。\n",
    "\n",
    "你也可以在 notebook 中新建一个单元格，输入例如 `np.exp?` 来快速查看该函数的文档说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：使用 numpy 实现 sigmoid 函数。\n",
    "\n",
    "**说明**：  \n",
    "变量 x 现在可以是一个实数、一个向量，或者一个矩阵。  \n",
    "在 numpy 中，我们用 numpy 数组（numpy arrays）来表示这些数据结构（向量、矩阵等）。目前你不需要了解更多细节。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "# 导入 numpy 库，并简写为 np，方便后续调用 numpy 函数，如 np.exp()\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    计算输入 x 的 sigmoid 函数值\n",
    "\n",
    "    参数:\n",
    "    x -- 标量（单个数值）或任意大小的 numpy 数组\n",
    "\n",
    "    返回:\n",
    "    s -- 输入 x 的 sigmoid 计算结果，类型与 x 相同（标量或数组）\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1 / (1 + np.exp(-x))  \n",
    "    # 计算 sigmoid 函数：\n",
    "    # 先计算 -x 的指数 np.exp(-x)，\n",
    "    # 对于标量，计算单个值；对于数组，逐元素计算\n",
    "    # 然后用 1 除以 (1 + 指数值) ，实现 sigmoid 的公式：1 / (1 + e^(-x))\n",
    "    \n",
    "    return s  \n",
    "    # 返回计算得到的 sigmoid 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid 梯度\r\n",
    "\r\n",
    "正如你在课程中看到的，为了使用反向传播优化损失函数，你需要计算梯度。我们你的第一个梯度函数。\r\n",
    "\r\n",
    "**练习**：实现函数 `sigmoid_grad()`，用于计算 sigmoid 函数关于输入 x 的梯度。公式如下：  \r\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\r\n",
    "\r\n",
    "通常你可以分两步来编写这个函数：  \r\n",
    "1. 令 s 为 x 的 sigmoid 值。你可以使用之前实现的 `sigmoid(x)` 函数。  \r\n",
    "2. 计算 $\\sigma'(x) = s(1 - s)$。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    计算 sigmoid 函数关于输入 x 的梯度（即斜率或导数）。\n",
    "    你可以先计算 sigmoid 函数的输出，存储在变量中，再用它计算梯度。\n",
    "    \n",
    "    参数:\n",
    "    x -- 标量或 numpy 数组，输入值\n",
    "    \n",
    "    返回:\n",
    "    ds -- 计算得到的梯度值，类型与 x 相同\n",
    "    \"\"\"\n",
    "    \n",
    "    s = sigmoid(x)  \n",
    "    # 调用之前实现的 sigmoid 函数，计算输入 x 的 sigmoid 值，结果存入 s\n",
    "    \n",
    "    ds = s * (1 - s)  \n",
    "    # 根据公式 sigmoid 导数 = sigmoid(x) * (1 - sigmoid(x))，计算梯度\n",
    "    \n",
    "    return ds  \n",
    "    # 返回计算得到的梯度值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - 重塑数组的形状（Reshaping arrays） ###\n",
    "\n",
    "在深度学习中，两个常用的 numpy 函数是 [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) 和 [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)。  \n",
    "- `X.shape` 用于获取矩阵或向量 `X` 的形状（维度）。  \n",
    "- `X.reshape(...)` 用于将 `X` 重塑为其他维度。  \n",
    "\n",
    "例如，在计算机科学中，一张图片可以表示为一个形状为 $(length, height, depth = 3)$ 的三维数组。  \n",
    "然而，在将图片作为算法输入时，我们通常将其转换为一个形状为 $(length \\times height \\times 3, 1)$ 的向量。  \n",
    "换句话说，我们将三维数组“展开”为一维向量（也称为重塑 reshape）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![猫咪](./images/image2vector.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：实现函数 `image2vector()`，它接受一个形状为 `(length, height, 3)` 的输入，并返回一个形状为 `(length*height*3, 1)` 的向量。  \n",
    "例如，如果你想将一个形状为 `(a, b, c)` 的数组 `v` 重塑为形状为 `(a*b, c)` 的向量，你可以这样做：\n",
    "```python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2]))  # v.shape[0] = a；v.shape[1] = b；v.shape[2] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    image -- 一个 numpy 数组，形状为 (length, height, depth)，即一张彩色图片的三维结构\n",
    "             - length：图片的宽度（像素数）\n",
    "             - height：图片的高度（像素数）\n",
    "             - depth：图片的通道数（彩色图片通常为 3，即 RGB）\n",
    "\n",
    "    返回:\n",
    "    v -- 一个向量，形状为 (length * height * depth, 1)，即将原图展开为一列向量\n",
    "    \"\"\"\n",
    "    \n",
    "    # 使用 numpy 的 reshape 函数，将三维图片数组重塑为一个二维向量\n",
    "    # image.shape[0]：图片的宽度（length）\n",
    "    # image.shape[1]：图片的高度（height）\n",
    "    # image.shape[2]：图片的深度/通道数（depth，通常为3）\n",
    "    # 目标形状为 (length × height × depth, 1)，即变为一列\n",
    "    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)      \n",
    "    \n",
    "    # 返回展开后的向量 v\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# 这是一个 3×3×2 的三维数组，模拟一个形状为 (3, 3, 2) 的“图像”\n",
    "# 通常图像的形状为 (num_px_x, num_px_y, 3)，其中 3 表示 RGB 三个通道\n",
    "# 这里的例子使用的是深度为 2，便于演示和调试\n",
    "image = np.array([[[ 0.67826139,  0.29380381],       # 第1行第1列的像素的两个通道值\n",
    "                   [ 0.90714982,  0.52835647],       # 第1行第2列的像素\n",
    "                   [ 0.4215251 ,  0.45017551]],      # 第1行第3列的像素\n",
    "\n",
    "                  [[ 0.92814219,  0.96677647],       # 第2行第1列的像素\n",
    "                   [ 0.85304703,  0.52351845],       # 第2行第2列的像素\n",
    "                   [ 0.19981397,  0.27417313]],      # 第2行第3列的像素\n",
    "\n",
    "                  [[ 0.60659855,  0.00533165],       # 第3行第1列的像素\n",
    "                   [ 0.10820313,  0.49978937],       # 第3行第2列的像素\n",
    "                   [ 0.34144279,  0.94630077]]])     # 第3行第3列的像素\n",
    "\n",
    "# 使用 image2vector 函数将上述三维图像数组展开为一列向量，并打印结果\n",
    "# image.shape 是 (3, 3, 2)，目标 reshape 成 (3*3*2, 1) 即 (18, 1)\n",
    "# 打印出的结果为 image2vector(image) = [[...], [...], ..., [...]]\n",
    "print(\"image2vector(image) = \" + str(image2vector(image)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - 行归一化（Normalizing rows）\r\n",
    "\r\n",
    "在机器学习和深度学习中，我们常用的另一个技巧是对数据进行**归一化（normalize）**。  \r\n",
    "归一化通常会带来更好的性能表现，因为归一化之后梯度下降（gradient descent）收敛得更快。  \r\n",
    "这里的归一化指的是将 $x$ 转换为 $ \\frac{x}{\\| x\\|} $（即将 $x$ 的每一行向量除以它的范数）。\r\n",
    "\r\n",
    "例如：\r\n",
    "\r\n",
    "$$x = \r\n",
    "\\begin{bmatrix}\r\n",
    "    0 & 3 & 4 \\\\\r\n",
    "    2 & 6 & 4 \\\\\r\n",
    "\\end{bmatrix}\\tag{3}$$\r\n",
    "\r\n",
    "那么：\r\n",
    "\r\n",
    "$$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \r\n",
    "\\begin{bmatrix}\r\n",
    "    5 \\\\\r\n",
    "    \\sqrt{56} \\\\\r\n",
    "\\end{bmatrix}\\tag{4}$$\r\n",
    "\r\n",
    "因此：\r\n",
    "\r\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \r\n",
    "\\begin{bmatrix}\r\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\r\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\r\n",
    "\\end{bmatrix}\\tag{5}$$\r\n",
    "\r\n",
    "注意：你可以将不同形状的矩阵进行除法运算，这在 Python 中是允许的，这种特性叫做**广播（broadcasting）**，你将在第 5 部分中学习到它。\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**练习**：实现 `normalizeRows()` 函数，用于对一个矩阵的每一行进行归一化。  \r\n",
    "当你将这个函数应用到输入矩阵 `x` 后，`x` 的每一行都应当是一个单位长度的向量（即向量的长度为 1）。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    实现一个函数，对矩阵 x 的每一行进行归一化处理（使每行向量的长度为1）\n",
    "\n",
    "    参数：\n",
    "    x -- 一个 NumPy 矩阵，形状为 (n, m)，表示有 n 行、每行 m 个特征\n",
    "\n",
    "    返回：\n",
    "    x -- 归一化后的 NumPy 矩阵（按行归一化）。允许直接修改原始变量 x。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 计算每一行的 L2 范数（即每行向量的长度）\n",
    "    # np.linalg.norm 是 NumPy 的线性代数模块中的范数函数\n",
    "    # 参数说明：\n",
    "    #    x              ：输入矩阵\n",
    "    #    axis = 1       ：沿水平方向（对每一行）计算范数\n",
    "    #    keepdims = True：保留二维结构，输出形状为 (n, 1)，便于后续广播\n",
    "    #    ord = 2         是默认值，表示计算 2 范数（欧几里得范数）\n",
    "    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "\n",
    "    # 打印范数矩阵的形状，调试用\n",
    "    print(x_norm.shape)  # 输出如 (n, 1)\n",
    "    \n",
    "    # 打印原始矩阵 x 的形状，调试用\n",
    "    print(x.shape)       # 输出如 (n, m)\n",
    "\n",
    "    # 用广播机制将每行向量除以其对应的范数，实现按行归一化\n",
    "    x = x / x_norm\n",
    "\n",
    "    # 返回归一化后的矩阵\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(2, 3)\n",
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "# 创建一个二维 NumPy 数组 x\n",
    "# np.array：用于构造一个 NumPy 数组\n",
    "# 数组共有 2 行 3 列，分别为 [0, 3, 4] 和 [1, 6, 4]\n",
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "\n",
    "# 打印归一化后的结果\n",
    "# normalizeRows(x)：对每一行进行 L2 范数归一化\n",
    "# str(...)：将结果转换为字符串，以便和前面的提示文字连接起来\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**:\n",
    "在 normalizeRows() 函数中，你可以尝试打印 x_norm 和 x 的形状，然后重新运行评估。你会发现它们的形状不同。这是正常的，因为 x_norm 计算的是 x 每一行的范数。因此 x_norm 的行数相同，但只有 1 列。那么当你用 x 除以 x_norm 时，它是如何工作的呢？这被称为广播（broadcasting），我们现在就来解释它！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - 广播机制与softmax函数 ####\r\n",
    "\r\n",
    "在numpy中理解\"广播(broadcasting)\"机制非常重要。这个机制对于在不同形状的数组之间进行数学运算非常有用。要了解广播机制的完整细节，你可以阅读官方的[广播机制文档](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：使用numpy实现一个softmax函数。你可以将softmax看作是一种归一化函数，当你的算法需要对两个或多个类别进行分类时会用到它。你将在本专项课程的第二门课中深入学习softmax。\r\n",
    "\r\n",
    "**说明**：\r\n",
    "- 对于向量 $x \\in \\mathbb{R}^{1\\times n}$，softmax函数定义为：\r\n",
    "$$softmax(x) = softmax(\\begin{bmatrix}\r\n",
    "    x_1  &&\r\n",
    "    x_2 &&\r\n",
    "    ...  &&\r\n",
    "    x_n  \r\n",
    "\\end{bmatrix}) = \\begin{bmatrix}\r\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\r\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\r\n",
    "    ...  &&\r\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \r\n",
    "\\end{bmatrix}$$\r\n",
    "\r\n",
    "- 对于矩阵 $x \\in \\mathbb{R}^{m \\times n}$，其中 $x_{ij}$ 表示矩阵第 $i$ 行第 $j$ 列的元素，softmax函数定义为：\r\n",
    "$$softmax(x) = softmax\\begin{bmatrix}\r\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\r\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\r\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\r\n",
    "\\end{bmatrix} = \\begin{bmatrix}\r\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\r\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\r\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\r\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\r\n",
    "\\end{bmatrix} = \\begin{pmatrix}\r\n",
    "    softmax\\text{(x的第一行)}  \\\\\r\n",
    "    softmax\\text{(x的第二行)} \\\\\r\n",
    "    ...  \\\\\r\n",
    "    softmax\\text{(x的最后一行)} \\\\\r\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"计算输入x每一行的softmax值\n",
    "    \n",
    "    你的代码应该能处理行向量和形状为(n,m)的矩阵\n",
    "    \n",
    "    参数:\n",
    "    x -- 一个形状为(n,m)的numpy矩阵\n",
    "    \n",
    "    返回:\n",
    "    s -- 等于x的softmax值的numpy矩阵，形状与x相同(n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 对x逐元素应用指数函数。使用np.exp(...)\n",
    "    # 这会将每个元素x_ij转换为e^{x_ij}\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # 创建一个向量x_sum，用于对x_exp的每一行求和\n",
    "    # 使用np.sum(..., axis=1, keepdims=True)：\n",
    "    # - axis=1表示沿行方向求和（对每行单独求和）\n",
    "    # - keepdims=True保持二维特性（即使结果是单列也保持矩阵形式）\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    \n",
    "    # 打印中间结果的形状用于调试（实际使用时可以移除）\n",
    "    print(x_exp.shape)  # 输出x_exp的形状\n",
    "    print(x_sum.shape)  # 输出x_sum的形状\n",
    "    \n",
    "    # 通过x_exp除以x_sum来计算softmax(x)\n",
    "    # 这里会自动利用numpy的广播机制：\n",
    "    # x_exp (n,m) 矩阵会逐元素除以 x_sum (n,1) 矩阵\n",
    "    # numpy会自动将x_sum在列方向上进行广播来匹配x_exp的形状\n",
    "    s = x_exp / x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n",
      "(2, 1)\n",
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：\r\n",
    "- 如果你打印上面 `x_exp`、`x_sum` 和 `s` 的形状，并重新运行评测单元，你会看到 `x_sum` 的形状是 `(2,1)`，而 `x_exp` 和 `s` 的形状是 `(2,5)`。**x_exp/x_sum** 之所以能运行，是因为 Python 的广播机制。\r\n",
    "\r\n",
    "恭喜你！你现在已经对 Python 的 numpy 有了很好的理解，并且实现了一些将在深度学习中使用的实用函数。\r\n",
    "数。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\r\n",
    "**你需要记住的是：**\r\n",
    "- np.exp(x) 适用于任意的 np.array x，并对每个元素应用指数函数\r\n",
    "- sigmoid 函数及其梯度\r\n",
    "- image2vector 在深度学习中经常使用\r\n",
    "- np.reshape 被广泛应用。以后你会发现，保持矩阵/向量维度正确，有助于消除很多错误\r\n",
    "- numpy 拥有高效的内置函数\r\n",
    "- 广播机制（broadcasting）非常有用\r",
    "</font>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2) 向量化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习中，你会处理非常大的数据集。因此，计算效率不高的函数可能会成为算法中的巨大瓶颈，导致模型训练耗时极长。为了保证代码的计算效率，你需要使用向量化。举例来说，试着比较下面几种实现点积、外积和逐元素乘积的不同方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点积dot = 278\n",
      " ----- 计算耗时 = 0.0ms\n",
      "外积outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " -----计算耗时 = 0.0ms\n",
      "逐元素乘法elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
      " ----- 计算耗时 = 0.0ms\n",
      "通用点积gdot = [20.12554901 19.82366974 31.71766588]\n",
      " ----- 计算耗时 = 0.0ms\n"
     ]
    }
   ],
   "source": [
    "import time  # 导入time模块，用于计算代码执行时间\n",
    "import numpy as np  # 导入numpy模块，方便后续数组操作\n",
    "\n",
    "# 定义两个长度为15的列表x1和x2，作为向量示例\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### 经典向量点积实现 ###\n",
    "tic = time.process_time()  # 记录开始时间（CPU执行时间）\n",
    "dot = 0  # 初始化点积结果为0\n",
    "for i in range(len(x1)):  # 遍历向量x1的每个索引i（0~14）\n",
    "    dot += x1[i] * x2[i]  # 累加对应元素乘积到dot中\n",
    "toc = time.process_time()  # 记录结束时间\n",
    "print(\"点积dot = \" + str(dot) + \"\\n ----- 计算耗时 = \" + str(1000 * (toc - tic)) + \"ms\")  \n",
    "# 输出点积结果及计算耗时（毫秒）\n",
    "\n",
    "### 经典外积实现 ###\n",
    "tic = time.process_time()  # 记录开始时间\n",
    "outer = np.zeros((len(x1), len(x2)))  \n",
    "# 创建一个形状为(15,15)的零矩阵，用来存储外积结果\n",
    "for i in range(len(x1)):  # 遍历x1的每个索引\n",
    "    for j in range(len(x2)):  # 遍历x2的每个索引\n",
    "        outer[i, j] = x1[i] * x2[j]  # 计算x1第i个元素与x2第j个元素的乘积，赋值给outer矩阵对应位置\n",
    "toc = time.process_time()  # 记录结束时间\n",
    "print(\"外积outer = \" + str(outer) + \"\\n -----计算耗时 = \" + str(1000 * (toc - tic)) + \"ms\")  \n",
    "# 输出外积矩阵及计算耗时（毫秒）\n",
    "\n",
    "### 经典逐元素乘法实现 ###\n",
    "tic = time.process_time()  # 记录开始时间\n",
    "mul = np.zeros(len(x1))  # 创建长度为15的零数组，用来存储逐元素乘积结果\n",
    "for i in range(len(x1)):  # 遍历每个索引\n",
    "    mul[i] = x1[i] * x2[i]  # 计算对应位置元素的乘积，并赋值给mul\n",
    "toc = time.process_time()  # 记录结束时间\n",
    "print(\"逐元素乘法elementwise multiplication = \" + str(mul) + \"\\n ----- 计算耗时 = \" + str(1000 * (toc - tic)) + \"ms\")  \n",
    "# 输出逐元素乘积结果及计算耗时（毫秒）\n",
    "\n",
    "### 经典通用点积实现 ###\n",
    "W = np.random.rand(3, len(x1))  \n",
    "# 创建一个随机的3行15列的二维numpy数组W，代表权重矩阵\n",
    "tic = time.process_time()  # 记录开始时间\n",
    "gdot = np.zeros(W.shape[0])  \n",
    "# 创建一个长度为3的零数组，用来存储点积结果\n",
    "for i in range(W.shape[0]):  # 遍历权重矩阵的每一行（0~2）\n",
    "    for j in range(len(x1)):  # 遍历x1的每个索引\n",
    "        gdot[i] += W[i, j] * x1[j]  # 计算W第i行与x1的点积，累加到gdot对应位置\n",
    "toc = time.process_time()  # 记录结束时间\n",
    "print(\"通用点积gdot = \" + str(gdot) + \"\\n ----- 计算耗时 = \" + str(1000 * (toc - tic)) + \"ms\")  \n",
    "# 输出通用点积结果及计算耗时（毫秒）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点积dot = 278\n",
      " ----- 计算耗时 = 0.0ms\n",
      "外积outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- 计算耗时 = 0.0ms\n",
      "逐元素乘法elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
      " ----- 计算耗时 = 0.0ms\n",
      "通用点积gdot = [20.12554901 19.82366974 31.71766588]\n",
      " ----- 计算耗时 = 0.0ms\n"
     ]
    }
   ],
   "source": [
    "# 定义两个向量 x1 和 x2，用于向量化运算示例\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### 向量化的点积运算 ###\n",
    "tic = time.process_time()  # 记录起始时间\n",
    "dot = np.dot(x1, x2)  # 使用 numpy 的 dot 函数计算点积\n",
    "toc = time.process_time()  # 记录结束时间\n",
    "print(\"点积dot = \" + str(dot) + \"\\n ----- 计算耗时 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "# 输出结果和耗时（毫秒）\n",
    "\n",
    "### 向量化的外积运算 ###\n",
    "tic = time.process_time()  # 记录起始时间\n",
    "outer = np.outer(x1, x2)  # 使用 numpy 的 outer 函数计算外积，返回一个矩阵\n",
    "toc = time.process_time()  # 记录结束时间\n",
    "print(\"外积outer = \" + str(outer) + \"\\n ----- 计算耗时 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "# 输出外积矩阵和耗时\n",
    "\n",
    "### 向量化的逐元素乘法 ###\n",
    "tic = time.process_time()  # 记录起始时间\n",
    "mul = np.multiply(x1, x2)  # 使用 numpy 的 multiply 函数进行逐元素乘法\n",
    "toc = time.process_time()  # 记录结束时间\n",
    "print(\"逐元素乘法elementwise multiplication = \" + str(mul) + \"\\n ----- 计算耗时 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "# 输出逐元素乘积和耗时\n",
    "\n",
    "### 向量化的通用点积运算 ###\n",
    "tic = time.process_time()  # 记录起始时间\n",
    "dot = np.dot(W, x1)  # 使用 numpy 的 dot 函数进行矩阵 W 与向量 x1 的乘积，结果是一个长度为3的向量\n",
    "toc = time.process_time()  # 记录结束时间\n",
    "print(\"通用点积gdot = \" + str(dot) + \"\\n ----- 计算耗时 = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "# 输出通用点积的结果和耗时\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如你可能已经注意到的，向量化的实现更加简洁且高效。对于更大的向量或矩阵，运行时间的差异会变得更加明显。\r\n",
    "\r\n",
    "**注意**：`np.dot()` 执行的是矩阵-矩阵或矩阵-向量的乘法运算。这与 `np.multiply()` 和 `*` 运算符不同，后两者执行的是逐元素（element-wise）乘法运算（在 Matlab/Octave 中相当于 `.*`。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 实现 L1 和 L2 损失函数\r\n",
    "\r\n",
    "**练习**：实现 L1 损失的 numpy 向量化版本。你可能会用到 `abs(x)` 函数（返回 x 的绝对值）。\r\n",
    "\r\n",
    "**提醒**：\r\n",
    "- 损失函数用于评估模型的性能。损失越大，表示你的预测值（$\\hat{y}$）与真实值（$y$）的差异越大。\r\n",
    "- 在深度学习中，你会使用诸如梯度下降（Gradient Descent）等优化算法来训练模型并最小化损失。\r\n",
    "- L1 损失函数定义如下：\r\n",
    "$$\r\n",
    "\\begin{align*}\r\n",
    "& L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}|\r\n",
    "\\end{align*} \\tag{6}\r\n",
    "$$\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    参数说明：\n",
    "    yhat -- 大小为 m 的向量，表示预测标签\n",
    "    y -- 大小为 m 的向量，表示真实标签\n",
    "    \n",
    "    返回值：\n",
    "    loss -- 上面定义的 L1 损失函数的值\n",
    "    \"\"\"\n",
    "    \n",
    "    # 使用 numpy 向量化操作：\n",
    "    # 先计算预测值与真实值之间的差，再取绝对值，最后对所有元素求和\n",
    "    loss = np.sum(np.abs(y - yhat))\n",
    "    \n",
    "    # 返回最终的 L1 损失值\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：实现 L2 损失的 numpy 向量化版本。有多种方式可以实现 L2 损失，你可能会发现 `np.dot()` 函数非常有用。提醒一下，如果 $x = [x_1, x_2, ..., x_n]$，那么 `np.dot(x, x)` 就等于 $\\sum_{j=0}^n x_j^{2}$。\r\n",
    "\r\n",
    "- L2 损失函数定义如下：\r\n",
    "$$\r\n",
    "\\begin{align*}\r\n",
    "& L_2(\\hat{y}, y) = \\sum_{i=0}^m (y^{(i)} - \\hat{y}^{(i)})^2\r\n",
    "\\end{align*} \\tag{7}\r\n",
    "$$\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    参数说明：\n",
    "    yhat -- 大小为 m 的向量，表示预测标签\n",
    "    y -- 大小为 m 的向量，表示真实标签\n",
    "    \n",
    "    返回值：\n",
    "    loss -- 上面定义的 L2 损失函数的值\n",
    "    \"\"\"\n",
    "    \n",
    "    # 先计算误差向量（真实值 - 预测值），然后用 np.dot 计算该向量与其转置的点积\n",
    "    # 即：(y - yhat) · (y - yhat)^T，也就是所有误差的平方和\n",
    "    loss = np.dot((y - yhat), (y - yhat).T)\n",
    "    \n",
    "    # 返回最终的 L2 损失值\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在计算 L2 损失时，公式是对所有误差平方求和：\r\n",
    "\r\n",
    "$$\r\n",
    "L_2(\\hat{y}, y) = \\sum_{i=0}^m (y^{(i)} - \\hat{y}^{(i)})^2\r\n",
    "$$\r\n",
    "\r\n",
    "用向量表示就是计算向量误差的平方和，即：\r\n",
    "\r\n",
    "$$\r\n",
    "(y - \\hat{y}) \\cdot (y - \\hat{y}) = \\sum_{i=0}^m (y_i - \\hat{y}_i)^2\r\n",
    "$$\r\n",
    "\r\n",
    "这里的 `(y - yhat)` 是一个向量。  \r\n",
    "为了计算它的平方和，可以用点积（dot product），即：\r\n",
    "\r\n",
    "```python\r\n",
    "np.dot((y - yhat), (y - yhat).T)\r\n",
    "(y - yhat))  # 就已经是 ∑(误差²)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在函数 `L2(yhat, y)` 中，计算的是 L2 损失（也称为平方误差损失），公式为：\r\n",
    "\r\n",
    "\\[\r\n",
    "L2 = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2\r\n",
    "\\]\r\n",
    "\r\n",
    "### 为什么要乘以转置？\r\n",
    "\r\n",
    "1. **误差向量定义：**\r\n",
    "\r\n",
    "   令误差向量为：\r\n",
    "   \\[\r\n",
    "   e = y - \\hat{y}\r\n",
    "   \\]\r\n",
    "\r\n",
    "   这是一个大小为 \\( m \\) 的列向量（或一维数组）。\r\n",
    "\r\n",
    "2. **点积的含义：**\r\n",
    "\r\n",
    "   当你用 `np.dot(e, e.T)` 计算时，实际上是计算：\r\n",
    "   \r\n",
    "   \\[\r\n",
    "   e \\cdot e^T = \\sum_{i=1}^m e_i \\times e_i = \\sum_{i=1}^m e_i^2\r\n",
    "   \\]\r\n",
    "\r\n",
    "   也就是误差向量中每个元素平方后的总和。\r\n",
    "\r\n",
    "3. **转置的作用：**\r\n",
    "\r\n",
    "   - `e` 是一个行向量或者一维数组，`e.T` 是它的转置。\r\n",
    "   - 乘以转置，是为了将误差向量的所有元素对应相乘并求和。\r\n",
    "   - 这是计算向量的内积（dot product）的常见方式，得到一个标量值。\r\n",
    "\r\n",
    "4. **总结：**\r\n",
    "\r\n",
    "   乘以转置的操作可以快速、简洁地实现平方误差求和，而不必显式使用循环逐元素平方后求和。\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 简单示例\r\n",
    "\r\n",
    "```python\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "y = np.array([1, 2, 3])\r\n",
    "yhat = np.array([1.1, 1.9, 3.2])\r\n",
    "e = y - yhat  # 误差向量\r\n",
    "\r\n",
    "loss = np.dot(e, e.T)  # 计算误差平方和\r\n",
    "print(loss)  # 输出标量，比如 0.1**2 + (-0.1)**2 + (-0.2)**2 = 0.06\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你完成了本节课程！希望这个简短的热身练习能对你后续的学习有所帮助。接下来的任务将会更加精彩和有趣！\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\r\n",
    "**需要记住的是：**\r\n",
    "- 向量化在深度学习中非常重要，它能带来计算效率和代码清晰性。\r\n",
    "- 你已经复习了 L1 和 L2 损失函数。\r\n",
    "- 你已经熟悉了许多 numpy 函数，比如：np.sum、np.dot、np.multiply、np.maximum 等等。\r\n",
    "</font>\r\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
