{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 教程\r\n",
    "\r\n",
    "欢迎来到本周的编程作业。到目前为止，你一直使用 numpy 来构建神经网络。现在我们将引导你使用一个深度学习框架，让你更轻松地构建神经网络。像 TensorFlow、PaddlePaddle、Torch、Caffe、Keras 等机器学习框架，可以显著加快你的机器学习开发速度。这些框架都有丰富的文档，你可以自由阅读。在本次作业中，你将学习在 TensorFlow 中完成以下任务：\r\n",
    "\r\n",
    "- 初始化变量\r\n",
    "- 启动会话\r\n",
    "- 训练算法\r\n",
    "- 实现神经网络\r\n",
    "\r\n",
    "编程框架不仅能缩短你的编码时间，有时还能进行优化，提升代码运行速度。\r\n",
    "\r\n",
    "## 1 - 探索 TensorFlow 库\r\n",
    "\r\n",
    "首先，导入库：\r\n",
    "```python\r\n",
    "import tensorflow as tf\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你已经导入了库，我们将带你了解它的不同应用。首先从一个示例开始，我们帮你计算一个训练样本的损失函数。\r\n",
    "\r\n",
    "损失函数定义为：  \r\n",
    "$$loss = \\mathcal{L}(\\hat{y}, y) = (\\hat y^{(i)} - y^{(i)})^2 \\tag{1}$$\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = tf.constant(36, name='y_hat')            # Define y_hat constant. Set to 36.\n",
    "y = tf.constant(39, name='y')                    # Define y. Set to 39\n",
    "\n",
    "loss = tf.Variable((y - y_hat)**2, name='loss')  # Create a variable for the loss\n",
    "\n",
    "init = tf.global_variables_initializer()         # When init is run later (session.run(init)),\n",
    "                                                 # the loss variable will be initialized and ready to be computed\n",
    "with tf.Session() as session:                    # Create a session and print the output\n",
    "    session.run(init)                            # Initializes the variables\n",
    "    print(session.run(loss))                     # Prints the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow中编写和运行程序通常包括以下步骤：\r\n",
    "\r\n",
    "1. 创建尚未执行/计算的张量（变量）。\r\n",
    "2. 编写这些张量之间的操作。\r\n",
    "3. 初始化你的张量。\r\n",
    "4. 创建一个会话（Session）。\r\n",
    "5. 运行会话，执行之前定义的操作。\r\n",
    "\r\n",
    "因此，当我们创建一个表示损失的变量时，我们实际上只是定义了损失作为其他量的函数，但并没有计算它的值。为了计算它的值，我们需要运行 `init = tf.global_variables_initializer()` 来初始化变量，最后通过运行会话才能得到 `loss` 的具体数值并打印出来。\r\n",
    "\r\n",
    "现在，我们来看一个简单的例子。请运行下面的代码单元：\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(10)\n",
    "c = tf.multiply(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如预期的那样，你不会直接看到数字20！你得到的是一个张量（tensor），提示这个结果是一个没有形状属性的张量，类型是“int32”。这说明你只是构建了“计算图”（computation graph），但还没有真正执行这段计算。\r\n",
    "\r\n",
    "为了真正进行乘法运算，你需要创建一个会话（session）并运行它。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "太好了！总结一下，**记得初始化变量，创建会话，并在会话中运行操作**。\r\n",
    "\r\n",
    "接下来，你还需要了解占位符（placeholder）。占位符是一个你可以稍后指定值的对象。  \r\n",
    "要为占位符指定值，可以使用“喂入字典”（feed dictionary，`feed_dict`变量）。下面我们创建了一个占位符 `x`，这让我们在运行会话时可以传入具体的数值。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the value of x in the feed_dict\n",
    "\n",
    "x = tf.placeholder(tf.int64, name = 'x')\n",
    "print(sess.run(2 * x, feed_dict = {x: 3}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当你第一次定义 `x` 的时候，不需要给它指定具体的值。占位符本质上是一个变量，你会在运行会话（session）时才给它赋值。我们称这个过程为**给占位符“喂数据”**。\r\n",
    "\r\n",
    "这里发生的事情是：当你定义计算所需的操作时，你是在告诉 TensorFlow 如何构建计算图（computation graph）。计算图中可以包含一些占位符，这些占位符的值你会在之后指定。最终，当你运行会话时，你是在告诉 TensorFlow 执行这个计算图。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - 线性函数\n",
    "\n",
    "让我们开始这个编程练习，计算以下方程：$Y = WX + b$，其中 $W$ 和 $X$ 是随机矩阵，$b$ 是随机向量。\n",
    "\n",
    "**练习**：计算 $WX + b$，其中 $W, X$ 和 $b$ 都是从正态分布中随机生成的。$W$ 的形状是 (4, 3)，$X$ 是 (3, 1)，$b$ 是 (4, 1)。例如，下面是如何定义一个形状为 (3,1) 的常量 $X$：\n",
    "```python\n",
    "X = tf.constant(np.random.randn(3,1), name = \"X\")\n",
    "\n",
    "```\n",
    "你可能会用到以下函数：\n",
    "\n",
    "- tf.matmul(..., ...) 用于矩阵乘法\n",
    "- tf.add(..., ...) 用于加法运算\n",
    "- np.random.randn(...) 用于随机初始化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_function\n",
    "\n",
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implements a linear function: \n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns: \n",
    "    result -- runs the session for Y = WX + b \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (4 lines of code)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # close the session \n",
    "    sess.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"result = \" + str(linear_function()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Expected Output ***: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "**result**\n",
    "</td>\n",
    "<td>\n",
    "[[-2.15657382]\n",
    " [ 2.95891446]\n",
    " [-1.08926781]\n",
    " [-0.84538042]]\n",
    "</td>\n",
    "</tr> \n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - 计算 sigmoid 函数\n",
    "\n",
    "很好！你刚刚实现了一个线性函数。TensorFlow 提供了许多常用的神经网络函数，比如 `tf.sigmoid` 和 `tf.softmax`。在这个练习中，我们将计算输入的 sigmoid 函数。\n",
    "\n",
    "你需要使用一个占位符变量 `x`。在运行 session 时，应该使用 feed dictionary 传入输入 `z`。在这个练习中，你需要 (i) 创建一个占位符 `x`，(ii) 使用 `tf.sigmoid` 定义计算 sigmoid 的操作，然后 (iii) 运行 session。\n",
    "\n",
    "**练习**：在下面实现 sigmoid 函数。你应该使用以下函数：\n",
    "\n",
    "- `tf.placeholder(tf.float32, name = \"...\")`\n",
    "- `tf.sigmoid(...)`\n",
    "- `sess.run(..., feed_dict = {x: z})`\n",
    "\n",
    "注意，TensorFlow 中创建和使用 session 有两种常见方式：\n",
    "\n",
    "**方式一：**\n",
    "```python\n",
    "sess = tf.Session()\n",
    "# 如果需要，运行变量初始化，然后执行操作\n",
    "result = sess.run(..., feed_dict = {...})\n",
    "sess.close()  # 关闭 session\n",
    "```\n",
    "**方式二：**\r\n",
    "```python\r\n",
    "with tf.Session() as sess: \r\n",
    "    # 如果需要，运行变量初始化，然后执行操作\r\n",
    "    result = sess.run(..., feed_dict = {...})\r\n",
    "    # 这样会自动帮你关闭 sessio```n :)\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    results -- the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### ( approx. 4 lines of code)\n",
    "    # Create a placeholder for x. Name it 'x'.\n",
    "\n",
    "    \n",
    "    # compute sigmoid(x)\n",
    "\n",
    "    \n",
    "    # Create a session, and run it. Please use the method 2 explained above. \n",
    "    # You should use a feed_dict to pass z's value to x. \n",
    "\n",
    "        # Run session and call the output \"result\"\n",
    "\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print (\"sigmoid(12) = \" + str(sigmoid(12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Expected Output ***: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "**sigmoid(0)**\n",
    "</td>\n",
    "<td>\n",
    "0.5\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "**sigmoid(12)**\n",
    "</td>\n",
    "<td>\n",
    "0.999994\n",
    "</td>\n",
    "</tr> \n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \r\n",
    "**总结一下，你现在已经掌握如何**：\r\n",
    "1. 创建占位符（placeholders）\r\n",
    "2. 指定对应你想计算的操作的计算图（computation graph）\r\n",
    "3. 创建会话（session）\r\n",
    "4. 运行会话（session），必要时使用 feed dictionary 来为占位符变量指定值\r\n",
    "</font>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - 计算代价函数（Cost）\r\n",
    "\r\n",
    "你也可以使用内置函数来计算神经网络的代价函数。所以，不必自己写代码去计算以下表达式，针对每个样本 $i=1,...,m$：\r\n",
    "$$ J = - \\frac{1}{m}  \\sum_{i = 1}^m  \\left( y^{(i)} \\log a^{[2](i)} + (1-y^{(i)}) \\log (1 - a^{[2](i)}) \\right) \\tag{2}$$\r\n",
    "\r\n",
    "你可以用 TensorFlow 一行代码实现它！\r\n",
    "\r\n",
    "**练习**：实现交叉熵损失函数。你将使用的函数是：\r\n",
    "\r\n",
    "- `tf.nn.sigmoid_cross_entropy_with_logits(logits = ..., labels = ...)`\r\n",
    "\r\n",
    "你的代码应当输入 `z`，计算 sigmoid （得到 `a`），然后计算交叉熵代价 $J$。这可以通过调用 `tf.nn.sigmoid_cross_entropy_with_logits` 一步完成，它计算的就是：\r\n",
    "\r\n",
    "$$ - \\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)} \\log \\sigma(z^{[2](i)}) + (1 - y^{(i)}) \\log (1 - \\sigma(z^{[2](i)})) \\right) \\tag{2} $$\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cost\n",
    "\n",
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost using the sigmoid cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "    labels -- vector of labels y (1 or 0) \n",
    "    \n",
    "    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n",
    "    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n",
    "    \n",
    "    Returns:\n",
    "    cost -- runs the session of the cost (formula (2))\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Create the placeholders for \"logits\" (z) and \"labels\" (y) (approx. 2 lines)\n",
    "\n",
    "\n",
    "    \n",
    "    # Use the loss function (approx. 1 line)\n",
    "\n",
    "    \n",
    "    # Create a session (approx. 1 line). See method 1 above.\n",
    "\n",
    "    \n",
    "    # Run the session (approx. 1 line).\n",
    "\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = sigmoid(np.array([0.2,0.4,0.7,0.9]))\n",
    "cost = cost(logits, np.array([0,0,1,1]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output** : \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **cost**\n",
    "        </td>\n",
    "        <td>\n",
    "        [ 1.00538719  1.03664088  0.41385433  0.39956614]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - 使用 One Hot 编码\r\n",
    "\r\n",
    "在深度学习中，标签向量 y 通常是从 0 到 C-1 的数字组成，其中 C 是类别数量。例如当 C=4 时，你可能有如下的 y 向量，需要转换成如下形式：\r\n",
    "\r\n",
    "<img src=\"images/onehot.png\" style=\"width:600px;height:150px;\">\r\n",
    "\r\n",
    "这种转换叫做“one hot”编码，因为转换后，每列只有一个元素是“热”的（即值为1）。在 numpy 中，你可能需要写几行代码才能实现，但在 TensorFlow 中，只需一行代码：\r\n",
    "\r\n",
    "- `tf.one_hot(labels, depth, axis)`\r\n",
    "\r\n",
    "**练习**：实现下面的函数，输入标签向量和类别总数 C，返回 one hot 编码。请使用 `tf.one_hot()` 实现。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_hot_matrix\n",
    "\n",
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **one_hot**\n",
    "        </td>\n",
    "        <td>\n",
    "        [[ 0.  0.  0.  1.  0.  0.]\n",
    " [ 1.  0.  0.  0.  0.  1.]\n",
    " [ 0.  1.  0.  0.  1.  0.]\n",
    " [ 0.  0.  1.  0.  0.  0.]]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - 用零和一初始化\r\n",
    "\r\n",
    "现在你将学习如何初始化全零或全一的向量。你会用到的函数是 `tf.ones()`。如果要初始化全零，可以用 `tf.zeros()`。这些函数接受一个形状（shape），返回对应维度的全零或全一数组。\r\n",
    "\r\n",
    "**练习**：实现下面的函数，输入一个形状（shape），返回一个对应形状的全一数组。\r\n",
    "\r\n",
    "- `tf.ones(shape)`\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ones\n",
    "\n",
    "def ones(shape):\n",
    "    \"\"\"\n",
    "    Creates an array of ones of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    shape -- shape of the array you want to create\n",
    "        \n",
    "    Returns: \n",
    "    ones -- array containing only ones\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create \"ones\" tensor using tf.ones(...). (approx. 1 line)\n",
    "\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "\n",
    "    \n",
    "    # Run the session to compute 'ones' (approx. 1 line)\n",
    "\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ones = \" + str(ones([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **ones**\n",
    "        </td>\n",
    "        <td>\n",
    "        [ 1.  1.  1.]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - 用 TensorFlow 构建你的第一个神经网络\r\n",
    "\r\n",
    "在本部分作业中，你将使用 TensorFlow 构建一个神经网络。请记住，使用 TensorFlow 实现模型有两个步骤：\r\n",
    "\r\n",
    "- 创建计算图（Computation Graph）\r\n",
    "- 运行计算图（Run the Graph）\r\n",
    "\r\n",
    "让我们先了解你将要解决的问题！\r\n",
    "\r\n",
    "### 2.0 - 问题描述：SIGNS 数据集\r\n",
    "\r\n",
    "某天下午，我和朋友们决定教计算机识别手语。我们花了几个小时在白墙前拍照，收集了如下数据集。你的任务是构建一个算法，帮助言语障碍者与不懂手语的人沟通。\r\n",
    "\r\n",
    "- **训练集**：1080 张图片，每张64×64像素，表示数字 0 到 5 的手势（每个数字180张图片）。\r\n",
    "- **测试集**：120 张图片，每张64×64像素，表示数字 0 到 5 的手势（每个数字20张图片）。\r\n",
    "\r\n",
    "注意，这只是 SIGNS 数据集的一个子集，完整数据集包含更多手势。\r\n",
    "\r\n",
    "以下是每个数字的样例图片，以及我们如何表示标签的说明。这些是原始图片，分辨率较高，未缩小到64×64像素。\r\n",
    "<img src=\"images/hands.png\" style=\"width:800px;height:350px;\">\r\n",
    "<caption><center> <u><font color='purple'> **图1** </u><font color='purple'>：SIGNS 数据集示例<br> <font color='black'> </center>\r\n",
    "\r\n",
    "运行下面的代码加载数据集。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the index below and run the cell to visualize some examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 70\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual you flatten the image dataset, then normalize it by dividing by 255. On top of that, you will convert each label to a one-hot vector as shown in Figure 1. Run the cell below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the training and test images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "# Normalize image vectors\n",
    "X_train = X_train_flatten/255.\n",
    "X_test = X_test_flatten/255.\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**，12288 来自于 $64 \\times 64 \\times 3$。每张图片是一个 64×64 像素的正方形，3 代表 RGB 三个颜色通道。请确保你理解这些形状的含义，再继续下一步操作。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**你的目标** 是构建一个能够高准确率识别手势的算法。为此，你将构建一个与之前用 numpy 实现的猫识别模型几乎相同的 TensorFlow 模型（但现在输出层用的是 softmax）。这是一个很好的机会来比较你用 numpy 实现的模型和 TensorFlow 实现的异同。\r\n",
    "\r\n",
    "**模型结构** 是 *LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX*。原来的 SIGMOID 输出层被替换成了 SOFTMAX。SOFTMAX 层是 SIGMOID 在多分类问题上的推广。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - 创建占位符（placeholders）\r\n",
    "\r\n",
    "你的第一个任务是为 `X` 和 `Y` 创建占位符（placeholders）。这将允许你在运行 session 时传入训练数据。\r\n",
    "\r\n",
    "**练习：** 请实现下面的函数，在 TensorFlow 中创建占位符。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_placeholders(12288, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **X**\n",
    "        </td>\n",
    "        <td>\n",
    "        Tensor(\"Placeholder_1:0\", shape=(12288, ?), dtype=float32) (not necessarily Placeholder_1)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Y**\n",
    "        </td>\n",
    "        <td>\n",
    "        Tensor(\"Placeholder_2:0\", shape=(10, ?), dtype=float32) (not necessarily Placeholder_2)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - 初始化参数\n",
    "\n",
    "你的第二个任务是在 TensorFlow 中初始化参数。\n",
    "\n",
    "**练习：** 请实现下面的函数，使用 Xavier 初始化权重，使用零初始化偏置。参数形状如下。举例来说，初始化 W1 和 b1 可以用：\n",
    "\n",
    "```python\n",
    "W1 = tf.get_variable(\"W1\", [25, 12288], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "b1 = tf.get_variable(\"b1\", [25, 1], initializer=tf.zeros_initializer())\n",
    "```\n",
    "请使用 seed=1，以确保结果与你的保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **W1**\n",
    "        </td>\n",
    "        <td>\n",
    "         < tf.Variable 'W1:0' shape=(25, 12288) dtype=float32_ref >\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **b1**\n",
    "        </td>\n",
    "        <td>\n",
    "        < tf.Variable 'b1:0' shape=(25, 1) dtype=float32_ref >\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **W2**\n",
    "        </td>\n",
    "        <td>\n",
    "        < tf.Variable 'W2:0' shape=(12, 25) dtype=float32_ref >\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **b2**\n",
    "        </td>\n",
    "        <td>\n",
    "        < tf.Variable 'b2:0' shape=(12, 1) dtype=float32_ref >\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如预期，参数还没有被计算（评估）。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Tensorflow中的前向传播\r\n",
    "\r\n",
    "现在你将实现tensorflow中的前向传播模块。该函数将接收一个参数字典，并完成前向传播。你将使用以下函数：\r\n",
    "\r\n",
    "- `tf.add(...,...)` 进行加法运算\r\n",
    "- `tf.matmul(...,...)` 进行矩阵乘法\r\n",
    "- `tf.nn.relu(...)` 应用ReLU激活函数\r\n",
    "\r\n",
    "**问题**：实现神经网络的前向传播。我们为你注释了对应的numpy代码，方便你比较tensorflow和numpy的实现。需要注意的是，前向传播在`z3`处停止。原因是tensorflow中最后一层线性输出会作为计算损失的输入，因此不需要计算`a3`！\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(12288, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **Z3**\n",
    "        </td>\n",
    "        <td>\n",
    "        Tensor(\"Add_2:0\", shape=(6, ?), dtype=float32)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能注意到前向传播并没有输出任何缓存（cache）。你将在后面讲解反向传播的时候理解为什么会这样。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 计算代价\n",
    "\n",
    "如前所述，使用以下方法计算代价非常简单：\n",
    "```python\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels = ...))\n",
    "```\n",
    "**问题**：请实现下面的代价函数。  \n",
    "- 需要注意的是，`tf.nn.softmax_cross_entropy_with_logits` 的输入 \"`logits`\" 和 \"`labels`\" 期望的形状是（样本数量，类别数）。因此我们已经帮你对 Z3 和 Y 做了转置。  \n",
    "- 此外，`tf.reduce_mean` 基本上是对所有样本求平均（即对样本求和再除以样本数）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(12288, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **cost**\n",
    "        </td>\n",
    "        <td>\n",
    "        Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - 反向传播与参数更新\n",
    "\n",
    "这就是你会感谢编程框架的地方。所有的反向传播和参数更新，都只需一行代码即可完成。将这行代码整合到模型中非常简单。\n",
    "\n",
    "在计算完代价函数后，你将创建一个 \"`optimizer`\" 对象。运行 `tf.session` 时，需要调用这个对象和代价函数。调用时，它会使用所选的方法和学习率对代价进行优化。\n",
    "\n",
    "例如，对于梯度下降，优化器代码为：\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "```\n",
    "执行优化时，可以这样写：\r\n",
    "```python\r\n",
    "_ , c = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y}```\n",
    "\n",
    "这段代码通过反向传递计算反向传播，即按反向顺序通过 TensorFlow 计算图，从代价传递到输入。\r\n",
    "\r\n",
    "**注意** 编程时，我们常用 `_` 作为“丢弃”变量，用来存储那些后续不再使用的值。这里，`_` 接收了 `optimizer` 的计算结果（我们不需要它），而 `c` 则接收了 `cost` 的\n",
    "\n",
    ")\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 - 构建模型\r\n",
    "\r\n",
    "现在，你将把所有内容整合在一起！\r\n",
    "\r\n",
    "**练习：** 实现该模型。你将调用之前实现的函数。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "运行以下单元格来训练你的模型！在我们的机器上大约需要 5 分钟。你的“第 100 轮训练后的代价”应为 1.016458。如果不是，不要浪费时间；点击笔记本上方栏的停止按钮（⬛）中断训练，然后尝试修正你的代码。如果代价正确，休息一下，5 分钟后再回来继续！\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**：\r\n",
    "\r\n",
    "<table> \r\n",
    "    <tr> \r\n",
    "        <td>\r\n",
    "            **训练准确率**\r\n",
    "        </td>\r\n",
    "        <td>\r\n",
    "        0.999074\r\n",
    "        </td>\r\n",
    "    </tr>\r\n",
    "    <tr> \r\n",
    "        <td>\r\n",
    "            **测试准确率**\r\n",
    "        </td>\r\n",
    "        <td>\r\n",
    "        0.716667\r\n",
    "        </td>\r\n",
    "    </tr>\r\n",
    "</table>\r\n",
    "\r\n",
    "太棒了，你的算法能够以71.7%的准确率识别表示数字0到5的符号。\r\n",
    "\r\n",
    "**见解**：  \r\n",
    "- 你的模型看起来足够大，可以很好地拟合训练集。然而，鉴于训练准确率和测试准确率的差异，你可以尝试添加 L2 或 dropout 正则化来减少过拟合。  \r\n",
    "- 可以把 session 理解为一个用于训练模型的代码块。每次你在一个小批量数据上运行 session，都会训练模型参数。总共运行了大量次数（1500个epoch），直到得到训练良好的参数。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你确实值得一个“点赞”，尽管如你所见，算法似乎分类错误。原因是训练集中并不包含任何“竖起大拇指”的样本，因此模型不知道如何处理它！我们称这种情况为“数据分布不匹配”，这是下一门课程《结构化机器学习项目》中会讲到的各种问题之一。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<font color='blue'>\n",
    "    \r\n",
    "**你应该记住的内容**：  \r\n",
    "- TensorFlow 是深度学习中使用的编程框架  \r\n",
    "- TensorFlow 中两个主要的对象类型是张量（Tensors）和操作（Operators）  \r\n",
    "- 使用 TensorFlow 编程时需要以下步骤：  \r\n",
    "    - 创建包含张量（变量、占位符等）和操作（如 tf.matmul、tf.add 等）的计算图  \r\n",
    "    - 创建会话（session）  \r\n",
    "    - 初始化会话  \r\n",
    "    - 运行会话以执行计算图  \r\n",
    "- 正如你在 model() 中看到的，可以多次执行同一个计算图  \r\n",
    "- 当在 “optimizer” 对象上运行会话时，反向传播和优化会自动完成  \r\n",
    "</font>\r\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "BFd89",
   "launcher_item_id": "AH2rK"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
