{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络：逐步实现\n",
    "\n",
    "在本程序中，你将使用 **numpy** 来实现卷积层（CONV）和池化层（POOL），包括前向传播和反向传播。\n",
    "\n",
    "**符号说明**：\n",
    "- 上标 $[l]$ 表示第 $l$ 层的对象。  \n",
    "  - 例子：$a^{[4]}$ 表示第4层的激活值；$W^{[5]}$ 和 $b^{[5]}$ 表示第5层的参数。\n",
    "\n",
    "- 上标 $(i)$ 表示第 $i$ 个样本的对象。  \n",
    "  - 例子：$x^{(i)}$ 表示第 $i$ 个训练样本的输入。\n",
    "\n",
    "- 下标 $i$ 表示向量的第 $i$ 个元素。  \n",
    "  - 例子：$a^{[l]}_i$ 表示第 $l$ 层激活值中的第 $i$ 个分量（假设这是一个全连接层）。\n",
    "\n",
    "- $n_H$、$n_W$ 和 $n_C$ 分别表示某一层的高度、宽度和通道数。  \n",
    "  如果要引用特定的第 $l$ 层，可以写作 $n_H^{[l]}$、$n_W^{[l]}$、$n_C^{[l]}$。  \n",
    "\n",
    "- $n_{H_{prev}}$、$n_{W_{prev}}$ 和 $n_{C_{prev}}$ 分别表示前一层的高度、宽度和通道数。  \n",
    "  如果引用特定的第 $l$ 层，可以写作 $n_H^{[l-1]}$、$n_W^{[l-1]}$、$n_C^{[l-1]}$。  \n",
    "\n",
    "我们假设你已经熟悉 `numpy`，并且完成了前面课程的学习。现在让我们开始吧！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 包\r\n",
    "\r\n",
    "我们先导入在本作业中需要用到的所有包。  \r\n",
    "- [numpy](www.numpy.org) 是 Python 中进行科学计算的基础包。  \r\n",
    "- [matplotlib](http://matplotlib.org) 是 Python 中用于绘图的库。  \r\n",
    "- `np.random.seed(1)` 用于保持所有随机函数调用的一致性，这将有助于我们对你的作业进行评分。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 导入必要的库\n",
    "# ==============================\n",
    "\n",
    "import numpy as np  \n",
    "# 导入 NumPy 库，用于高效的数值计算和矩阵运算\n",
    "# np 是 NumPy 的常用别名\n",
    "\n",
    "import h5py  \n",
    "# 导入 h5py 库，用于读取和写入 HDF5 文件格式（常用于存储大规模数据集）\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "# 导入 Matplotlib 库的 pyplot 模块，用于绘图\n",
    "# plt 是 pyplot 模块的常用别名\n",
    "\n",
    "# ==============================\n",
    "# 配置 Matplotlib 绘图\n",
    "# ==============================\n",
    "\n",
    "%matplotlib inline  \n",
    "# Jupyter Notebook 魔法命令\n",
    "# 用于在 Notebook 内部直接显示绘制的图像，而不是弹出新的窗口\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)  \n",
    "# 设置绘图默认大小\n",
    "# figure.figsize 参数表示图像的宽度和高度，单位为英寸 (inch)\n",
    "# 这里设置为宽 5.0 inch，高 4.0 inch\n",
    "\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  \n",
    "# 设置图像显示的插值方式\n",
    "# 'nearest' 表示不进行插值，保持像素原始显示效果\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'gray'  \n",
    "# 设置图像默认颜色映射（colormap）为灰度图\n",
    "# 适合显示单通道灰度图像，例如黑白图\n",
    "\n",
    "# ==============================\n",
    "# 自动重新加载模块\n",
    "# ==============================\n",
    "\n",
    "%load_ext autoreload  \n",
    "# Jupyter Notebook 魔法命令\n",
    "# 加载 autoreload 扩展，用于自动重新加载外部导入的 Python 模块\n",
    "# 方便在调试或修改模块后立即生效，无需重启 Notebook\n",
    "\n",
    "%autoreload 2  \n",
    "# 配置 autoreload 行为\n",
    "# 2 表示对所有导入的模块都自动重新加载\n",
    "# 当模块的源代码修改后，下次调用时会自动生效\n",
    "\n",
    "# ==============================\n",
    "# 设置随机数种子\n",
    "# ==============================\n",
    "\n",
    "np.random.seed(1)  \n",
    "# 设置 NumPy 随机数生成器的种子\n",
    "# 目的是保证随机数的可复现性\n",
    "# 同样的代码每次运行都会生成相同的随机数序列\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 作业大纲\n",
    "\n",
    "在本作业中，你将实现卷积神经网络的基本构建模块！  \n",
    "你要实现的每个函数都会附带详细的说明，引导你逐步完成实现：\n",
    "\n",
    "- 卷积相关函数，包括：\n",
    "  - 零填充（Zero Padding）\n",
    "  - 卷积窗口（Convolve window）\n",
    "  - 卷积前向传播（Convolution forward）\n",
    "  - 卷积反向传播（可选，Convolution backward）\n",
    "\n",
    "- 池化相关函数，包括：\n",
    "  - 池化前向传播（Pooling forward）\n",
    "  - 创建掩码（Create mask）\n",
    "  - 值分配（Distribute value）\n",
    "  - 池化反向传播（可选，Pooling backward）\n",
    "\n",
    "在本Notebook中，你将用 `numpy` 从零开始实现这些函数。  \n",
    "在下一个Notebook中，你将使用PyTorch中对应的函数来构建如下模型：\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "**注意**：对于每一个前向传播函数，都有一个对应的反向传播函数。  \n",
    "因此，在实现每一步前向传播模块时，你都需要在缓存（cache）中存储一些参数，这些参数会在反向传播计算梯度时使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 卷积神经网络\r\n",
    "\r\n",
    "尽管编程框架让卷积的使用变得很简单，但卷积仍然是深度学习中最难理解的概念之一。  \r\n",
    "一个卷积层会将输入体（input volume）转换为一个不同大小的输出体（output volume），如下图所示：\r\n",
    "\r\n",
    "<img src=\"images/conv_nn.png\" style=\"width:350px;height:200px;\">\r\n",
    "\r\n",
    "在这一部分中，你将逐步构建卷积层的各个步骤。  \r\n",
    "你将首先实现两个辅助函数：  \r\n",
    "- 一个用于实现零填充（zero padding）；  \r\n",
    "- 另一个用于计算卷积操作本身。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - 零填充（Zero-Padding）\r\n",
    "\r\n",
    "零填充会在图像的边界周围补充零值：\r\n",
    "\r\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\r\n",
    "<caption><center> <u> <font color='purple'> **图1** </u><font color='purple'> ：**零填充** <br> 一个具有3个通道（RGB）的图像，填充大小为2。 </center></caption>\r\n",
    "\r\n",
    "零填充的主要好处如下：\r\n",
    "\r\n",
    "- 它允许你在使用卷积层（CONV layer）时，不必让特征图的高度和宽度缩小。  \r\n",
    "  这对构建更深的网络非常重要，否则随着层数增加，图像的高度和宽度会逐渐缩小。  \r\n",
    "  一个重要的特例就是 **“same” 卷积**，在这种情况下，经过一层卷积后，高度和宽度保持不变。  \r\n",
    "\r\n",
    "- 它帮助我们保留更多图像边界处的信息。  \r\n",
    "  如果没有填充，在下一层中，只有极少的值会受到图像边缘像素的影响。\r\n",
    "\r\n",
    "**练习**：实现下面的函数，它将一个批次中的所有图像都用零进行填充。  \r\n",
    "请使用 [np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html)。  \r\n",
    "\r\n",
    "例如，如果你想对一个形状为 $(5,5,5,5,5)$ 的数组 `a` 进行填充：  \r\n",
    "- 在第2维上填充 `pad = 1`  \r\n",
    "- 在第4维上填充 `pad = 3`  \r\n",
    "- 其他维度不填充  \r\n",
    "\r\n",
    "那么你需要这样写：  \r\n",
    "```python\r\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant',``` constant_values=(..,..))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 定义函数：zero_pad\n",
    "# ==============================\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    对输入的图像批次 X 进行零填充（zero padding）。\n",
    "    填充只作用在图像的高度和宽度维度上。\n",
    "    \n",
    "    参数：\n",
    "    X -- numpy 数组，形状为 (m, n_H, n_W, n_C)\n",
    "         m：样本数量（即图像数量）\n",
    "         n_H：图像高度\n",
    "         n_W：图像宽度\n",
    "         n_C：图像通道数（例如 3 表示 RGB）\n",
    "         \n",
    "    pad -- 整数，每张图像在上下左右的填充像素数\n",
    "    \n",
    "    返回：\n",
    "    X_pad -- 填充后的图像数组，形状为 (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ------------------------------\n",
    "    # 使用 numpy.pad 对图像进行填充\n",
    "    # ------------------------------\n",
    "    X_pad = np.pad(\n",
    "        X,  # 需要填充的数组\n",
    "        \n",
    "        # pad_width 参数：每个维度的填充规则\n",
    "        (\n",
    "            (0, 0),       # 第 0 维：样本数 m，不填充\n",
    "            (pad, pad),   # 第 1 维：图像高度 n_H，上下各填充 pad 个像素\n",
    "            (pad, pad),   # 第 2 维：图像宽度 n_W，左右各填充 pad 个像素\n",
    "            (0, 0)        # 第 3 维：通道数 n_C，不填充\n",
    "        ),\n",
    "        \n",
    "        # mode 参数：填充方式\n",
    "        'constant',  # 表示使用常数值填充\n",
    "        \n",
    "        # constant_values 参数：填充的常数值\n",
    "        constant_values=0  # 用 0 填充，即零填充\n",
    "    )\n",
    "    \n",
    "    # 返回填充后的图像数组\n",
    "    return X_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (4, 7, 7, 2)\n",
      "x[1,1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x257b8bbd0d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================\n",
    "# 设置随机数种子，保证可复现\n",
    "# ==============================\n",
    "np.random.seed(1)  \n",
    "# 设置 NumPy 随机数生成器种子为 1\n",
    "# 确保每次生成的随机数序列相同\n",
    "\n",
    "# ==============================\n",
    "# 生成随机输入数据 x\n",
    "# ==============================\n",
    "x = np.random.randn(4, 3, 3, 2)  \n",
    "# np.random.randn 生成标准正态分布随机数\n",
    "# 形状为 (4, 3, 3, 2)\n",
    "# 4：样本数（即 4 张图像）\n",
    "# 3：图像高度\n",
    "# 3：图像宽度\n",
    "# 2：通道数（每张图像有 2 个通道，例如模拟 RGB 的前两个通道或特征图）\n",
    "\n",
    "# ==============================\n",
    "# 对 x 进行零填充\n",
    "# ==============================\n",
    "x_pad = zero_pad(x, 2)  \n",
    "# 调用之前定义的 zero_pad 函数\n",
    "# pad 参数为 2，表示在图像高度和宽度的每一边都填充 2 个零\n",
    "# 返回填充后的数组 x_pad，形状应为 (4, 3+2*2, 3+2*2, 2) = (4,7,7,2)\n",
    "\n",
    "# ==============================\n",
    "# 打印输入和填充后的形状信息\n",
    "# ==============================\n",
    "print(\"x.shape =\", x.shape)  \n",
    "# 打印原始数组 x 的形状\n",
    "# 期望输出：(4, 3, 3, 2)\n",
    "\n",
    "print(\"x_pad.shape =\", x_pad.shape)  \n",
    "# 打印填充后的数组 x_pad 的形状\n",
    "# 期望输出：(4, 7, 7, 2)\n",
    "\n",
    "# ==============================\n",
    "# 打印部分内容查看\n",
    "# ==============================\n",
    "print(\"x[1,1] =\", x[1,1])  \n",
    "# 打印第 2 张图像（索引 1）第 2 行（索引 1）所有列和通道的数据\n",
    "# 用于查看原始图像的中间部分\n",
    "\n",
    "print(\"x_pad[1,1] =\", x_pad[1,1])  \n",
    "# 打印填充后第 2 张图像（索引 1）第 2 行的数据\n",
    "# 注意由于填充了 2 行零，x_pad 的行索引对应原图像内容会向下、向右偏移\n",
    "# 有助于理解填充效果\n",
    "\n",
    "# ==============================\n",
    "# 可视化原始图像和填充后的图像\n",
    "# ==============================\n",
    "fig, axarr = plt.subplots(1, 2)  \n",
    "# 创建 1 行 2 列的子图\n",
    "# fig 表示整个图像窗口\n",
    "# axarr 是子图数组，axarr[0] 和 axarr[1] 分别对应左图和右图\n",
    "\n",
    "axarr[0].set_title('x')  \n",
    "# 设置左图标题为 'x'（原图像）\n",
    "\n",
    "axarr[0].imshow(x[0,:,:,0])  \n",
    "# 显示第 1 张图像（索引 0）的第 1 个通道（索引 0）\n",
    "# imshow 默认显示二维矩阵为灰度图（之前设置了 plt.rcParams['image.cmap'] = 'gray'）\n",
    "\n",
    "axarr[1].set_title('x_pad')  \n",
    "# 设置右图标题为 'x_pad'（填充后的图像）\n",
    "\n",
    "axarr[1].imshow(x_pad[0,:,:,0])  \n",
    "# 显示第 1 张填充后的图像的第 1 个通道\n",
    "# 便于直观比较填充效果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **x.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (4, 3, 3, 2)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x_pad.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (4, 7, 7, 2)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x[1,1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [[ 0.90085595 -0.68372786]\n",
    " [-0.12289023 -0.93576943]\n",
    " [-0.26788808  0.53035547]]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x_pad[1,1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [[ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - 单步卷积（Single step of convolution）\r\n",
    "\r\n",
    "在这一部分，你将实现卷积的单步操作，即将滤波器应用到输入的单个位置。  \r\n",
    "这个步骤用于构建卷积单元（convolutional unit），该单元会：\r\n",
    "\r\n",
    "- 接收一个输入体（input volume）  \r\n",
    "- 在输入的每个位置应用滤波器  \r\n",
    "- 输出另一个体（通常大小不同）\r\n",
    "\r\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\r\n",
    "<caption><center> <u> <font color='purple'> **图2** </u><font color='purple'> ：**卷积操作** <br> 使用 2x2 的滤波器，步幅为1（stride = 每次滑动窗口的移动量） </center></caption>\r\n",
    "\r\n",
    "在计算机视觉应用中，左侧矩阵中的每个值对应一个像素值。  \r\n",
    "我们用一个 3x3 的滤波器与图像进行卷积：先逐元素相乘，然后将结果求和。  \r\n",
    "在本练习的第一步，你将实现卷积的单步操作，即将滤波器应用到输入的一个位置，从而得到一个单一的实数输出。\r\n",
    "\r\n",
    "在本Notebook后续部分，你将把这个函数应用到输入的多个位置，以实现完整的卷积操作。\r\n",
    "\r\n",
    "**练习**：实现 `conv_single_step()` 函数。  \r\n",
    "[提示](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html)。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 定义函数：conv_single_step\n",
    "# ==============================\n",
    "\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    对输入数据的一个小片段（slice）应用一个卷积核（filter）。\n",
    "    这是卷积操作中的单步计算，用于计算输出特征图的一个位置。\n",
    "\n",
    "    参数：\n",
    "    a_slice_prev -- 输入数据的一个切片，形状为 (f, f, n_C_prev)\n",
    "                    f：卷积核的高度和宽度（假设是正方形）\n",
    "                    n_C_prev：上一层的通道数（输入特征图通道数）\n",
    "    \n",
    "    W -- 卷积核权重参数，形状为 (f, f, n_C_prev)\n",
    "         对应 a_slice_prev 的每个元素都有一个权重\n",
    "    \n",
    "    b -- 偏置参数，形状为 (1, 1, 1)\n",
    "         卷积核对应的偏置，用于每个卷积核输出位置\n",
    "    \n",
    "    返回：\n",
    "    Z -- 标量，卷积结果\n",
    "         对 a_slice_prev 与 W 对应元素相乘并加上偏置后的求和结果\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================\n",
    "    # 对应元素相乘并加偏置\n",
    "    # ==============================\n",
    "    s = np.multiply(a_slice_prev, W) + b\n",
    "    # np.multiply(a_slice_prev, W) 对应位置逐元素相乘\n",
    "    # 再加上偏置 b\n",
    "    # 此时 s 仍然是一个三维矩阵，形状为 (f, f, n_C_prev)\n",
    "\n",
    "    # ==============================\n",
    "    # 对 s 中所有元素求和得到卷积结果\n",
    "    # ==============================\n",
    "    Z = np.sum(s)\n",
    "    # np.sum(s) 将 s 中所有元素累加，得到一个标量\n",
    "    # 这就是卷积在当前位置的输出值\n",
    "\n",
    "    # ==============================\n",
    "    # 返回卷积结果\n",
    "    # ==============================\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -23.16021220252078\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 设置随机数种子，保证可复现\n",
    "# ==============================\n",
    "np.random.seed(1)  \n",
    "# 设置 NumPy 随机数生成器种子为 1\n",
    "# 确保每次生成的随机数序列相同\n",
    "\n",
    "# ==============================\n",
    "# 生成输入数据切片 a_slice_prev\n",
    "# ==============================\n",
    "a_slice_prev = np.random.randn(4, 4, 3)  \n",
    "# np.random.randn 生成标准正态分布随机数\n",
    "# 形状为 (4, 4, 3)\n",
    "# 4x4 表示卷积核大小 f=4\n",
    "# 3 表示输入通道数 n_C_prev=3\n",
    "# 用于模拟上一层输出的一个小片段\n",
    "\n",
    "# ==============================\n",
    "# 生成卷积核权重 W\n",
    "# ==============================\n",
    "W = np.random.randn(4, 4, 3)  \n",
    "# np.random.randn 生成标准正态分布随机数\n",
    "# 形状与 a_slice_prev 相同 (4, 4, 3)\n",
    "# 用于卷积核权重参数\n",
    "\n",
    "# ==============================\n",
    "# 生成卷积偏置 b\n",
    "# ==============================\n",
    "b = np.random.randn(1, 1, 1)  \n",
    "# 偏置参数，形状为 (1, 1, 1)\n",
    "# 每个卷积核有一个对应的偏置值\n",
    "\n",
    "# ==============================\n",
    "# 调用 conv_single_step 进行卷积计算\n",
    "# ==============================\n",
    "Z = conv_single_step(a_slice_prev, W, b)  \n",
    "# 计算卷积核在 a_slice_prev 上的单步输出\n",
    "# 返回标量 Z\n",
    "\n",
    "# ==============================\n",
    "# 打印卷积结果\n",
    "# ==============================\n",
    "print(\"Z =\", Z)  \n",
    "# 输出卷积结果\n",
    "# Z 是一个标量，表示该卷积核在该切片的响应值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z**\n",
    "        </td>\n",
    "        <td>\n",
    "            -23.1602122025\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - 卷积神经网络 - 前向传播（Forward pass）\n",
    "\n",
    "在前向传播中，你将使用多个滤波器对输入进行卷积。  \n",
    "每次卷积都会产生一个二维矩阵输出，然后将这些输出堆叠起来，得到一个三维体（3D volume）：\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "**练习**：实现下面的函数，将滤波器 `W` 应用于输入激活 `A_prev`。  \n",
    "该函数的输入包括：\n",
    "- `A_prev`：上一层输出的激活值（一个批次的 m 个输入）  \n",
    "- F 个滤波器/权重，用 `W` 表示  \n",
    "- 偏置向量 `b`，每个滤波器对应一个单独的偏置  \n",
    "- 超参数字典 `hyperparameters`，包含步幅（stride）和填充（padding）\n",
    "\n",
    "**提示**：\n",
    "1. 要选择矩阵 `a_prev`（形状为 (5,5,3)）左上角的 2x2 切片，可以写：\n",
    "```python\n",
    "a_slice_prev = a_prev[0:2,0:2,:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这在你下面定义 `a_slice_prev` 时会很有用，需要使用你定义的 `start/end` 索引。  \n",
    "2. 要定义 `a_slice`，你需要先定义其四个角的索引：`vert_start`、`vert_end`、`horiz_start` 和 `horiz_end`。  \n",
    "下图有助于你理解如何在代码中使用 h、w、f 和 s 来确定每个角的位置。\n",
    "\n",
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **图3** </u><font color='purple'> ：**使用上下左右起止点定义切片（2x2滤波器）** <br> 图中仅展示单通道。 </center></caption>\n",
    "\n",
    "**提醒**：  \n",
    "卷积输出形状与输入形状的关系公式为：\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor + 1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor + 1 $$\n",
    "$$ n_C = \\text{卷积中使用的滤波器数量} $$\n",
    "\n",
    "在本练习中，我们不考虑向量化实现，而是用 **for 循环** 完成所有操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 定义函数：conv_forward\n",
    "# ==============================\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    实现卷积层的前向传播（forward propagation）\n",
    "    \n",
    "    参数：\n",
    "    A_prev -- 上一层输出的激活值，形状为 (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "              m：样本数\n",
    "              n_H_prev：上一层图像高度\n",
    "              n_W_prev：上一层图像宽度\n",
    "              n_C_prev：上一层通道数\n",
    "    \n",
    "    W -- 卷积核权重，形状为 (f, f, n_C_prev, n_C)\n",
    "         f：卷积核大小（假设为正方形）\n",
    "         n_C：输出通道数（卷积核个数）\n",
    "    \n",
    "    b -- 偏置，形状为 (1, 1, 1, n_C)\n",
    "    \n",
    "    hparameters -- 字典，包含卷积超参数\n",
    "                   \"stride\"：步长\n",
    "                   \"pad\"：填充数\n",
    "    \n",
    "    返回：\n",
    "    Z -- 卷积输出，形状为 (m, n_H, n_W, n_C)\n",
    "    cache -- 存储前向传播需要的值，用于反向传播\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================\n",
    "    # 获取输入数据的基本信息\n",
    "    # ==============================\n",
    "    (m , n_H_prev , n_W_prev , n_C_prev) = A_prev.shape\n",
    "    # m：样本数\n",
    "    # n_H_prev：上一层高度\n",
    "    # n_W_prev：上一层宽度\n",
    "    # n_C_prev：上一层通道数\n",
    "\n",
    "    # ==============================\n",
    "    # 获取权重矩阵的基本信息\n",
    "    # ==============================\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    # f：卷积核高度和宽度\n",
    "    # n_C_prev：输入通道数（与上一层通道一致）\n",
    "    # n_C：输出通道数（卷积核个数）\n",
    "\n",
    "    # ==============================\n",
    "    # 获取卷积超参数\n",
    "    # ==============================\n",
    "    stride = hparameters[\"stride\"]  # 步长\n",
    "    pad = hparameters[\"pad\"]        # 填充数\n",
    "\n",
    "    # ==============================\n",
    "    # 计算卷积输出的高度和宽度\n",
    "    # ==============================\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1  \n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1  \n",
    "    # 公式：n_H = (n_H_prev - f + 2*pad)/stride + 1\n",
    "    #      n_W = (n_W_prev - f + 2*pad)/stride + 1\n",
    "    # int() 用于向下取整，保证输出为整数\n",
    "\n",
    "    # ==============================\n",
    "    # 初始化卷积输出 Z\n",
    "    # ==============================\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))  \n",
    "    # 形状为 (样本数, 输出高度, 输出宽度, 输出通道数)\n",
    "\n",
    "    # ==============================\n",
    "    # 对输入数据进行零填充\n",
    "    # ==============================\n",
    "    A_prev_pad = zero_pad(A_prev, pad)  \n",
    "    # 调用之前定义的 zero_pad 函数\n",
    "\n",
    "    # ==============================\n",
    "    # 卷积计算\n",
    "    # ==============================\n",
    "    for i in range(m):  # 遍历每个样本\n",
    "        a_prev_pad = A_prev_pad[i]  # 第 i 个样本的填充激活矩阵\n",
    "        \n",
    "        for h in range(n_H):  # 遍历输出高度\n",
    "            for w in range(n_W):  # 遍历输出宽度\n",
    "                for c in range(n_C):  # 遍历输出通道（卷积核）\n",
    "                    \n",
    "                    # ==============================\n",
    "                    # 定位当前切片的位置\n",
    "                    # ==============================\n",
    "                    vert_start = h * stride        # 垂直开始位置\n",
    "                    vert_end = vert_start + f      # 垂直结束位置\n",
    "                    horiz_start = w * stride       # 水平开始位置\n",
    "                    horiz_end = horiz_start + f    # 水平结束位置\n",
    "                    \n",
    "                    # ==============================\n",
    "                    # 取出当前切片\n",
    "                    # ==============================\n",
    "                    a_slice_prev = a_prev_pad[\n",
    "                        vert_start:vert_end, \n",
    "                        horiz_start:horiz_end, \n",
    "                        :\n",
    "                    ]  \n",
    "                    # 形状为 (f, f, n_C_prev)\n",
    "                    # 注意这里“穿透”取出所有通道的数据\n",
    "\n",
    "                    # ==============================\n",
    "                    # 执行单步卷积\n",
    "                    # ==============================\n",
    "                    Z[i, h, w, c] = conv_single_step(\n",
    "                        a_slice_prev, \n",
    "                        W[:, :, :, c], \n",
    "                        b[0, 0, 0, c]\n",
    "                    )  \n",
    "                    # conv_single_step 返回一个标量，存入 Z 的对应位置\n",
    "\n",
    "    # ==============================\n",
    "    # 确认输出形状正确\n",
    "    # ==============================\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    # ==============================\n",
    "    # 缓存前向传播需要的数据，用于反向传播\n",
    "    # ==============================\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "\n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.15585932488906465\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 设置随机数种子，保证可复现\n",
    "# ==============================\n",
    "np.random.seed(1)  \n",
    "# 设置 NumPy 随机数生成器种子为 1\n",
    "# 确保每次生成的随机数序列相同\n",
    "\n",
    "# ==============================\n",
    "# 生成上一层的激活输出 A_prev\n",
    "# ==============================\n",
    "A_prev = np.random.randn(10, 4, 4, 3)  \n",
    "# np.random.randn 生成标准正态分布随机数\n",
    "# 形状为 (10, 4, 4, 3)\n",
    "# 10：样本数 m\n",
    "# 4：上一层图像高度 n_H_prev\n",
    "# 4：上一层图像宽度 n_W_prev\n",
    "# 3：上一层通道数 n_C_prev\n",
    "\n",
    "# ==============================\n",
    "# 生成卷积核权重 W\n",
    "# ==============================\n",
    "W = np.random.randn(2, 2, 3, 8)  \n",
    "# 形状为 (f, f, n_C_prev, n_C)\n",
    "# f = 2：卷积核高度和宽度\n",
    "# n_C_prev = 3：输入通道数\n",
    "# n_C = 8：输出通道数（卷积核数量）\n",
    "\n",
    "# ==============================\n",
    "# 生成卷积偏置 b\n",
    "# ==============================\n",
    "b = np.random.randn(1, 1, 1, 8)  \n",
    "# 形状为 (1, 1, 1, n_C)\n",
    "# 每个卷积核对应一个偏置\n",
    "\n",
    "# ==============================\n",
    "# 定义卷积超参数 hparameters\n",
    "# ==============================\n",
    "hparameters = {\n",
    "    \"pad\": 2,     # 填充数为 2\n",
    "    \"stride\": 1   # 步长为 1\n",
    "}\n",
    "\n",
    "# ==============================\n",
    "# 调用 conv_forward 进行卷积前向传播\n",
    "# ==============================\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "# Z：卷积输出，形状为 (10, n_H, n_W, 8)\n",
    "# cache_conv：存储前向传播中用到的数据，用于反向传播\n",
    "\n",
    "# ==============================\n",
    "# 打印卷积输出的平均值\n",
    "# ==============================\n",
    "print(\"Z's mean =\", np.mean(Z))  \n",
    "# np.mean(Z) 计算 Z 中所有元素的平均值\n",
    "# 用于简单验证卷积输出\n",
    "\n",
    "# ==============================\n",
    "# 打印 cache 中部分数据\n",
    "# ==============================\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])  \n",
    "# cache_conv[0] = A_prev\n",
    "# 打印 A_prev 第 1 个样本（索引 0）第 2 行（索引 1）第 3 列（索引 2）第 4 个通道（索引 3）的值\n",
    "# 用于检查缓存的数据是否正确\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z's mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.155859324889\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cache_conv[0][1][2][3]**\n",
    "        </td>\n",
    "        <td>\n",
    "            [-0.20075807  0.18656139  0.41005165]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，卷积层（CONV layer）通常还会包含一个激活函数，这时可以添加如下代码：\r\n",
    "\r\n",
    "```python\r\n",
    "# 对窗口进行卷积，得到一个输出神经元\r\n",
    "Z[i, h, w, c] = ...\r\n",
    "# 应用激活函数\r\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\r",
    "在此处，你无需实现激活函数。 here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 池化层（Pooling layer）\r\n",
    "\r\n",
    "池化（POOL）层用于减小输入的高度和宽度。  \r\n",
    "它有助于减少计算量，同时使特征检测器对输入位置更具不变性。  \r\n",
    "池化层的两种类型如下：\r\n",
    "\r\n",
    "- **最大池化层（Max-pooling layer）**：在输入上滑动一个 $(f, f)$ 的窗口，将窗口内的最大值存储到输出中。  \r\n",
    "- **平均池化层（Average-pooling layer）**：在输入上滑动一个 $(f, f)$ 的窗口，将窗口内的平均值存储到输出中。\r\n",
    "\r\n",
    "<table>\r\n",
    "<td>\r\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\r\n",
    "<td>\r\n",
    "\r\n",
    "<td>\r\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\r\n",
    "<td>\r\n",
    "</table>\r\n",
    "\r\n",
    "这些池化层没有可训练的参数，因此无需在反向传播中更新参数。  \r\n",
    "但是，它们有超参数，例如窗口大小 $f$，用于指定在多大范围内计算最大值或平均值。\r\n",
    "\r\n",
    "### 4.1 - 池化层前向传播（Forward Pooling）\r\n",
    "\r\n",
    "现在，你将实现 **MAX-POOL** 和 **AVG-POOL**，在同一个函数中实现两者。\r\n",
    "\r\n",
    "**练习**：实现池化层的前向传播。请参考下面注释中的提示。\r\n",
    "\r\n",
    "**提醒**：\r\n",
    "由于池化层没有填充，其输出形状与输入形状的关系公式为：\r\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor + 1 $$\r\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor + 1 $$\r\n",
    "$$ n_C = n_{C_{prev}} $$\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 定义函数：pool_forward\n",
    "# ==============================\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    实现池化层的前向传播（forward pass）\n",
    "    \n",
    "    参数：\n",
    "    A_prev -- 输入数据，形状为 (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "              m：样本数\n",
    "              n_H_prev：输入高度\n",
    "              n_W_prev：输入宽度\n",
    "              n_C_prev：输入通道数\n",
    "    \n",
    "    hparameters -- 字典，包含池化层超参数\n",
    "                   \"f\"：池化窗口大小\n",
    "                   \"stride\"：步长\n",
    "    \n",
    "    mode -- 池化方式，字符串 \"max\" 或 \"average\"\n",
    "            \"max\" 表示最大池化\n",
    "            \"average\" 表示平均池化\n",
    "    \n",
    "    返回：\n",
    "    A -- 池化输出，形状为 (m, n_H, n_W, n_C)\n",
    "    cache -- 缓存，用于池化层的反向传播，包含输入和超参数\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================\n",
    "    # 获取输入数据的维度信息\n",
    "    # ==============================\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    # m：样本数\n",
    "    # n_H_prev：输入高度\n",
    "    # n_W_prev：输入宽度\n",
    "    # n_C_prev：输入通道数\n",
    "\n",
    "    # ==============================\n",
    "    # 获取池化超参数\n",
    "    # ==============================\n",
    "    f = hparameters[\"f\"]          # 池化窗口大小\n",
    "    stride = hparameters[\"stride\"]  # 步长\n",
    "\n",
    "    # ==============================\n",
    "    # 计算输出维度\n",
    "    # ==============================\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)  # 输出高度\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)  # 输出宽度\n",
    "    n_C = n_C_prev                           # 输出通道数与输入相同\n",
    "\n",
    "    # ==============================\n",
    "    # 初始化输出矩阵 A\n",
    "    # ==============================\n",
    "    A = np.zeros((m, n_H, n_W, n_C))  \n",
    "    # 形状为 (样本数, 输出高度, 输出宽度, 通道数)\n",
    "\n",
    "    # ==============================\n",
    "    # 遍历每个样本和输出位置\n",
    "    # ==============================\n",
    "    for i in range(m):  # 遍历样本\n",
    "        for h in range(n_H):  # 遍历输出高度\n",
    "            for w in range(n_W):  # 遍历输出宽度\n",
    "                for c in range(n_C):  # 遍历通道\n",
    "                    \n",
    "                    # ==============================\n",
    "                    # 定位当前池化窗口位置\n",
    "                    # ==============================\n",
    "                    vert_start = h * stride        # 垂直开始位置\n",
    "                    vert_end = vert_start + f      # 垂直结束位置\n",
    "                    horiz_start = w * stride       # 水平开始位置\n",
    "                    horiz_end = horiz_start + f    # 水平结束位置\n",
    "\n",
    "                    # ==============================\n",
    "                    # 提取当前窗口切片\n",
    "                    # ==============================\n",
    "                    a_slice_prev = A_prev[\n",
    "                        i, \n",
    "                        vert_start:vert_end, \n",
    "                        horiz_start:horiz_end, \n",
    "                        c\n",
    "                    ]  \n",
    "                    # 形状为 (f, f)\n",
    "                    # 只取当前通道的数据\n",
    "\n",
    "                    # ==============================\n",
    "                    # 执行池化操作\n",
    "                    # ==============================\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_slice_prev)  # 最大池化\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_slice_prev)  # 平均池化\n",
    "\n",
    "    # ==============================\n",
    "    # 校验输出形状\n",
    "    # ==============================\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    # ==============================\n",
    "    # 缓存输入和超参数，用于反向传播\n",
    "    # ==============================\n",
    "    cache = (A_prev, hparameters)\n",
    "\n",
    "    # ==============================\n",
    "    # 再次确认输出形状\n",
    "    # ==============================\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[1.74481176 1.6924546  2.10025514]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 1.51981682 2.18557541]]]]\n",
      "\n",
      "mode = average\n",
      "A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n",
      "\n",
      "\n",
      " [[[-0.09525108  0.28325018  0.33035185]]]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 设置随机数种子，保证可复现\n",
    "# ==============================\n",
    "np.random.seed(1)  \n",
    "# 设置 NumPy 随机数生成器种子为 1\n",
    "# 确保每次生成的随机数序列相同\n",
    "\n",
    "# ==============================\n",
    "# 生成上一层输出 A_prev\n",
    "# ==============================\n",
    "A_prev = np.random.randn(2, 4, 4, 3)  \n",
    "# 形状为 (2, 4, 4, 3)\n",
    "# 2：样本数 m\n",
    "# 4：输入高度 n_H_prev\n",
    "# 4：输入宽度 n_W_prev\n",
    "# 3：输入通道数 n_C_prev\n",
    "\n",
    "# ==============================\n",
    "# 定义池化超参数\n",
    "# ==============================\n",
    "hparameters = {\n",
    "    \"stride\": 1,  # 步长\n",
    "    \"f\": 4        # 池化窗口大小\n",
    "}\n",
    "\n",
    "# ==============================\n",
    "# 调用 pool_forward 执行最大池化\n",
    "# ==============================\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "# 默认 mode=\"max\"，执行最大池化\n",
    "# 返回池化输出 A 和缓存 cache\n",
    "\n",
    "# ==============================\n",
    "# 打印最大池化结果\n",
    "# ==============================\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "# 输出池化结果 A\n",
    "# 由于池化窗口大小 f=4 且步长 stride=1，输出每个样本每个通道都是窗口内的最大值\n",
    "\n",
    "print()\n",
    "\n",
    "# ==============================\n",
    "# 调用 pool_forward 执行平均池化\n",
    "# ==============================\n",
    "A, cache = pool_forward(A_prev, hparameters, mode=\"average\")\n",
    "# mode=\"average\"，执行平均池化\n",
    "# 返回池化输出 A 和缓存 cache\n",
    "\n",
    "# ==============================\n",
    "# 打印平均池化结果\n",
    "# ==============================\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)\n",
    "# 输出池化结果 A\n",
    "# 每个输出值是窗口内所有元素的平均值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜！你现在已经实现了卷积神经网络中所有层的前向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - 卷积神经网络中的反向传播\n",
    "\n",
    "在现代深度学习框架中，你通常只需要实现前向传播，框架会自动处理反向传播，因此大多数深度学习工程师无需关注反向传播的细节。  \n",
    "卷积神经网络的反向传播相对复杂，但如果你愿意，可以通过本Notebook的可选部分，了解卷积网络中的反向传播是如何工作的。\n",
    "\n",
    "在前面的课程中，当你实现一个简单的（全连接）神经网络时，你使用反向传播计算损失函数关于参数的导数，以更新参数。  \n",
    "类似地，在卷积神经网络中，你也需要计算损失函数关于参数的导数以更新参数。  \n",
    "反向传播的公式比较复杂，课程中未推导，但我们在下面简单介绍。\n",
    "\n",
    "### 5.1 - 卷积层反向传播\n",
    "\n",
    "我们先实现卷积层（CONV layer）的反向传播。\n",
    "\n",
    "#### 5.1.1 - 计算 dA\n",
    "\n",
    "对于某个滤波器 $W_c$ 和给定训练样本，计算 $dA$ 的公式为：\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "其中 $W_c$ 是滤波器，$dZ_{hw}$ 是卷积层输出 $Z$ 在第 h 行、第 w 列的梯度（对应在第 i 个步幅向左、第 j 个步幅向下时的点积结果）。  \n",
    "注意，每次更新 $dA$ 时，我们用相同的滤波器 $W_c$ 乘以不同的 $dZ$。  \n",
    "这是因为在前向传播中，每个滤波器与不同的切片（a_slice）做点积求和。  \n",
    "因此在计算 dA 的反向传播时，我们只需将所有切片的梯度累加。\n",
    "\n",
    "在代码中，对应的 for 循环内实现为：\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 - 计算 dW\r\n",
    "\r\n",
    "对于某个滤波器 $W_c$，计算 $dW_c$（单个滤波器的导数）相对于损失的公式为：\r\n",
    "\r\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\r\n",
    "\r\n",
    "其中 $a_{slice}$ 对应生成激活值 $Z_{ij}$ 时使用的输入切片。  \r\n",
    "因此，该公式得到该切片对滤波器 $W$ 的梯度。  \r\n",
    "由于是同一个滤波器 $W$，我们只需将所有切片的梯度累加得到 $dW$。\r\n",
    "\r\n",
    "在代码中，对应的 for 循环内实现为：\r\n",
    "```python\r\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 - 计算 db\r\n",
    "\r\n",
    "对于某个滤波器 $W_c$，计算 $db$ 相对于损失的公式为：\r\n",
    "\r\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\r\n",
    "\r\n",
    "正如你在基础神经网络中看到的，db 是通过对 $dZ$ 求和得到的。  \r\n",
    "在这里，你只需对卷积层输出 $Z$ 关于损失的所有梯度求和即可。\r\n",
    "\r\n",
    "在代码中，对应的 for 循环内实现为：\r\n",
    "```python\r\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：实现下面的 `conv_backward` 函数。  \n",
    "你需要对所有训练样本、滤波器、高度和宽度进行求和，然后使用上面公式（1、2、3）计算各个导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 定义函数：conv_backward\n",
    "# ==============================\n",
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    实现卷积层的反向传播（backward propagation）\n",
    "    \n",
    "    参数：\n",
    "    dZ -- 卷积层输出 Z 对损失函数的梯度，形状为 (m, n_H, n_W, n_C)\n",
    "          m：样本数\n",
    "          n_H：卷积输出高度\n",
    "          n_W：卷积输出宽度\n",
    "          n_C：卷积输出通道数\n",
    "    \n",
    "    cache -- 前向传播的缓存，包含 (A_prev, W, b, hparameters)\n",
    "    \n",
    "    返回：\n",
    "    dA_prev -- 卷积层输入 A_prev 对损失函数的梯度，形状为 (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- 卷积核权重 W 对损失函数的梯度，形状为 (f, f, n_C_prev, n_C)\n",
    "    db -- 卷积核偏置 b 对损失函数的梯度，形状为 (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================\n",
    "    # 获取缓存中的前向传播参数\n",
    "    # ==============================\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "\n",
    "    # ==============================\n",
    "    # 获取输入数据 A_prev 的维度\n",
    "    # ==============================\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # ==============================\n",
    "    # 获取 dZ 的维度\n",
    "    # ==============================\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "\n",
    "    # ==============================\n",
    "    # 获取卷积核权重 W 的维度\n",
    "    # ==============================\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    # ==============================\n",
    "    # 获取卷积超参数\n",
    "    # ==============================\n",
    "    pad = hparameters[\"pad\"]       # 填充数\n",
    "    stride = hparameters[\"stride\"] # 步长\n",
    "\n",
    "    # ==============================\n",
    "    # 初始化梯度矩阵\n",
    "    # ==============================\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))  # 输入梯度\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))                   # 权重梯度\n",
    "    db = np.zeros((1, 1, 1, n_C))                          # 偏置梯度\n",
    "\n",
    "    # ==============================\n",
    "    # 对 A_prev 和 dA_prev 进行零填充\n",
    "    # ==============================\n",
    "    A_prev_pad = zero_pad(A_prev, pad)        # 填充输入\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)     # 填充输入梯度\n",
    "\n",
    "    # ==============================\n",
    "    # 遍历每个样本\n",
    "    # ==============================\n",
    "    for i in range(m):\n",
    "        # 取第 i 个样本的填充输入和填充梯度\n",
    "        a_prev_pad = A_prev_pad[i]  # 第 i 个样本，填充后的 A_prev\n",
    "        da_prev_pad = dA_prev_pad[i]  # 第 i 个样本，填充后的 dA_prev\n",
    "\n",
    "        # ==============================\n",
    "        # 遍历卷积输出的每个位置\n",
    "        # ==============================\n",
    "        for h in range(n_H):        # 输出高度\n",
    "            for w in range(n_W):    # 输出宽度\n",
    "                for c in range(n_C):  # 输出通道\n",
    "                    \n",
    "                    # ==============================\n",
    "                    # 定位卷积窗口在输入中的位置\n",
    "                    # ==============================\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # ==============================\n",
    "                    # 提取输入切片\n",
    "                    # ==============================\n",
    "                    a_slice = a_prev_pad[\n",
    "                        vert_start:vert_end,\n",
    "                        horiz_start:horiz_end,\n",
    "                        :\n",
    "                    ]  # 形状 (f, f, n_C_prev)\n",
    "\n",
    "                    # ==============================\n",
    "                    # 计算梯度\n",
    "                    # ==============================\n",
    "                    # 1) dA_prev_pad 累加卷积核梯度\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]\n",
    "\n",
    "                    # 2) dW 累加输入切片梯度\n",
    "                    dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
    "\n",
    "                    # 3) db 累加 dZ\n",
    "                    db[:, :, :, c] += dZ[i, h, w, c]\n",
    "\n",
    "        # ==============================\n",
    "        # 去掉填充，得到第 i 个样本的最终 dA_prev\n",
    "        # ==============================\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "\n",
    "    # ==============================\n",
    "    # 校验输出形状\n",
    "    # ==============================\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.608990675868995\n",
      "dW_mean = 10.581741275547566\n",
      "db_mean = 76.37106919563735\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 设置随机数种子，保证可复现\n",
    "# ==============================\n",
    "np.random.seed(1)\n",
    "# 设置 NumPy 随机数生成器种子为 1\n",
    "# 确保每次生成的随机数序列相同\n",
    "\n",
    "# ==============================\n",
    "# 调用 conv_backward 执行卷积反向传播\n",
    "# ==============================\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "# 参数说明：\n",
    "# Z：卷积层前向传播的输出，用作 dZ（假设损失对 Z 的梯度为 Z 本身，这里便于测试）\n",
    "# cache_conv：conv_forward 返回的缓存，包含 (A_prev, W, b, hparameters)\n",
    "# 返回：\n",
    "# dA：输入 A_prev 的梯度，形状 (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "# dW：卷积核权重 W 的梯度，形状 (f, f, n_C_prev, n_C)\n",
    "# db：偏置 b 的梯度，形状 (1, 1, 1, n_C)\n",
    "\n",
    "# ==============================\n",
    "# 打印输入梯度的平均值\n",
    "# ==============================\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "# np.mean(dA)：计算 dA 中所有元素的平均值\n",
    "# 用于快速检查梯度范围是否合理\n",
    "\n",
    "# ==============================\n",
    "# 打印卷积核梯度的平均值\n",
    "# ==============================\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "# np.mean(dW)：计算 dW 中所有元素的平均值\n",
    "# 用于检查权重梯度是否正常\n",
    "\n",
    "# ==============================\n",
    "# 打印偏置梯度的平均值\n",
    "# ==============================\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "# np.mean(db)：计算 db 中所有元素的平均值\n",
    "# 用于检查偏置梯度是否合理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 池化层 - 反向传播（backward pass）\n",
    "\n",
    "接下来，我们实现池化层的反向传播，从 **最大池化（MAX-POOL）** 层开始。  \n",
    "虽然池化层没有可训练参数，但仍需要将梯度反向传播通过池化层，以便计算前一层的梯度。\n",
    "\n",
    "### 5.2.1 最大池化（Max pooling） - 反向传播\n",
    "\n",
    "在实现池化层的反向传播之前，你需要先实现一个辅助函数 `create_mask_from_window()`，其作用如下：\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "如图所示，该函数创建了一个“掩码（mask）”矩阵，用于记录输入矩阵最大值的位置。  \n",
    "值为 True（1）表示最大值的位置，其余为 False（0）。  \n",
    "稍后你会看到，平均池化的反向传播类似，但使用的掩码不同。\n",
    "\n",
    "**练习**：实现 `create_mask_from_window()` 函数。  \n",
    "该函数将用于池化层的反向传播。\n",
    "\n",
    "**提示**：\n",
    "- [np.max()]() 可以用于计算数组的最大值。  \n",
    "- 如果有一个矩阵 X 和一个标量 x：\n",
    "```python\n",
    "A = (X == x)\n",
    "\n",
    "会返回一个与 X 相同大小的矩阵 A，其中：\n",
    "\n",
    "A[i,j] = True   如果 X[i,j] = x\r\n",
    "A[i,j] = False  如果 X[i,j] != x\n",
    "\n",
    "这里你不需要考虑矩阵中存在多个最大值的情况。\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 定义函数：create_mask_from_window\n",
    "# ==============================\n",
    "\n",
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    根据输入矩阵 x 创建一个掩码（mask），标识 x 中的最大值位置。\n",
    "    \n",
    "    参数：\n",
    "    x -- 输入矩阵，形状为 (f, f)\n",
    "         f：池化窗口大小\n",
    "    \n",
    "    返回：\n",
    "    mask -- 与输入 x 形状相同的布尔矩阵\n",
    "            在最大值的位置为 True，其余位置为 False\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================\n",
    "    # 找出 x 中最大值的位置\n",
    "    # ==============================\n",
    "    mask = x == np.max(x)\n",
    "    # np.max(x)：计算 x 中的最大值\n",
    "    # x == np.max(x)：返回一个布尔矩阵，最大值位置为 True，其余为 False\n",
    "    # 注意：如果有多个最大值，mask 中对应位置都会是 True\n",
    "\n",
    "    # ==============================\n",
    "    # 返回掩码\n",
    "    # ==============================\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 设置随机数种子，保证可复现\n",
    "# ==============================\n",
    "np.random.seed(1)\n",
    "# 设置 NumPy 随机数生成器种子为 1\n",
    "# 确保每次生成的随机数序列相同\n",
    "\n",
    "# ==============================\n",
    "# 生成随机矩阵 x\n",
    "# ==============================\n",
    "x = np.random.randn(2, 3)\n",
    "# 生成一个 2×3 的随机矩阵，值服从标准正态分布 N(0,1)\n",
    "# 作为池化窗口的示例输入\n",
    "\n",
    "# ==============================\n",
    "# 调用 create_mask_from_window 创建掩码\n",
    "# ==============================\n",
    "mask = create_mask_from_window(x)\n",
    "# mask 与 x 形状相同\n",
    "# True 表示 x 中最大值的位置，其余位置为 False\n",
    "\n",
    "# ==============================\n",
    "# 打印输入矩阵 x\n",
    "# ==============================\n",
    "print('x = ', x)\n",
    "# 显示池化窗口矩阵，方便观察最大值位置\n",
    "\n",
    "# ==============================\n",
    "# 打印掩码矩阵 mask\n",
    "# ==============================\n",
    "print(\"mask = \", mask)\n",
    "# 显示布尔矩阵 mask\n",
    "# True 对应 x 中的最大值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么我们要记录最大值的位置？  \r\n",
    "这是因为这个位置的输入值最终影响了输出，也就影响了损失（cost）。  \r\n",
    "\r\n",
    "反向传播（backprop）是计算损失函数相对于输入和参数的梯度，因此任何影响最终损失的值都应该有非零梯度。  \r\n",
    "因此，反向传播会将梯度“传回”到那个对损失有影响的特定输入值。\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - 平均池化（Average pooling） - 反向传播\r\n",
    "\r\n",
    "在最大池化中，每个输入窗口对输出的“影响”只来自单个输入值——最大值。  \r\n",
    "而在平均池化中，输入窗口的每个元素对输出的影响相等。  \r\n",
    "因此，在实现反向传播时，需要实现一个辅助函数，将梯度均匀分配到输入窗口中。\r\n",
    "\r\n",
    "例如，如果在前向传播中使用 2x2 的平均池化滤波器，则反向传播时的掩码应为：\r\n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\r\n",
    "1/4 && 1/4 \\\\\r\n",
    "1/4 && 1/4\r\n",
    "\\end{bmatrix}\\tag{5}$$\r\n",
    "\r\n",
    "这意味着 $dZ$ 矩阵的每个位置对输出贡献相等，因为在前向传播时我们取了平均值。\r\n",
    "\r\n",
    "**练习**：实现下面的函数，将值 dz 平均分配到指定形状的矩阵中。  \r\n",
    "[提示](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 定义函数：distribute_value\n",
    "# ==============================\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    将输入的标量 dz 平均分配到一个指定形状的矩阵中\n",
    "    \n",
    "    参数：\n",
    "    dz -- 输入标量，表示要分配的梯度值\n",
    "    shape -- 目标矩阵的形状 (n_H, n_W)，表示池化窗口大小\n",
    "    \n",
    "    返回：\n",
    "    a -- 分配完成的矩阵，形状为 (n_H, n_W)\n",
    "         每个元素的值为 dz / (n_H * n_W)\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================\n",
    "    # 获取矩阵的高度和宽度\n",
    "    # ==============================\n",
    "    (n_H, n_W) = shape\n",
    "    # n_H：矩阵高度\n",
    "    # n_W：矩阵宽度\n",
    "\n",
    "    # ==============================\n",
    "    # 计算每个位置分配的平均值\n",
    "    # ==============================\n",
    "    average = dz / (n_H * n_W)\n",
    "    # dz：梯度标量\n",
    "    # 将 dz 平均分配到 n_H * n_W 个位置上\n",
    "\n",
    "    # ==============================\n",
    "    # 创建矩阵并填充平均值\n",
    "    # ==============================\n",
    "    a = np.ones(shape) * average\n",
    "    # np.ones(shape)：创建一个全为 1 的矩阵\n",
    "    # 乘以 average，将每个元素赋值为平均梯度值\n",
    "\n",
    "    # ==============================\n",
    "    # 返回分配后的矩阵\n",
    "    # ==============================\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 调用 distribute_value 测试函数\n",
    "# ==============================\n",
    "a = distribute_value(2, (2, 2))\n",
    "# dz = 2：要分配的梯度标量\n",
    "# shape = (2, 2)：目标矩阵形状为 2×2\n",
    "# 返回的 a 是一个 2×2 矩阵，每个元素都等于 2 / (2*2) = 0.5\n",
    "\n",
    "# ==============================\n",
    "# 打印分配后的矩阵\n",
    "# ==============================\n",
    "print('distributed value =', a)\n",
    "# 输出结果：\n",
    "# distributed value = [[0.5 0.5]\n",
    "#                      [0.5 0.5]]\n",
    "# 可以直观看到标量 dz 被平均分配到 2×2 矩阵的每个位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 综合：池化层反向传播（Pooling backward）\r\n",
    "\r\n",
    "现在你已经具备了计算池化层反向传播所需的所有工具。\r\n",
    "\r\n",
    "**练习**：实现 `pool_backward` 函数，支持两种模式（`\"max\"` 和 `\"average\"`）。  \r\n",
    "你将再次使用 4 个 for 循环（遍历训练样本、高度、宽度和通道）。  \r\n",
    "使用 `if/elif` 判断模式：\r\n",
    "- 如果模式为 `'average'`，使用上面实现的 `distribute_value()` 函数生成与 `a_slice` 同形状的矩阵，并均匀分配梯度。  \r\n",
    "- 否则模式为 `'max'`，使用 `create_mask_from_window()` 创建掩码，并将其乘以对应的 `dZ` 值。\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 定义函数：pool_backward\n",
    "# ==============================\n",
    "\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    实现池化层的反向传播\n",
    "    \n",
    "    参数：\n",
    "    dA -- 池化层输出 A 的梯度，形状与前向传播输出 A 相同\n",
    "    cache -- 前向传播缓存，包含 (A_prev, hparameters)\n",
    "    mode -- 池化类型，可选 \"max\" 或 \"average\"\n",
    "    \n",
    "    返回：\n",
    "    dA_prev -- 池化层输入 A_prev 的梯度，形状与 A_prev 相同\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================\n",
    "    # 获取缓存中的前向传播数据\n",
    "    # ==============================\n",
    "    (A_prev, hparameters) = cache\n",
    "    # A_prev：前向传播的输入\n",
    "    # hparameters：池化层的超参数字典，包含 f（窗口大小）和 stride（步长）\n",
    "\n",
    "    # ==============================\n",
    "    # 获取池化超参数\n",
    "    # ==============================\n",
    "    f = hparameters[\"f\"]       # 池化窗口大小\n",
    "    stride = hparameters[\"stride\"]  # 步长\n",
    "\n",
    "    # ==============================\n",
    "    # 获取输入和梯度的基本信息\n",
    "    # ==============================\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    # m：样本数，n_H_prev：输入高度，n_W_prev：输入宽度，n_C_prev：输入通道数\n",
    "\n",
    "    (m, n_H, n_W, n_C) = dA.shape\n",
    "    # n_H, n_W, n_C：池化输出的高度、宽度和通道数\n",
    "\n",
    "    # ==============================\n",
    "    # 初始化输入梯度矩阵\n",
    "    # ==============================\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    # 形状与 A_prev 相同，用于存储池化反向传播的梯度\n",
    "\n",
    "    # ==============================\n",
    "    # 遍历每个样本\n",
    "    # ==============================\n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]  # 第 i 个样本输入\n",
    "\n",
    "        # ==============================\n",
    "        # 遍历输出高度和宽度\n",
    "        # ==============================\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):  # 遍历通道\n",
    "\n",
    "                    # ==============================\n",
    "                    # 定位池化窗口在输入中的位置\n",
    "                    # ==============================\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # ==============================\n",
    "                    # 根据池化类型计算梯度\n",
    "                    # ==============================\n",
    "                    if mode == \"max\":\n",
    "                        # ------------------------------\n",
    "                        # 最大池化反向传播\n",
    "                        # ------------------------------\n",
    "                        # 取池化窗口的输入切片\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # 创建掩码，标记最大值位置\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # 将上游梯度 dA[i,h,w,c] 只传回最大值位置\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
    "\n",
    "                    elif mode == \"average\":\n",
    "                        # ------------------------------\n",
    "                        # 平均池化反向传播\n",
    "                        # ------------------------------\n",
    "                        da = dA[i, h, w, c]  # 上游梯度\n",
    "                        shape = (f, f)       # 池化窗口大小\n",
    "                        # 平均分配梯度到窗口中的每个位置\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "\n",
    "    # ==============================\n",
    "    # 校验输出形状\n",
    "    # ==============================\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "\n",
    "    return dA_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 设置随机数种子，保证可复现\n",
    "# ==============================\n",
    "np.random.seed(1)\n",
    "# 设置 NumPy 随机数生成器种子为 1\n",
    "# 确保每次生成的随机数序列相同\n",
    "\n",
    "# ==============================\n",
    "# 生成池化层前向传播输入\n",
    "# ==============================\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "# 生成形状为 (5,5,3,2) 的随机数组\n",
    "# 5 个样本，高度 5，宽度 3，通道数 2\n",
    "\n",
    "# ==============================\n",
    "# 定义池化层超参数\n",
    "# ==============================\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "# stride = 1：池化窗口滑动步长\n",
    "# f = 2：池化窗口大小 2×2\n",
    "\n",
    "# ==============================\n",
    "# 前向传播池化层\n",
    "# ==============================\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "# A：池化层输出\n",
    "# cache：前向传播缓存 (A_prev, hparameters)，用于反向传播\n",
    "\n",
    "# ==============================\n",
    "# 构造随机的上游梯度 dA\n",
    "# ==============================\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "# 形状与池化层输出 A 相同\n",
    "# 作为池化反向传播的梯度输入\n",
    "\n",
    "# ==============================\n",
    "# 池化反向传播：最大池化\n",
    "# ==============================\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "# 使用最大池化反向传播\n",
    "# dA_prev：池化层输入 A_prev 的梯度，形状与 A_prev 相同\n",
    "\n",
    "# ==============================\n",
    "# 打印最大池化结果\n",
    "# ==============================\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "# np.mean(dA)：上游梯度 dA 的平均值\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])\n",
    "# 打印第 1 个样本，第 1 行的梯度矩阵，便于观察梯度分布\n",
    "\n",
    "print()\n",
    "\n",
    "# ==============================\n",
    "# 池化反向传播：平均池化\n",
    "# ==============================\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "# 使用平均池化反向传播\n",
    "# dA_prev：池化层输入 A_prev 的梯度，形状与 A_prev 相同\n",
    "# 上游梯度在窗口中平均分配\n",
    "\n",
    "# ==============================\n",
    "# 打印平均池化结果\n",
    "# ==============================\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "# np.mean(dA)：上游梯度 dA 的平均值\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])\n",
    "# 打印第 1 个样本，第 1 行的梯度矩阵\n",
    "# 可以观察到平均池化梯度如何在窗口中均匀分布\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恭喜！\n",
    "\n",
    "恭喜你完成本次作业。  \n",
    "你现在已经理解了卷积神经网络的工作原理，并实现了神经网络的所有基本模块。  \n",
    "在下一节中，你将使用 PyTorch 实现一个完整的卷积神经网络（ConvNet）。\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
